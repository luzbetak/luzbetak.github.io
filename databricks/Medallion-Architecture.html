---
---
{% include menu.html title="Medallion Architecture in Delta Lake" %}

<h1>Medallion Architecture</h1>
<h2>Cost Savings</h2>
<ul>
    <li><strong>Reduced Storage Costs:</strong> Storing data incrementally (Bronze, Silver, Gold) minimizes storage costs by reducing the need to retain multiple versions or raw data copies. Only essential transformations are stored in the Gold layer, conserving storage space.</li>
    <li><strong>Optimized Resource Usage:</strong> Data processing tasks can be tailored to each layer, allowing for efficient resource allocation. For instance, resource-intensive tasks can be confined to specific layers, reducing cloud costs for computing and storage.</li>
    <li><strong>Lower Maintenance Costs:</strong> By standardizing data processing and quality checks across layers, maintenance overhead is reduced. This decreases the need for frequent data reconciliations and error correction, cutting down operational expenses.</li>
</ul>

<h2>Overview of Layers</h2>
<ul>
    <li><strong>Bronze Layer (Raw Data):</strong> Stores unprocessed data from various sources in its original state, preserving data fidelity. This serves as a single source of truth, allowing for reprocessing and auditing when needed.</li>
    <li><strong>Silver Layer (Cleansed Data):</strong> Data from the Bronze layer is cleaned, validated, and transformed. This intermediate layer improves data quality and ensures that only reliable data moves downstream, reducing the need for extensive processing in later stages.</li>
    <li><strong>Gold Layer (Curated Data):</strong> Contains refined, aggregated data tailored to specific business needs, such as analytics and reporting. This layer is aligned with business logic and optimized for high-performance queries and dashboards.</li>
</ul>

<h2>Key Benefits</h2>
<ul>
    <li><strong>Data Quality:</strong> Incremental processing through each layer enhances data reliability, reducing the need for costly data cleansing operations later in the workflow and enabling more accurate analytics and machine learning applications.</li>
    <li><strong>Scalability:</strong> The structured approach efficiently handles large data volumes. As the data grows, the architecture scales without requiring significant infrastructure investment, lowering overall data management costs.</li>
    <li><strong>Flexibility:</strong> Different teams can work on separate layers simultaneously, enabling parallel processing and faster data pipelines. This reduces development time, lowering operational costs associated with waiting on dependencies and rework.</li>
    <li><strong>Enhanced Data Governance:</strong> By having distinct layers, the architecture supports robust data governance practices, including access controls, data lineage tracking, and compliance measures. This approach reduces costs associated with regulatory compliance and minimizes the risk of expensive data breaches.</li>
</ul>


<p>In summary, the Medallion Architecture not only enhances data quality, scalability, and governance but also results in considerable cost savings by optimizing data storage, processing, and resource allocation across the entire data pipeline.</p>

<p><hr width=1000 align=left>
        <h1>Medallion Architecture with Partitioning</h1>
    <pre><code class=language-python>
spark = SparkSession.builder \
  .appName("Medallion Architecture") \
  .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
  .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
  .config("spark.executor.instances", "256") \
  .config("spark.sql.shuffle.partitions", "1024") \
  .getOrCreate()

# Paths for the Bronze, Silver, and Gold layers
bronze_path = "/mnt/delta/bronze"
silver_path = "/mnt/delta/silver"
gold_path   = "/mnt/delta/gold"

# ----------- Bronze Layer (Raw Data Ingestion) ----------------------------- #
raw_data = spark.read().format("json").load("/mnt/raw_data/")
raw_data = raw_data.repartition(1024)
raw_data.write().format("delta").mode("overwrite").save(bronze_path)


# ----------- Silver Layer (Data Cleaning and Transformation) --------------- #
bronze_df = spark.read().format("delta").load(bronze_path)

# Filter data for today and the last 365 days
silver_df = bronze_df.filter(
    (col("date") >= date_sub(current_date(), 365)) & (col("date") <= current_date())
)

silver_df = silver_df.dropDuplicates().filter(col("status").isNotNull())
silver_df = silver_df.repartition(365, col("date"))
silver_df.write().format("delta").mode("overwrite").save(silver_path)


# ----------- Gold Layer (Aggregated and Optimized Data) ------------------ #
silver_df = spark.read().format("delta").load(silver_path)

# Perform aggregations for reporting.
gold_df   = silver_df.groupBy("category") 
                     .agg( 
                          count("order_id").alias("total_orders"), 
                          sum("sales_amount").alias("total_sales") 
                         ) 
gold_df = gold_df.repartition(100, col("category"), col("date"))
gold_df.write().format("delta").mode("overwrite").save(gold_path)

spark.stop()    
</code></pre>

<p><hr width=1000 align=left>
<h1>PySpark Aggregation Tables</h1>

<h2>Data Description</h2>
<p>Dataset containing three columns: <strong>category</strong>, <strong>order_id</strong>, and <strong>sales_amount</strong>. The dataset is grouped by the <strong>category</strong> column, and we perform aggregation to count the total number of orders and sum up the sales amounts for each category.</p>

<h3>Original Dataset</h3>
<table class="comparison-table">
    <tr>
        <th>category</th>
        <th>order_id</th>
        <th>sales_amount</th>
    </tr>
    <tr>
        <td>Grocery</td>
        <td>1</td>
        <td>100</td>
    </tr>
    <tr>
        <td>Electronics</td>
        <td>2</td>
        <td>200</td>
    </tr>
    <tr>
        <td>Grocery</td>
        <td>3</td>
        <td>150</td>
    </tr>
    <tr>
        <td>Electronics</td>
        <td>4</td>
        <td>300</td>
    </tr>
    <tr>
        <td>Grocery</td>
        <td>5</td>
        <td>100</td>
    </tr>
</table>

<h3>Aggregated Result</h3>
<p>After performing the aggregation, the dataset is grouped by <strong>category</strong>, and we compute two new columns: <strong>total_orders</strong> and <strong>total_sales</strong>.</p>

<table class="comparison-table">
    <tr>
        <th>category</th>
        <th>total_orders</th>
        <th>total_sales</th>
    </tr>
    <tr>
        <td>Grocery</td>
        <td>3</td>
        <td>350</td>
    </tr>
    <tr>
        <td>Electronics</td>
        <td>2</td>
        <td>500</td>
    </tr>
</table>

  {% include footer.html %}

  </body>
</html>
