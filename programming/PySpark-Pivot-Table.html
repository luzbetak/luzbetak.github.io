---
---
{% include menu.html title="Kevin Luzbetak Github Pages" %}

<h1>Pivot Table Overview</h1>
<p>
    A <strong>pivot table</strong> is a tool used for summarizing data, allowing you to group and aggregate information based on categorical columns.
    In the context of PySpark, a pivot table transforms unique values from one column into multiple columns, aggregating values using functions like <code>sum</code>, <code>count</code>, <code>average</code>, etc.
    This is useful for reshaping data into a more digestible format, especially for reporting or analytics purposes.
</p>

<h2>Sample Data (JSON Format)</h2>
<pre><code class="python">
[
    {"employee": "Alice", "region": "North", "sales": 100},
    {"employee": "Bob", "region": "North", "sales": 200},
    {"employee": "Alice", "region": "South", "sales": 300},
    {"employee": "Bob", "region": "South", "sales": 400},
    {"employee": "Alice", "region": "East", "sales": 150},
    {"employee": "Bob", "region": "West", "sales": 250}
]
</code></pre>

<h2>PySpark Application to Display Both Tables</h2>
<p>The following PySpark code demonstrates how to display the original DataFrame (Table 1) and the pivoted DataFrame (Table 2):</p>
<pre><code class="python">
# Import necessary modules
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Pivot Table Example with Two Tables") \
    .getOrCreate()

# Sample data as a list of dictionaries
data = [
    {"employee": "Alice", "region": "North", "sales": 100},
    {"employee": "Bob", "region": "North", "sales": 200},
    {"employee": "Alice", "region": "South", "sales": 300},
    {"employee": "Bob", "region": "South", "sales": 400},
    {"employee": "Alice", "region": "East", "sales": 150},
    {"employee": "Bob", "region": "West", "sales": 250}
]

# Create DataFrame from the data
df = spark.createDataFrame(data)

# Display Table 1 - The Original DataFrame
df.show()

# Pivot the table to show sales by employee per region
pivot_df = df.groupBy("employee").pivot("region").agg(sum("sales"))

# Display Table 2 - The Pivoted DataFrame
pivot_df.show()

# Stop the Spark session
spark.stop()
</code></pre>

<h2>Explanation</h2>
<ul>
    <li><code class="python">df.show()</code> displays the original DataFrame (Table 1), containing employee, region, and sales data.</li>
    <li><code class="python">pivot_df.show()</code> displays the pivoted DataFrame (Table 2), with sales aggregated by employee and region.</li>
</ul>

<h2>Output</h2>
<p>The output would look something like this:</p>

<h3>Table 1 - Original DataFrame</h3>
<pre><code class="python">
+--------+-------+-----+
|employee| region|sales|
+--------+-------+-----+
|   Alice|  North|  100|
|     Bob|  North|  200|
|   Alice|  South|  300|
|     Bob|  South|  400|
|   Alice|   East|  150|
|     Bob|   West|  250|
+--------+-------+-----+
</code></pre>

<h3>Table 2 - Pivoted DataFrame</h3>
<pre><code class="python">
+--------+-----+-----+------+-----+
|employee| East|North| South| West|
+--------+-----+-----+------+-----+
|   Alice|  150|  100|   300| null|
|     Bob| null|  200|   400|  250|
+--------+-----+-----+------+-----+
</code></pre>

