---
---
{% include menu.html title="Kevin Luzbetak Github Pages" %}

    <h1>Pivot Table Overview</h1>
    <p>
        A <strong>pivot table</strong> is a tool used for summarizing data, allowing you to group and aggregate information based on categorical columns.
        In the context of PySpark, a pivot table transforms unique values from one column into multiple columns, aggregating values using functions like <code><span class="string">sum</span></code>, <code><span class="string">count</span></code>, <code><span class="string">average</span></code>, etc.
        This is useful for reshaping data into a more digestible format, especially for reporting or analytics purposes.
    </p>
    
    <h2>PySpark Application Using Pivot Table</h2>
    <p>The following PySpark code demonstrates how to create a pivot table with the provided sample data:</p>
    <pre><code class="python">
<span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession
<span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> sum

spark = SparkSession.builder \
    .appName(<span class="string">"Pivot Table Example"</span>) \
    .getOrCreate()

<span class="comment"># Sample data as a list of dictionaries</span>
data = [
    {"employee": <span class="string">"Alice"</span>, "region": <span class="string">"North"</span>, "sales": 100},
    {"employee": <span class="string">"Bob"</span>,   "region": <span class="string">"North"</span>, "sales": 200},
    {"employee": <span class="string">"Alice"</span>, "region": <span class="string">"South"</span>, "sales": 300},
    {"employee": <span class="string">"Bob"</span>,   "region": <span class="string">"South"</span>, "sales": 400},
    {"employee": <span class="string">"Alice"</span>, "region": <span class="string">"East"</span>,  "sales": 150},
    {"employee": <span class="string">"Bob"</span>,   "region": <span class="string">"West"</span>,  "sales": 250}
]

df = spark.createDataFrame(data)

df.show()

<span class="comment"># Pivot the table to show sales by employee per region</span>
pivot_df = df.groupBy(<span class="string">"employee"</span>).pivot(<span class="string">"region"</span>).agg(sum(<span class="string">"sales"</span>))

<span class="comment"># Show the pivoted DataFrame</span>
pivot_df.show()

spark.stop()
    </code></pre>

    <h2>Explanation</h2>
    <ul>
        <li><code class="python">groupBy("employee")</code> groups the data by employee.</li>
        <li><code class="python">pivot("region")</code> creates new columns for each unique value in the "region" column (e.g., "North", "South", "East", "West").</li>
        <li><code class="python">agg(sum("sales"))</code> aggregates the sales data by summing the values for each combination of employee and region.</li>
    </ul>

<h2>Output</h2>
<p>The output would look something like this:</p>

<h3>Table 1 - Original DataFrame</h3>
<pre><code class="python">
+--------+-------+-----+
|employee| region|sales|
+--------+-------+-----+
|   Alice|  North|  100|
|     Bob|  North|  200|
|   Alice|  South|  300|
|     Bob|  South|  400|
|   Alice|   East|  150|
|     Bob|   West|  250|
+--------+-------+-----+
</code></pre>

<h3>Table 2 - Pivoted DataFrame</h3>
<pre><code class="python">
+--------+-----+-----+------+-----+
|employee| East|North| South| West|
+--------+-----+-----+------+-----+
|   Alice|  150|  100|   300| null|
|     Bob| null|  200|   400|  250|
+--------+-----+-----+------+-----+
</code></pre>

  {% include footer.html %}

  </body>
</html>

