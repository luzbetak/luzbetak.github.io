---
---
{% include menu.html title="Model Parameters" %}

  <h1>Model Parameters = Words × Dimensions</h1>

  <h2>Key Concepts</h2>
  <ul>
    <li><strong>Word</strong>: A symbolic label (token) such as <strong>cat</strong>, <strong>dog</strong>, or <strong>car</strong>.</li>
    <li><strong>Dimension</strong>: The number of numeric values used to represent a word.</li>
    <li><strong>Parameter</strong>: A single stored numeric value. Every number in an embedding vector is one parameter.</li>
  </ul>

  <h2>Why This Example Has Exactly 6 Parameters</h2>
  <ol>
    <li>There are <strong>3 words</strong>: cat, dog, and car.</li>
    <li>Each word has <strong>2 dimensions</strong>.</li>
    <li>Total parameters = <strong>3 × 2 = 6</strong>.</li>
  </ol>

  <h3>Where the 6 Parameters Actually Live</h3>

  <pre><code class="language-text">
    Feature Dimensions:  [Object, Animal]
                   cat → [0, 1] → 2 Dimensions
                   dog → [0, 1] → 2 Dimensions
                   car → [1, 0] → 2 Dimensions
          -------------------------------------
          Total: → 3 × 2 = 6 Parameters


          +---------------------------------------+
          |  Vector DB: with Feature Dimensions   |
          +--------+--------+--------+------------+
          |  Word  | Object | Animal | Dimensions |
          +--------+--------+--------+------------+
          |  cat   |   0    |   1    |     2      |
          |  dog   |   0    |   1    |     2      |
          |  car   |   1    |   0    |     2      |
          +--------+--------+--------+------------+
          | Total  |      3 × 2 = 6 Parameters    |
          +--------+--------+--------+------------+
  </pre></code>


  <pre><code class="language-text">
   <h3> Bridging the example to real models:</h3>              
    +------------------+------------+------------+------------------+
    |      Model       |   Words    | Dimensions |    Parameters    |
    +------------------+------------+------------+------------------+
    | This example     |          3 |          2 |                6 |
    | GPT-2 embeddings |     50,257 |        768 |       38,597,376 |
    +------------------+------------+------------+------------------+
  </pre></code>

  <h2>Python Code Example</h2>

  <pre><code class="language-python">import numpy as np

embedding_table = {

    #--- Feature Dim: [Object, Animal] ---#
    "cat": np.array([  0     ,  1 ]),
    "dog": np.array([  0     ,  1 ]),
    "car": np.array([  1     ,  0 ]),

}

def embed(word):
    return embedding_table[word]

print(embed("cat"))
print(embed("dog"))</code></pre>

  <h2>Final Summary</h2>
  <ul>
    <li><strong>Words</strong> are labels.</li>
    <li><strong>Dimensions</strong> describe vector length per word.</li>
    <li><strong>Parameters</strong> are the total stored numeric values.</li>
    <li>This example has <strong>6 parameters</strong>.</li>
  </ul>


  <h1>How the Embedding Numbers Are Computed</h1>

  <h2>Goal</h2>
  <ul>
    <li>Show a <strong>simple, concrete way</strong> to compute the numbers in embedding vectors.</li>
    <li>Explain where values like <strong>[0, 1]</strong> and <strong>[0, 1]</strong> come from.</li>
  </ul>

  <h2>Important Note</h2>
  <ul>
    <li>In real models, these numbers are <strong>learned during training</strong>.</li>
    <li>Here we use a <strong>toy mathematical rule</strong> to make the idea understandable.</li>
  </ul>

  <h2>Simple Rule to Compute Embedding Numbers</h2>
  <ul>
    <li>Assign each word two numeric features:</li>
    <ul>
      <li><strong>Feature 1</strong>: how “animal-like” the word is</li>
      <li><strong>Feature 2</strong>: how “object-like” the word is</li>
    </ul>
    <li>Each feature is a number between <strong>0.0</strong> and <strong>1.0</strong>.</li>
    <li>The two feature values together form a <strong>2-dimensional embedding</strong>.</li>
  </ul>

  <h3>Manual Feature Assignment</h3>
  <ul>
    <li><strong>cat</strong>: mostly animal → [0, 1]</li>
    <li><strong>dog</strong>: mostly animal → [0, 1]</li>
    <li><strong>car</strong>: mostly object → [1, 0]</li>
  </ul>

  <h2>Python Example: Computing the Numbers</h2>

 <hr />

  <pre><code class="language-python">def compute_embedding(animal_score, object_score):
    # Combine two numeric features into a vector
    return [animal_score, object_score]

cat_embedding = compute_embedding(0, 1)
dog_embedding = compute_embedding(0, 1)

print("cat:", cat_embedding)
print("dog:", dog_embedding)</code></pre>

  <h2>What This Represents</h2>
  <ul>
    <li>Each number is a <strong>parameter value</strong>.</li>
    <li>The vector length (2 numbers) is the <strong>dimension</strong>.</li>
    <li>The rule that assigns numbers is the <strong>model logic</strong>.</li>
  </ul>

  <h2>Connection to Real Models</h2>
  <ul>
    <li>Real embedding models start with random numbers.</li>
    <li>Numbers are adjusted using training data and optimization.</li>
    <li>After training, vectors encode semantic meaning.</li>
  </ul>

  <h2>Final Summary</h2>
  <ul>
    <li><strong>[0, 1]</strong> and <strong>[0, 1]</strong> are examples of computed feature values.</li>
    <li>The computation rule is simple here for clarity.</li>
    <li>Real models learn these values automatically.</li>
  </ul>


{% include footer.html %}
