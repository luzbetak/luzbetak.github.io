[
    {
        "id": 1,
        "title": "Big-O-Notation-Time-Complexity.html",
        "content": "---\n---\n{% include menu.html title=\"Kevin Luzbetak - Computer Science\" %} Big O Notation - Time Complexity Big O notation is used to describe the efficiency of an algorithm, focusing on its time complexity (how the execution time grows with input size) and space complexity (how much extra memory is needed). It expresses the worst-case scenario performance of an algorithm. Common Complexities O(1) – Constant time: The algorithm's runtime does not change with the input size. Example: Accessing an element in an array by index. O(log n) – Logarithmic time: The algorithm reduces the problem size by a constant factor with each step. Example: Binary search in a sorted array. O(n) – Linear time: The runtime grows proportionally with the input size. Example: Traversing a list of n elements. O(n log n) – Linearithmic time: The algorithm performs a linear number of operations for each logarithmic division. Example: Merge sort and quicksort in their average cases. O(n²) – Quadratic time: The runtime grows quadratically with input size. Example: A double nested loop, like in bubble sort or selection sort. O(2^n) – Exponential time: The runtime doubles with each addition to the input size. Example: Solving the traveling salesman problem using brute force. O(n!) – Factorial time: The runtime increases factorially with the input size. Example: Generating all permutations of a set. Data Structures and Their Time Complexities Hash Table Access: O(1) (average case), O(n) (worst case due to collisions) Search: O(1) (average case) Insertion/Deletion: O(1) (average case) B-Tree Access/Search: O(log n) Insertion/Deletion: O(log n) Space Complexity: O(n) Balanced Binary Search Tree (e.g., AVL Tree, Red-Black Tree) Access/Search: O(log n) Insertion/Deletion: O(log n) Binary Search Tree (BST) Access/Search: O(log n) (average), O(n) (worst case, unbalanced) Insertion/Deletion: O(log n) (average), O(n) (worst case) B-Tree Balancing B-Trees are inherently balanced during indexing, so they do not require rebalancing in the way that some other tree structures, like AVL trees or Red-Black trees, do. Balanced Nature: B-Trees are self-balancing by design. When you insert or delete keys, the tree is adjusted to maintain its balance. This is achieved by splitting or merging nodes as necessary during insertions and deletions, ensuring that all leaf nodes remain at the same level and the tree remains balanced. Indexing: During the indexing process, as new keys are inserted into the B-Tree, the structure automatically ensures that the tree remains balanced. This is done by maintaining certain properties, such as: All nodes (except the root) having at least a minimum number of children. The height of the tree being kept as low as possible to optimize search operations. Because of this, B-Trees are particularly well-suited for use in databases and file systems where efficient data retrieval and storage are critical. Array (Fixed size) Access: O(1) Search: O(n) Insertion: O(n) (if resizing required) Deletion: O(n) Linked List Access: O(n) Search: O(n) Insertion/Deletion (at head): O(1) Heap (Priority Queue) Access (max or min): O(1) Insertion/Deletion: O(log n) Graph (Adjacency Matrix/Adjacency List) Search (DFS, BFS): O(V + E), where V is the number of vertices and E is the number of edges Stack/Queue Access: O(n) Insertion/Deletion: O(1) {% include footer.html %}",
        "url": "/Big-O-Notation-Time-Complexity.html"
    },
    {
        "id": 2,
        "title": "Gunning-Fog-Index.html",
        "content": "---\n---\n{% include menu.html title=\"Python Algorithms\" %} Gunning Fog Index The Gunning Fog Index is a readability test that estimates the years of formal education needed to understand a text on the first reading. It takes into account the number of words, the number of complex words (words with three or more syllables), and the number of sentences in a text. import re\n\ndef count_words(text):\n    \"\"\"Counts the number of words in a text.\"\"\"\n    words = re.findall(r'\\w+', text)\n    return len(words)\n\ndef count_sentences(text):\n    \"\"\"Counts the number of sentences in a text.\"\"\"\n    sentences = re.split(r'[.!?]+', text)\n    return len(sentences) - 1 if text[-1] in '.!?' else len(sentences)\n\ndef count_complex_words(text):\n    \"\"\"Counts the number of complex words in a text (words with three or more syllables).\"\"\"\n    def syllable_count(word):\n        word = word.lower()\n        vowels = \"aeiouy\"\n        count = 0\n        if word[0] in vowels:\n            count += 1\n        for index in range(1, len(word)):\n            if word[index] in vowels and word[index - 1] not in vowels:\n                count += 1\n        if word.endswith(\"e\"):\n            count -= 1\n        if count == 0:\n            count += 1\n        return count\n\n    words = re.findall(r'\\w+', text)\n    complex_words = [word for word in words if syllable_count(word) >= 3]\n    return len(complex_words)\n\ndef gunning_fog_index(text):\n    \"\"\"Calculates the Gunning Fog Index for a given text.\"\"\"\n    num_words = count_words(text)\n    num_sentences = count_sentences(text)\n    num_complex_words = count_complex_words(text)\n\n    if num_sentences == 0:\n        return 0  # Avoid division by zero\n\n    average_sentence_length = num_words / num_sentences\n    percentage_complex_words = (num_complex_words / num_words) * 100\n\n    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n    return fog_index\n\n# Example usage\ntext = (\n    \"The Gunning Fog Index is a readability test for English writing. \"\n    \"It estimates the years of formal education needed to understand the text on the first reading. \"\n    \"Complex words are those with three or more syllables.\"\n)\n\nfog_index = gunning_fog_index(text)\nprint(f\"The Gunning Fog Index for the given text is: {fog_index:.2f}\") OUTPUT The Gunning Fog Index for the given text is: 10.40 Gunning Fog Index Explanation The Gunning Fog Index is a readability test that estimates the years of formal education needed to understand a text on the first reading. The implementation calculates the Gunning Fog Index using the following steps: Words: The code counts all words in the text using regular expressions. Sentences: Sentences are counted by splitting the text at sentence-ending punctuation marks, such as period (.), exclamation mark (!), and question mark (?). Complex Words: A complex word is defined as one with three or more syllables. The syllable_count function estimates the number of syllables in a word by analyzing vowel patterns. Gunning Fog Index Calculation: The formula used for the Gunning Fog Index is: Average sentence length = number of words / number of sentences. Percentage of complex words = (number of complex words / number of words) * 100. The Gunning Fog Index is calculated as: 0.4 * (average sentence length + percentage of complex words) . Usage: A Gunning Fog Index of 7-8 is considered optimal for general readership. A higher score suggests that the text is more difficult to read, requiring higher levels of education. This implementation provides a way to rank texts based on readability using the Gunning Fog Index, which is useful in various Natural Language Processing (NLP) applications such as document analysis, content creation, and education. {% include footer.html %}",
        "url": "/Gunning-Fog-Index.html"
    },
    {
        "id": 3,
        "title": "Keras.html",
        "content": "---\n---\n{% include menu.html title=\"Keras (Training Neural Networks\" %} Keras Overview Keras is an open-source deep learning library that provides a high-level API for building and training neural networks. It is user-friendly,\n    modular, and extensible, allowing developers to create complex models with minimal code. Keras runs on top of low-level deep learning frameworks\n    such as TensorFlow, Theano, and CNTK, making it both versatile and powerful. Key Features: Ease of Use: Keras offers a simple and consistent API to build neural networks quickly. Modular: It allows you to build models by combining different building blocks (layers, optimizers, loss functions). Flexibility: Keras can be used for a variety of tasks including image classification, text generation, reinforcement learning, and more. Support for both CPU and GPU: Keras can run on both CPU and GPU hardware, enabling faster training when using GPUs. Backed by TensorFlow: Keras is now part of the TensorFlow core library, making it the default high-level API for TensorFlow. 1. Feedforward Neural Network (Classification Task) A Feedforward Neural Network (FNN) is a type of artificial neural network where the connections between nodes do not form a cycle. It is called \"feedforward\" because the data flows only in one direction—from the input layer to the output layer—without any loops or feedback connections. Structure: Input Layer: This layer receives the input features. Each node in the input layer corresponds to one feature of the data. Hidden Layers: These are the layers where most of the computation occurs. The hidden layers use activation functions like ReLU (Rectified Linear Unit) to introduce non-linearity, allowing the network to learn complex patterns in the data. Output Layer: This layer produces the final prediction. For classification tasks, the output layer typically uses a sigmoid (binary classification) or softmax (multi-class classification) activation function to output probabilities for each class. Example of a Classification Task: In the example of classifying data with synthetic features: The network learns to map the input features (e.g., 20 features) to the correct class labels (0 or 1) by adjusting the weights and biases during training. The process involves forward propagation (calculating output) and backpropagation (adjusting weights based on the loss function and optimizer like Adam). It is suitable for basic tasks like classifying tabular data. Use Case: Binary or multi-class classification on structured data such as customer churn prediction, medical diagnosis, or fraud detection. import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Generate synthetic data for classification\nX, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Normalize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=20, activation='relu'))  # Input layer\nmodel.add(Dense(16, activation='relu'))  # Hidden layer\nmodel.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Accuracy: {accuracy:.2f}\") 2. Convolutional Neural Network (CNN) for Image Classification A Convolutional Neural Network (CNN) is a specialized neural network primarily used for processing structured grid-like data such as images. CNNs are particularly effective at recognizing spatial hierarchies in data, making them ideal for tasks like image and video recognition. Structure: Input Layer: The input to a CNN is typically a multi-dimensional image (e.g., a grayscale image with a size of 28x28 pixels has one channel, while a colored image has 3 channels: RGB). Convolutional Layers: These layers apply convolution operations to the input, using filters (kernels) to detect features like edges, corners, and textures. The output of each convolutional layer is a feature map that highlights these patterns. Pooling Layers: These layers reduce the spatial dimensions of the feature maps (e.g., max pooling), helping to retain important information while reducing computational complexity. Flattening: After the convolutional and pooling layers, the 2D feature maps are flattened into a 1D vector that can be fed into a fully connected layer. Fully Connected Layers: These layers are similar to a feedforward network and are used to make the final classification decision. The output layer typically uses a softmax activation function for multi-class classification. Example of Image Classification: In the MNIST digit classification example: The network takes a 28x28 pixel grayscale image as input, passes it through convolutional layers to detect edges and patterns, and eventually predicts the digit class (0-9) using fully connected layers. CNNs are excellent at reducing the number of parameters by reusing the filters across the image, making them highly efficient for image processing tasks. Use Case: Image classification tasks such as handwriting recognition, object detection, medical image analysis, and facial recognition. from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\n\n# Load the MNIST dataset (handwritten digits)\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape the data to fit the model\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28, 1)\n\n# Normalize the data\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\n# One-hot encode the labels\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Define the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.2f}\") Summary of Differences: Feedforward Neural Network: Best for structured, tabular data where features are not spatially related. Convolutional Neural Network: Best for image or grid-like data where spatial hierarchies matter (e.g., recognizing objects or patterns in images). Key Components in Keras: Models: The core structure is either Sequential (a linear stack of layers) or Functional API (more flexible for complex models). Layers: Building blocks of neural networks such as Dense, Conv2D, LSTM, etc. Optimizers: Algorithms for adjusting weights during training (e.g., Adam, SGD). Loss Functions: Used to minimize the error in predictions (e.g., binary_crossentropy , mean_squared_error ). Metrics: Metrics like accuracy used to evaluate model performance. Keras provides a highly accessible platform for building deep learning models quickly while abstracting many low-level complexities. {% include footer.html %}",
        "url": "/Keras.html"
    },
    {
        "id": 4,
        "title": "Tensors-Machine-Learning.html",
        "content": "---\n---\n{% include menu.html title=\"Tensors (Multi-Dimensional Array)\" %} What Tensor in Machine Learning? In essence, a tensor is a multidimensional array used to represent data. Think of it as a generalization of vectors and matrices. A scalar (a single number) is a 0-dimensional tensor. A vector (a list of numbers) is a 1-dimensional tensor. A matrix (a table of numbers) is a 2-dimensional tensor. And we can have tensors with 3, 4, or even more dimensions! Tensors are the fundamental data structure in deep learning frameworks like TensorFlow and PyTorch. They allow us to efficiently represent and\nprocess complex data like images (3D tensors: height, width, color channels), videos (4D tensors: time, height, width, color channels), and even\nnatural language (where words are embedded in high-dimensional spaces). Simple Python Code with Tensors (using NumPy) NumPy is a powerful library for numerical computations in Python and provides excellent support for working with tensors (which NumPy calls 'ndarrays'). import numpy as np\n\n# Create a 2D tensor (a matrix)\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6]])\n\n# Create a 3D tensor\ntensor_3d = np.array([[[1, 2], [3, 4]],\n                      [[5, 6], [7, 8]]])\n\n# Access elements of the tensors\nprint(\"Element at row 1, column 2 of the matrix:\", matrix[1, 2])\nprint(\"Element at depth 0, row 1, column 0 of the 3D tensor:\", tensor_3d[0, 1, 0])\n\n# Perform operations on tensors\nsum_of_matrix = np.sum(matrix)\nprint(\"Sum of all elements in the matrix:\", sum_of_matrix)\n\n# Output\n# Element at row 1, column 2 of the matrix: 6\n# Element at depth 0, row 1, column 0 of the 3D tensor: 3\n# Sum of all elements in the matrix: 21 In this code: Import the NumPy library. Create a 2D tensor ( matrix ) and a 3D tensor ( tensor_3d ) using np.array . Access specific elements using indexing (remember, indexing starts at 0). Perform an operation (summation) on the matrix using np.sum . Key Points Tensors enable us to represent and manipulate data with any number of dimensions. Deep learning frameworks heavily rely on tensors for efficient computation on GPUs. NumPy provides a solid foundation for working with tensors in Python. import numpy as np\n\n# create a 3D tensor with dimensions (2, 3, 4)\ntensor = np.zeros((2, 3, 4))\n\n# fill the tensor with some values\ntensor[0, 0, 0] = 1\ntensor[0, 1, 1] = 2\ntensor[0, 2, 2] = 3\ntensor[1, 0, 3] = 4\ntensor[1, 1, 2] = 5\ntensor[1, 2, 1] = 6\n\n# print the tensor\nprint(tensor)\n\n# access a specific value in the tensor (e.g. the value at index (1, 0, 3))\nvalue = tensor[1, 0, 3]\nprint(value)\n\n# sum all the values in the tensor\ntotal = np.sum(tensor)\nprint(total)\n\n# compute the mean of all the values in the tensor\naverage = np.mean(tensor)\nprint(average) In this example, we create a 3D tensor with dimensions (2, 3, 4) using the `np.zeros()` function. We then fill the tensor with some values using indexing. We print the tensor, access a specific value, compute the sum and mean of all the values in the tensor using the `np.sum()` and `np.mean()` functions. Note that 3D tensors are also commonly used in deep learning frameworks such as TensorFlow or PyTorch. The syntax for creating and manipulating 3D tensors in those frameworks is similar to NumPy, but with additional functionality for building and training machine learning models. {% include footer.html %}",
        "url": "/Tensors-Machine-Learning.html"
    },
    {
        "id": 5,
        "title": "Scikit-learn.html",
        "content": "---\n---\n{% include menu.html title=\"Scikit-learn\" %} Scikit-learn Overview Scikit-learn is a widely used open-source Python library for machine learning, providing simple and efficient tools for data analysis and\n    modeling. It is built on top of popular libraries like NumPy , SciPy , and Matplotlib , and offers\n    a wide range of algorithms for supervised and unsupervised learning. Key Features Preprocessing: Tools for scaling, normalization, encoding categorical variables, and handling missing data. Model Selection: Cross-validation, grid search, and hyperparameter tuning for selecting the best models. Supervised Learning: Algorithms like linear regression, support vector machines (SVM), decision trees, and random forests. Unsupervised Learning: Algorithms for clustering (K-means, DBSCAN) and dimensionality reduction (PCA, t-SNE). Metrics: A variety of metrics for evaluating model performance, such as accuracy, precision, recall, and AUC-ROC. Model Persistence: Allows saving trained models for later use via joblib or pickle. Scikit-learn is highly accessible for both beginners and professionals, with well-documented APIs and good integration with other data science tools like Pandas. from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\ndata = load_iris()\nX = data.data  # Features\ny = data.target  # Target labels\n\n# Split the dataset into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\n\n# Print the accuracy\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print a comparison of actual vs predicted values\nprint(\"\\nActual vs Predicted:\")\nfor actual, predicted in zip(y_test, y_pred):\n    print(f\"Actual: {actual}, Predicted: {predicted}\")\n\n# Print some sample data points from the test set to explain the results\nprint(\"\\nSample Data Points (Features) from the Test Set:\")\nfor i in range(5):\n    print(f\"Test Sample {i+1}: {X_test[i]}, Predicted Label: {y_pred[i]}, Actual Label: {y_test[i]}\")\n\n#------------------------------------------------------------------------------------#\n# Output\n# Accuracy: 1.00\n# \n# Actual vs Predicted:\n# Actual: 1, Predicted: 1\n# Actual: 0, Predicted: 0\n# Actual: 2, Predicted: 2\n# Actual: 1, Predicted: 1\n# Actual: 1, Predicted: 1\n# Actual: 0, Predicted: 0\n# Actual: 1, Predicted: 1\n# Actual: 2, Predicted: 2\n# Actual: 1, Predicted: 1\n# Actual: 1, Predicted: 1\n# Actual: 2, Predicted: 2\n# Actual: 0, Predicted: 0\n# Actual: 0, Predicted: 0\n# Actual: 0, Predicted: 0\n# Actual: 0, Predicted: 0\n# Actual: 1, Predicted: 1\n# Actual: 2, Predicted: 2\n# Actual: 1, Predicted: 1\n# Actual: 1, Predicted: 1\n# Actual: 2, Predicted: 2\n# Actual: 0, Predicted: 0\n# Actual: 2, Predicted: 2\n# Actual: 0, Predicted: 0\n# Actual: 2, Predicted: 2\n# Actual: 2, Predicted: 2\n# Actual: 2, Predicted: 2\n# Actual: 2, Predicted: 2\n# Actual: 2, Predicted: 2\n# Actual: 0, Predicted: 0\n# Actual: 0, Predicted: 0\n# \n# Sample Data Points (Features) from the Test Set:\n# Test Sample 1: [6.1 2.8 4.7 1.2], Predicted Label: 1, Actual Label: 1\n# Test Sample 2: [5.7 3.8 1.7 0.3], Predicted Label: 0, Actual Label: 0\n# Test Sample 3: [7.7 2.6 6.9 2.3], Predicted Label: 2, Actual Label: 2\n# Test Sample 4: [6.  2.9 4.5 1.5], Predicted Label: 1, Actual Label: 1\n# Test Sample 5: [6.8 2.8 4.8 1.4], Predicted Label: 1, Actual Label: 1\n#------------------------------------------------------------------------------------# Scikit-learn Code Summary This code performs the following actions: Loads the Iris dataset. Splits the dataset into training and test sets. Initializes and trains a Random Forest classifier. Predicts on the test set and calculates the accuracy of the model. About the Iris Dataset Yes, the code will automatically load the Iris dataset using the load_iris() function provided by Scikit-learn. The Iris dataset is one of several built-in toy datasets included in Scikit-learn, so you don't need to manually download or load any external files. Overview of How it Works load_iris() fetches the Iris dataset, which contains data on 150 samples of iris flowers, with features like sepal length, sepal width, petal length, and petal width. The dataset is structured in a Bunch object, which behaves like a dictionary and holds both the data (features) and the target (labels). You can directly access the data and labels using data.data for the features and data.target for the labels. No external setup is needed to use this dataset—it's available right out of the box in Scikit-learn. Scikit-learn Comparison Comparison of Actual vs Predicted Labels: This helps you see if the predictions match the actual labels, explaining the high accuracy. Sample Data Points: This will print a few data points from the test set (both features and corresponding predictions) to show you how well the model is performing on specific samples. If the accuracy is 1.00 , the actual labels and predicted labels should match perfectly for all test samples, and you will see identical values in the comparison output. {% include footer.html %}",
        "url": "/Scikit-learn.html"
    }
]
