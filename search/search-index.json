[
    {
        "id": 1,
        "title": "TF-IDF.html",
        "content": "{% include menu.html title=\"TF IDF\" %}  python # # # TF IDF (Term Frequency Inverse Document Frequency) # # from sklearn.feature_extraction.text import TfidfVectorizer from prettytable import PrettyTable # Example documents docs = [ \"This is a sample document. \",      \"This document is another sample document.",
        "url": "/TF-IDF.html"
    },
    {
        "id": 2,
        "title": "Retrieval-Augmented-Generation.html",
        "content": "{% include menu.html title=\"RAG (Retrieval Augmented Generation)\" %} Building a High Performance RAG Solution with Pgvectorscale and Python 1. RAG (Retrieval Augmented Generation) RAG enhances the response generation process by retrieving relevant documents from an external knowledge base (e.g., a vector database) and using these documents to inform the generated responses.",
        "url": "/Retrieval-Augmented-Generation.html"
    },
    {
        "id": 3,
        "title": "Tensors-Machine-Learning.html",
        "content": "{% include menu.html title=\"Tensors (Multi Dimensional Array)\" %} Tensor in Machine In essence, a tensor is a multidimensional array used to represent data. Think of it as a generalization of vectors and matrices.",
        "url": "/Tensors-Machine-Learning.html"
    },
    {
        "id": 4,
        "title": "Random-Forest-Classifier-Model.html",
        "content": "{% include menu.html title=\"Random Forest Classifier Model\" %} Random Forest Classifieri with TF IDF Vectorizer This model consists of a collection of decision trees (the \"forest\"), where each tree is trained on a random subset of the data. The final prediction is made by averaging the predictions of all the individual trees, which helps reduce overfitting and improves generalization.",
        "url": "/Random-Forest-Classifier-Model.html"
    },
    {
        "id": 5,
        "title": "Scikit-learn.html",
        "content": "{% include menu.html title=\"Scikit learn\" %} Scikit learn Overview Scikit learn is a widely used open source Python library for machine learning, providing simple and efficient tools for data analysis and   modeling. It is built on top of popular libraries like NumPy , SciPy , and Matplotlib , and offers   a wide range of algorithms for supervised and unsupervised learning.",
        "url": "/Scikit-learn.html"
    },
    {
        "id": 6,
        "title": "Vector-Database.html",
        "content": "{% include menu.html title=\"Vector Database\" %} Vector Database A vector database is a specialized type of database designed to efficiently store, retrieve, and query data in vector format. Vectors, often representing numerical or feature embeddings from high dimensional data (e.g., images, text, audio), are used extensively in machine learning models.",
        "url": "/Vector-Database.html"
    },
    {
        "id": 7,
        "title": "hello.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak  Computer Science\" %} Python Syntax Highlighting def greet(name):   print(f\"Hello, {name}!\") def add(a, b):   return a + b if __name__ == \"__main__\":   greet(\"World\")   result = add(5, 3)   print(f\"5 + 3 = {result}\") SQL Syntax Highlighting SELECT name, age FROM users WHERE age > 18 ORDER BY age DESC; Bash Syntax Highlighting #!/bin/bash echo \"Hello, World!\"",
        "url": "/hello.html"
    },
    {
        "id": 8,
        "title": "Medallion-Architecture.html",
        "content": "",
        "url": "/Medallion-Architecture.html"
    },
    {
        "id": 9,
        "title": "Keras.html",
        "content": "{% include menu.html title=\"Keras (Training Neural Networks\" %} Keras Overview Keras is an open source deep learning library that provides a high level API for building and training neural networks. It is user friendly,   modular, and extensible, allowing developers to create complex models with minimal code.",
        "url": "/Keras.html"
    },
    {
        "id": 10,
        "title": "Gunning-Fog-Index.html",
        "content": "{% include menu.html title=\"Python Algorithms\" %} Gunning Fog Index The Gunning Fog Index is a readability test that estimates the years of formal education needed to understand a text on the first reading. It takes into account the number of words, the number of complex words (words with three or more syllables), and the number of sentences in a text.",
        "url": "/Gunning-Fog-Index.html"
    },
    {
        "id": 11,
        "title": "PyTorch-Sentiment-Analysis-Model.html",
        "content": "{% include menu.html title=\"sentiment analysis model\" %} Build and Save Sentiment Model  python import torch from torch.utils.data import Dataset, DataLoader from torch import nn from transformers import BertTokenizer, BertModel from sklearn.metrics import accuracy_score, classification_report # Define the TextDataset class class TextDataset(Dataset):   def __init__(self, texts, labels):     self.texts   = texts     self.labels  = labels     self.tokenizer = BertTokenizer.from_pretrained('bert base uncased')     def __len__(self):     return len(self.texts)     def __getitem__(self, idx):     text  = self.texts[idx]     label = self.labels[idx]     tokens = self.tokenizer(       text,       padding='max_length',       max_length=128,       truncation=True,       return_tensors='pt'     )     input_ids = tokens['input_ids'].squeeze(0)     attention_mask = tokens['attention_mask'].squeeze(0)     return {       'input_ids': input_ids,       'attention_mask': attention_mask,       'labels': torch.tensor(label, dtype=torch.float)     } # Expanded dataset with both positive and negative examples text_data = [   \"This movie is amazing! \",        # positive   \"I really disliked the plot.",
        "url": "/PyTorch-Sentiment-Analysis-Model.html"
    },
    {
        "id": 12,
        "title": "Time-Complexity-Big-O-Notation.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak  Computer Science\" %} Big O Notation  Time Complexity Big O notation is used to describe the efficiency of an algorithm, focusing on its time complexity (how the execution time grows with input size) and space complexity (how much extra memory is needed). It expresses the worst case scenario performance of an algorithm.",
        "url": "/Time-Complexity-Big-O-Notation.html"
    },
    {
        "id": 13,
        "title": "PyTorch.html",
        "content": "{% include menu.html title=\"PyTorch Neural Network\" %} PyTorch Machine Learning Library PyTorch is an open source machine learning library used for a wide variety of tasks such as deep learning, natural language processing (NLP), and computer vision. It provides a flexible platform to build machine learning models and comes with strong support for GPU acceleration, making it popular among researchers and developers.",
        "url": "/PyTorch.html"
    },
    {
        "id": 14,
        "title": "Managed-External-Live-Tables.html",
        "content": "{% include menu.html title=\"Delta Live (DLT), Managed, External Tables\" %} Databricks Delta Live (DLT), Managed, External Tables Key Differences: Feature Delta Live Tables (DLT) Managed Tables External Tables Data Management Managed pipelines with automation for data ingestion, transformation, and output Fully managed by Databricks Data stored externally, metadata managed by Databricks Storage Location Can use managed or external storage Databricks File System (DBFS) or default cloud storage External storage (e.g., S3, Blob, HDFS) Data Lifecycle Lifecycle managed by DLT pipelines Data is deleted when the table is dropped Data remains after the table is dropped Use Case Automated ETL pipelines and real time data processing Temporary or internal datasets managed by Databricks Persistent or shared datasets Automation & Monitoring Automated pipeline execution, monitoring, and quality checks No automation for tasks No automation for tasks 1. Delta Live Tables (DLT) Delta Live Tables (DLT) is a framework designed for building and managing ETL pipelines.",
        "url": "/bricks/Managed-External-Live-Tables.html"
    },
    {
        "id": 15,
        "title": "PySpark-Coding-Examples.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak  Computer Science\" %} PySpark SQL Exercises 1. Select Unique Records from pyspark.sql import SparkSession # Sample data data = [(1, \"Alice\" , 25), (2, \"Bob\" , 30), (3, \"Alice\" , 25)]     df = spark.createDataFrame(data, [ \"id\" , \"name\" , \"age\" ])     df.createOrReplaceTempView( \"people\" ) # Select distinct names unique_names_df = spark.sql( \"SELECT DISTINCT name FROM people\" ) # Show result unique_names_df.show() 2.",
        "url": "/bricks/PySpark-Coding-Examples.html"
    },
    {
        "id": 16,
        "title": "RDBMS-Schemas.html",
        "content": "{% include menu.html title=\"RDBMS Schemas\" %} The following are common database schemas used in relational database management systems (RDBMS), including the Star Schema, from which the Snowflake Schema is derived: Star Schema : The foundational schema that involves a central fact table connected directly to several dimension tables. Simpler structure with denormalized dimension tables.",
        "url": "/bricks/RDBMS-Schemas.html"
    },
    {
        "id": 17,
        "title": "PySpark-Data-Streaming.html",
        "content": "{% include menu.html title=\"PySpark Data Streaming\" %} PySpark Data Streaming Real Time Data Processing: PySpark Streaming enables the processing of live data streams, allowing you to handle continuous data input, like logs, sensor data, or tweets. DStream (Discretized Stream): The core abstraction in PySpark Streaming is the DStream, which represents a continuous stream of data divided into small batches (micro batches).",
        "url": "/bricks/PySpark-Data-Streaming.html"
    },
    {
        "id": 18,
        "title": "Databricks-Delta-Lake.html",
        "content": "{% include menu.html title=\"Databricks Delta Lake Design for Big Data\" %} Databricks Delta Lake Design for Big Data Unified Data Processing Delta Lake allows seamless support for both batch and streaming data processing using a single data copy. This provides flexibility in handling various types of workloads without the need to duplicate data.",
        "url": "/bricks/Databricks-Delta-Lake.html"
    },
    {
        "id": 19,
        "title": "RDBMS-Snowflake-Schema.html",
        "content": "{% include menu.html title=\"Snowflake Schema\" %} Snowflake Schema The Snowflake Schema is a variation of the Star Schema in a relational database. It involves normalizing the dimension tables, which means breaking them down into multiple related tables to eliminate redundancy.",
        "url": "/bricks/RDBMS-Snowflake-Schema.html"
    },
    {
        "id": 20,
        "title": "etl-pipeline.html",
        "content": "{% include menu.html title=\"Key Points of ETL (Extract, Transform, Load)\" %} ETL (Extract, Transform, Load) 1. Extract Data Sources  ETL begins with data extraction from various sources such as databases, APIs, files, or cloud services.",
        "url": "/bricks/etl-pipeline.html"
    },
    {
        "id": 21,
        "title": "Databricks-PySpark.html",
        "content": "{% include menu.html title=\"PySpark and Databricks Deep Dive\" %} PySpark and Databricks Deep Dive 1. PySpark Overview PySpark is the Python API for Apache Spark, an open source distributed computing system.",
        "url": "/bricks/Databricks-PySpark.html"
    },
    {
        "id": 22,
        "title": "PySpark-Pivot-Table.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak Github Pages\" %} Pivot Table Overview A pivot table is a tool used for summarizing data, allowing you to group and aggregate information based on categorical columns. In the context of PySpark, a pivot table transforms unique values from one column into multiple columns, aggregating values using functions like sum , count , average , etc.",
        "url": "/bricks/PySpark-Pivot-Table.html"
    },
    {
        "id": 23,
        "title": "RDBMS-Star-Schema.html",
        "content": "{% include menu.html title=\"RDBMS Star Schema\" %} RDBMS Star Schema The Star Schema is a popular database schema design used in data warehousing. It is named for its resemblance to a star, where a central fact table is surrounded by dimension tables.",
        "url": "/bricks/RDBMS-Star-Schema.html"
    },
    {
        "id": 24,
        "title": "PySpark-SQL-Functions-Parquet.html",
        "content": "{% include menu.html title=\"PySpark SQL Functions\" %} PySpark Spark SQL 1. Select Unique Records from pyspark.sql import SparkSession # Sample data data = [(1, \"Alice\" , 25), (2, \"Bob\" , 30), (3, \"Alice\" , 25)]     df = spark.createDataFrame(data, [ \"id\" , \"name\" , \"age\" ])     df.createOrReplaceTempView( \"people\" ) # Select distinct names unique_names_df = spark.sql( \"SELECT DISTINCT name\"             \"FROM people\" ) # Show result unique_names_df.show() 2.",
        "url": "/bricks/PySpark-SQL-Functions-Parquet.html"
    },
    {
        "id": 25,
        "title": "Medallion-Architecture.html",
        "content": "{% include menu.html title=\"Medallion Architecture in Delta Lake\" %} Medallion Architecture in Delta Lake Overview of Medallion Architecture The Medallion Architecture is a layered approach used in Delta Lake to optimize data quality and performance as data progresses through various stages. It divides the data into three primary layers, referred to as Bronze, Silver, Gold tiers, each representing different levels of data quality, transformation, and availability.",
        "url": "/bricks/Medallion-Architecture.html"
    },
    {
        "id": 26,
        "title": "Medallion-Architecture-Partitioning-Code.html",
        "content": "{% include menu.html title=\"Medallion Architecture with 256 Node Cluster\" %} Medallion Architecture with Partitioning spark = SparkSession.builder \\  . appName ( \"Medallion Architecture\" ) \\  .",
        "url": "/bricks/Medallion-Architecture-Partitioning-Code.html"
    },
    {
        "id": 27,
        "title": "Managed-External-Tables.html",
        "content": "{% include menu.html title=\"Managed and External Tables in Delta Lake\" %} Managed and External Tables in Delta Lake Managed Table Storage Location : Delta Lake automatically manages both the data and metadata. The data is stored in a location controlled by the Delta Lake system.",
        "url": "/bricks/Managed-External-Tables.html"
    },
    {
        "id": 28,
        "title": "Column-Shuffle-Repartition.html",
        "content": "{% include menu.html title=\"Repartition in PySpark\" %} Repartition in PySpark How Repartitioning Works Shuffling Data: When you call repartition(256) , Spark performs a full shuffle of the data across the specified number of partitions. The goal is to redistribute the data evenly across all partitions, which allows parallel processing across multiple nodes.",
        "url": "/bricks/Column-Shuffle-Repartition.html"
    },
    {
        "id": 29,
        "title": "Optimizing-Join-Queries.html",
        "content": "{% include menu.html title=\"Optimizing Multiple Join Queries in Legacy Data Warehousing\" %} Optimizing Multiple Join Queries in Legacy Data Warehousing When dealing with multiple join queries in a legacy data warehousing environment, performance optimization is crucial, especially given the constraints that might be present, such as older hardware, less flexible architectures, or limited scalability. Here are key considerations and steps to optimize performance: 1.",
        "url": "/bricks/Optimizing-Join-Queries.html"
    },
    {
        "id": 30,
        "title": "Relational-Databases.html",
        "content": "{% include menu.html title=\"Relational Databases and Data Warehousing\" %} Relational Databases and Data Warehousing 1. Relational Databases Relational databases are structured to store data in tables (or relations) where rows represent records and columns represent attributes.",
        "url": "/bricks/Relational-Databases.html"
    },
    {
        "id": 31,
        "title": "PySpark-Lazy-Evaluation.html",
        "content": "{% include menu.html title=\"Lazy Evaluation in PySpark\" %} Lazy Evaluation in PySpark Lazy evaluation is a key concept in PySpark (and Spark in general) that refers to the deferred execution of operations until an action is triggered. This means that when you define transformations on your data, PySpark doesn\u2019t immediately execute them.",
        "url": "/bricks/PySpark-Lazy-Evaluation.html"
    },
    {
        "id": 32,
        "title": "Delta-Live-Tables.html",
        "content": "{% include menu.html title=\"Delta Live Tables (DLT) in Databricks\" %} Delta Live Tables (DLT) in Databricks Delta Live Tables (DLT) in Databricks is a framework for building reliable, scalable, and simple data pipelines. It is built on top of Delta Lake and simplifies creating, managing, and monitoring data pipelines.",
        "url": "/bricks/Delta-Live-Tables.html"
    },
    {
        "id": 33,
        "title": "PySpark-Handling-Missing-Data.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak Github Pages\" %} PySpark is the Python API for Apache Spark, an open source, distributed computing system designed for processing large scale data. PySpark enables Python developers to write Spark applications using the popular Python programming language, offering a powerful framework for big data processing and analytics.",
        "url": "/bricks/PySpark-Handling-Missing-Data.html"
    },
    {
        "id": 34,
        "title": "PySpark-Questions-Answers.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak Github Pages\" %} PySpark Questions and Answers What is PySpark? PySpark is the Python API for Apache Spark, an open source distributed computing system.",
        "url": "/bricks/PySpark-Questions-Answers.html"
    },
    {
        "id": 35,
        "title": "{{ include.title }}",
        "content": "{{ include.title }} Machine Learning Programming Databricks AWS Data DevOps Search Github",
        "url": "/_includes/menu.html"
    },
    {
        "id": 36,
        "title": "footer.html",
        "content": "\u00a9 Kevin Luzbetak",
        "url": "/_includes/footer.html"
    },
    {
        "id": 37,
        "title": "debugging-kubernetes-performance.html",
        "content": "{% include menu.html title=\"Debugging Kubernetes Performance\" %} Debugging Kubernetes Performance Debugging performance issues in a Kubernetes environment can be complex due to the distributed nature of applications and the variety of components involved. Here are key steps and tools to help you identify and resolve performance problems in Kubernetes: 1.",
        "url": "/devops/debugging-kubernetes-performance.html"
    },
    {
        "id": 38,
        "title": "github.html",
        "content": "{% include menu.html title=\"GitHub Overview\" %} GitHub Overview What is GitHub? GitHub is a web based platform that provides hosting for software development and version control using Git.",
        "url": "/devops/github.html"
    },
    {
        "id": 39,
        "title": "general-101.html",
        "content": "{% include menu.html title=\"PySpark and Databricks Deep Dive 101\" %} PySpark and Databricks Deep Dive 101 1. PySpark Overview PySpark is the Python API for Apache Spark, an open source distributed computing system.",
        "url": "/devops/general-101.html"
    },
    {
        "id": 40,
        "title": "apache-nifi.html",
        "content": "{% include menu.html title=\"Apache NiFi\" %} Apache NiFi Overview Apache NiFi is an open source data integration tool designed to automate the flow of data between systems. It provides a user friendly web based interface that allows users to design, monitor, and control data flows through a visual programming approach.",
        "url": "/devops/apache-nifi.html"
    },
    {
        "id": 41,
        "title": "docker.html",
        "content": "{% include menu.html title=\"Key Points about Docker\" %} Key Points about Docker Containerization : Docker enables the creation and management of containers, which are lightweight, portable, and isolated environments that bundle an application and its dependencies together. This ensures consistency across different environments, such as development, testing, and production.",
        "url": "/devops/docker.html"
    },
    {
        "id": 42,
        "title": "apache-airflow.html",
        "content": "{% include menu.html title=\"Apache Airflow Overview\" %} Apache Airflow Apache Airflow is an open source platform used to programmatically author, schedule, and monitor workflows. It allows you to define your workflows as Directed Acyclic Graphs (DAGs) using Python, where each node in the graph represents a task.",
        "url": "/devops/apache-airflow.html"
    },
    {
        "id": 43,
        "title": "Kubernetes.html",
        "content": "{% include menu.html title=\"Kubernetes Overview\" %} Kubernetes Overview What is Kubernetes? Kubernetes, often abbreviated as K8s, is an open source container orchestration platform that automates the deployment, scaling, and management of containerized applications.",
        "url": "/devops/Kubernetes.html"
    },
    {
        "id": 44,
        "title": "software-delivery.html",
        "content": "{% include menu.html title=\"Software Delivery  CI/CD Tools\" %} Software Delivery Software delivery refers to the process of developing, testing, and deploying software applications to end users or production environments. It encompasses the entire lifecycle, from initial development and coding through quality assurance, staging, and finally, deployment.",
        "url": "/devops/software-delivery.html"
    },
    {
        "id": 45,
        "title": "bashrc.html",
        "content": "{% include menu.html title=\"bashrc\" %} Search Code # # # search one two # search tpy one two # # function search() {  if [ $# eq 0 ]; then   echo \"Please provide at least one keyword as an argument.\" return 1  fi  if [[ $1 == * ]]; then   local cmd=\"rg ${1} l '${2}'\"  else   local cmd=\"rg l '${1}'\"  fi  shift  for keyword in \"$@\"; do   cmd+=\" | xargs I{} rg l '${keyword}' {}\"  done  eval $cmd } # # # Search all the python file for keyword open # spy open py # # function spy() {  if [ $# eq 0 ]; then   echo \"Please provide a keyword as the first argument.\"",
        "url": "/devops/bashrc.html"
    },
    {
        "id": 46,
        "title": "Python-Search-Algorithms.html",
        "content": "{% include menu.html title=\"Python Algorithms\" %} Python Search Algorithms Find the Intersection of Two Lists def list_intersection(list1: list, list2: list) > list:   return list(set(list1) & set(list2)) print(list_intersection([1, 2, 3, 4], [3, 4, 5, 6])) # Output: [3, 4] Binary Search The binary search algorithm works by repeatedly dividing the array into two halves until the target element is found or a single element remains. In each iteration, we compare the middle element of the current half with the target element.",
        "url": "/programming/Python-Search-Algorithms.html"
    },
    {
        "id": 47,
        "title": "Python-String-Algorithms.html",
        "content": "{% include menu.html title=\"Python Algorithms\" %} Python String Algorithms Reverse a String def reverse_string(s: str) > str:     return s[:: 1]   print(reverse_string(\"hello\")) # Output: \"olleh\" Count the Number of Vowels in a String def count_vowels(s: str) > int:   vowels = 'aeiouAEIOU'   return sum(1 for char in s if char in vowels)    print(count_vowels(\"hello world\")) # Output: 3 Find the First Non Repeated Character in a String def first_non_repeated_char(s: str) > str:   counts = {}   for char in s:     counts[char] = counts.get(char, 0) + 1   for char in s:     if counts[char] == 1:       return char   return None print(first_non_repeated_char(\"swiss\")) # Output: \"w\" Longest Common Subsequence The longest_common_subsequence function calculates the length and the actual sequence of the longest common subsequence (LCS) between two input strings. It uses dynamic programming to build a 2D table (dp) that tracks the LCS length up to each pair of indices.",
        "url": "/programming/Python-String-Algorithms.html"
    },
    {
        "id": 48,
        "title": "Python-Coding-Exercise-Algorithms.html",
        "content": "{% include menu.html title=\"Python Coding Exercise Algorithms\" %} Python Coding Exercise Algorithms 1. Reverse a String def reverse_string(s: str ) > str : return s[:: 1] # Example usage print(reverse_string( \"hello\" )) # Output: \"olleh\" 2.",
        "url": "/programming/Python-Coding-Exercise-Algorithms.html"
    },
    {
        "id": 49,
        "title": "101.html",
        "content": "{% include menu.html title=\"Python Algorithms\" %} Python Algorithms Two Sum Problem This Python function two_sum is designed to solve the \"two sum\" problem. The problem is to find two numbers in a list (nums) that add up to a specific target value (target).",
        "url": "/programming/101.html"
    },
    {
        "id": 50,
        "title": "Python-Programming-Language.html",
        "content": "{% include menu.html title=\"Python Programming Language\" %} Python Programming Language 1. Data Structures Lists, Tuples, Sets, and Dictionaries: Understand the properties and use cases of each.",
        "url": "/programming/Python-Programming-Language.html"
    },
    {
        "id": 51,
        "title": "mysql-lag-function.html",
        "content": "{% include menu.html title=\"MySQL LAG Function\" %} MySQL LAG Function Explanation Overview The LAG function in MySQL is a window function that allows you to access data from a previous row within the same result set. It is particularly useful for calculating differences between sequential rows, such as in time series analysis.",
        "url": "/programming/mysql-lag-function.html"
    },
    {
        "id": 52,
        "title": "Python Functions Example",
        "content": "{% include menu.html title=\"Python CSV Data Structures\" %} Python CSV Data Structures Reading CSV Data into Different Data Structures import csv # Sample CSV data with open ( 'data.csv' , 'r' ) as file:   reader = csv.reader(file) # Skipping the header next (reader) # Reading into a list data_list = [row for row in reader] with open ( 'data.csv' , 'r' ) as file:   reader = csv.reader(file) next (reader) # Reading into a tuple data_tuple = tuple (reader) with open ( 'data.csv' , 'r' ) as file:   reader = csv.reader(file) next (reader) # Reading into a set data_set = { tuple (row) for row in reader} with open ( 'data.csv' , 'r' ) as file:   reader = csv.DictReader(file) # Reading into a dictionary data_dict = [row for row in reader] print ( 'List:' , data_list) print ( 'Tuple:' , data_tuple) print ( 'Set:' , data_set) print ( 'Dictionary:' , data_dict) Explanation This Python program demonstrates how to read data from a CSV file and load it into different data structures like Lists, Tuples, Sets, and Dictionaries. List: A dynamic array that holds the rows of the CSV as individual list elements.",
        "url": "/programming/Python-Function-OOP-Data-Structure.html"
    },
    {
        "id": 53,
        "title": "Sudoku-Board-Verification.html",
        "content": "{% include menu.html title=\"Sudoku Board Verification\" %} Sudoku Board Verification  python import pprint # # # Define a 9x9 matrix representing a Sudoku board matrix1 = [   [7, 1, 8, 5, 3, 2, 9, 4, 6],   [5, 3, 2, 6, 9, 4, 1, 8, 7],   [6, 9, 4, 7, 1, 8, 3, 2, 5],   [1, 2, 7, 3, 4, 5, 8, 6, 9],   [9, 8, 6, 1, 2, 7, 4, 5, 3],   [3, 4, 5, 9, 8, 6, 2, 7, 1],   [4, 6, 3, 8, 7, 9, 5, 1, 2],   [8, 7, 9, 2, 5, 1, 6, 3, 4],   [2, 5, 1, 4, 6, 3, 7, 9, 8], ] # # def verify_sudoku_board(board, value):   \"\"\"   Verifies whether a given 9x9 Sudoku board is valid by checking that   each row, column, and 3x3 sub grid (window) sums to a specified value. Args:     board (list of list of int): The 9x9 Sudoku board represented as a list of lists.",
        "url": "/programming/Sudoku-Board-Verification.html"
    },
    {
        "id": 54,
        "title": "Python-Algorithms.html",
        "content": "{% include menu.html title=\"Python Algorithms\" %} Python Algorithms Remove Duplicates from a List def remove_duplicates(nums: list) > list:   return list(set(nums))   print(remove_duplicates([1, 2, 2, 3, 4, 4, 5])) # Output: [1, 2, 3, 4, 5] Two Sum The two sum algorithm works by iterating through the array and for each element, finding its complement (i.e., the other number that adds up to the target value). We use a nested loop to iterate through the remaining elements of the array and check if their sum is equal to the target value.",
        "url": "/programming/Python-Algorithms.html"
    },
    {
        "id": 55,
        "title": " Hamming and Levenshtein distances ",
        "content": "Hamming and Levenshtein distances # Import the necessary libraries import math from collections import Counter import numpy as np def hamming_distance(str1, str2):   \"\"\"Calculate the Hamming distance between two strings. Args:     str1 (string): The first string for comparison.",
        "url": "/programming/algorithms2.html"
    },
    {
        "id": 56,
        "title": "Python Code for 12 Popular Algorithms",
        "content": "Python Code for 12 Popular Algorithms Python Code for 12 Popular Algorithms in Coding Interviews The following Python code implements the 12 most popular algorithms commonly asked in coding interviews: Binary Search : Implement a binary search algorithm to find an element in a sorted array. Two Sum : Given an array of integers, find two elements that add up to a specific target value.",
        "url": "/programming/algorithms.html"
    },
    {
        "id": 57,
        "title": "Fibonacci-Generator.html",
        "content": "{% include menu.html title=\"Kevin Luzbetak Github Pages\" %} Fibonacci Generator def fibonacci_generator():   a, b = 0, 1   while True:     yield a     a, b = b, a + b fib_gen = fibonacci_generator() # Generate and print the first 10 Fibonacci numbers for _ in range(10):   print(next(fib_gen)) Fibonacci Generator Explained This Python code defines a generator function that produces Fibonacci numbers indefinitely: Function Definition : The function fibonacci_generator() is defined to yield an infinite sequence of Fibonacci numbers. Yield Statement : The yield statement produces the current value of a and pauses the function's execution, preserving its state for the next iteration.",
        "url": "/programming/Fibonacci-Generator.html"
    },
    {
        "id": 58,
        "title": "Amazon-RDS.html",
        "content": "{% include menu.html title=\"Amazon RDS\" %} Amazon RDS (Relational Database Service) Amazon RDS (Relational Database Service) is a managed relational database service provided by AWS that simplifies the setup, operation, and scaling of a relational database in the cloud. It supports several popular database engines, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server.",
        "url": "/aws/Amazon-RDS.html"
    },
    {
        "id": 59,
        "title": "AWS-CloudWatch.html",
        "content": "{% include menu.html title=\"AWS CloudWatch\" %} AWS CloudWatch AWS CloudWatch is a comprehensive monitoring and observability service provided by Amazon Web Services (AWS). It is designed to help you monitor and track the performance of your applications, infrastructure, and services running on AWS and on premises environments.",
        "url": "/aws/AWS-CloudWatch.html"
    },
    {
        "id": 60,
        "title": "Amazon-S3.html",
        "content": "{% include menu.html title=\"AWS S3\" %} Amazon S3 (Simple Storage Service) is a scalable object storage service provided by Amazon Web Services (AWS). It is designed for storing and retrieving any amount of data from anywhere on the internet, offering a range of features that make it suitable for a wide variety of use cases, from data backup to serving large scale applications.",
        "url": "/aws/Amazon-S3.html"
    },
    {
        "id": 61,
        "title": "ETL-Pipeline-AWS.html",
        "content": "{% include menu.html title=\"Building an ETL Pipeline on AWS\" %} Building an ETL Pipeline on AWS 1. Data Ingestion (Extract) AWS S3 (Simple Storage Service): Store raw data in S3 buckets.",
        "url": "/aws/ETL-Pipeline-AWS.html"
    },
    {
        "id": 62,
        "title": "AWS-EMR.html",
        "content": "{% include menu.html title=\"AWS EMR (Elastic MapReduce)\" %} AWS EMR (Elastic MapReduce) AWS EMR (Elastic MapReduce) is a cloud based big data platform that provides a managed Hadoop framework, enabling you to process and analyze vast amounts of data quickly and cost effectively. It allows you to run big data frameworks like Apache Hadoop, Apache Spark, HBase, Presto, Flink, and others on the AWS cloud.",
        "url": "/aws/AWS-EMR.html"
    },
    {
        "id": 63,
        "title": "AWS-Glue-Workflow.html",
        "content": "{% include menu.html title=\"Amazon AWS\" %} AWS Glue Workflow AWS Glue Workflow is a feature of AWS Glue that allows you to create and manage complex ETL (Extract, Transform, Load) workflows. It helps you orchestrate multiple ETL jobs and crawlers in a sequence or in parallel, enabling you to automate and manage the flow of data through your ETL processes.",
        "url": "/aws/AWS-Glue-Workflow.html"
    },
    {
        "id": 64,
        "title": "AWS-Kinesis-Data-Streams.html",
        "content": "{% include menu.html title=\"AWS Kinesis or AWS Data Streams\" %} AWS Kinesis or AWS Data Streams AWS Kinesis and AWS Data Streams refer to the same service, with \"AWS Kinesis Data Streams\" being the full name. It's a service designed for real time data streaming, allowing you to collect, process, and analyze data as it arrives.",
        "url": "/aws/AWS-Kinesis-Data-Streams.html"
    },
    {
        "id": 65,
        "title": "AWS-Step-Functions.html",
        "content": "{% include menu.html title=\"AWS Step Functions\" %} AWS Step Functions AWS Step Functions is a serverless orchestration service that allows you to sequence AWS services and automate business processes. It enables you to build and run complex workflows by defining a state machine in which each step (or state) performs a task, such as invoking an AWS Lambda function, making API calls, or waiting for human input.",
        "url": "/aws/AWS-Step-Functions.html"
    },
    {
        "id": 66,
        "title": "AWS.html",
        "content": "{% include menu.html title=\"AWS Glue Data Catalog\" %} AWS Glue Data Catalog The AWS Glue Data Catalog is a centralized metadata repository that stores information about data sources, such as databases, tables, and schemas, in your AWS environment. It is a core component of AWS Glue, designed to make it easier to organize, discover, and manage data for your ETL (Extract, Transform, Load) processes.",
        "url": "/aws/AWS.html"
    },
    {
        "id": 67,
        "title": "AWS-Config-Inspector.html",
        "content": "{% include menu.html title=\"AWS Config Inspector\" %} AWS Config AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.",
        "url": "/aws/AWS-Config-Inspector.html"
    },
    {
        "id": 68,
        "title": "KMS-Key-Management-Service.html",
        "content": "{% include menu.html title=\"KMS  Key Management Service\" %} AWS Key Management Service (KMS) AWS Key Management Service (KMS) is a managed service that enables you to create, control, and manage encryption keys used to secure your data across AWS services and applications. KMS integrates seamlessly with various AWS services to provide a unified and consistent approach to encryption, making it easier to protect sensitive data in the cloud.",
        "url": "/aws/KMS-Key-Management-Service.html"
    },
    {
        "id": 69,
        "title": "Lambda-Serverless-Computing.html",
        "content": "{% include menu.html title=\"AWS Lambda\" %} AWS Lambda: AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS). It allows you to run code without provisioning or managing servers, enabling you to build applications that respond quickly to new information.",
        "url": "/aws/Lambda-Serverless-Computing.html"
    },
    {
        "id": 70,
        "title": "Auto-Scaling-EC2-EMR.html",
        "content": "{% include menu.html title=\"Auto Scaling EC2 EMR\" %} Auto Scaling for EC2/EMR Auto Scaling for Amazon EC2 and EMR (Elastic MapReduce) is a service that automatically adjusts the number of EC2 instances or EMR cluster nodes in your application or data processing environment based on the current demand. This ensures that you have the right amount of resources to handle the load while optimizing cost efficiency by scaling down when demand is low.",
        "url": "/aws/Auto-Scaling-EC2-EMR.html"
    },
    {
        "id": 71,
        "title": "AWS-CloudWatch-Events.html",
        "content": "{% include menu.html title=\"AWS CloudWatch Events\" %} AWS CloudWatch Events AWS CloudWatch Events is a service that delivers a near real time stream of system events that describe changes in AWS resources. It enables you to respond to these changes by triggering functions, running scripts, or making API calls, making it a powerful tool for automating your cloud infrastructure.",
        "url": "/aws/AWS-CloudWatch-Events.html"
    },
    {
        "id": 72,
        "title": "AWS-Glue-ETL-Service.html",
        "content": "{% include menu.html title=\"AWS Glue\" %} AWS Glue AWS Glue is a fully managed extract, transform, and load (ETL) service provided by Amazon Web Services (AWS). It is designed to simplify the process of moving, transforming, and preparing data for analytics.",
        "url": "/aws/AWS-Glue-ETL-Service.html"
    },
    {
        "id": 73,
        "title": "S3-Transfer-Acceleration.html",
        "content": "{% include menu.html title=\"S3 Transfer Acceleration\" %} S3 Transfer Acceleration Amazon S3 Transfer Acceleration is a feature that enables fast, easy, and secure transfers of files over long distances between your clients and an S3 bucket. Transfer Acceleration leverages Amazon CloudFront's globally distributed edge locations to accelerate data transfer by routing your uploads to the closest edge location, which then routes the data to Amazon S3 over optimized network paths.",
        "url": "/aws/S3-Transfer-Acceleration.html"
    },
    {
        "id": 74,
        "title": "IAM-Identity-Access-Management.html",
        "content": "{% include menu.html title=\"IAM  Identity Access Management\" %} AWS Identity and Access Management (IAM) AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. IAM enables you to manage users, groups, and roles, and set permissions to allow or deny access to specific AWS services and resources.",
        "url": "/aws/IAM-Identity-Access-Management.html"
    },
    {
        "id": 75,
        "title": "AWS-Glue-Data-Catalog.html",
        "content": "{% include menu.html title=\"AWS Glue Data Catalog\" %} AWS Glue Data Catalog The AWS Glue Data Catalog is a centralized metadata repository that stores information about data sources, such as databases, tables, and schemas, in your AWS environment. It is a core component of AWS Glue, designed to make it easier to organize, discover, and manage data for your ETL (Extract, Transform, Load) processes.",
        "url": "/aws/AWS-Glue-Data-Catalog.html"
    },
    {
        "id": 76,
        "title": "AWS-CloudTrail.html",
        "content": "{% include menu.html title=\"AWS CloudTrail\" %} AWS CloudTrail AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.",
        "url": "/aws/AWS-CloudTrail.html"
    },
    {
        "id": 77,
        "title": "AWS-Redshift.html",
        "content": "{% include menu.html title=\"AWS Redshift\" %} AWS Redshift AWS Redshift is a fully managed, petabyte scale data warehouse service in the cloud. It allows you to run complex analytical queries against structured and semi structured data using standard SQL.",
        "url": "/aws/AWS-Redshift.html"
    },
    {
        "id": 78,
        "title": "Apache-Parquet.html",
        "content": "{% include menu.html title=\"Apache Parquet Format\" %} Apache Parquet Format Apache Parquet is a columnar storage file format optimized for use with data processing systems like Apache Hadoop, Apache Spark, and cloud based data lakes. It is highly efficient for large scale data storage and retrieval, especially for analytic workloads.",
        "url": "/data/Apache-Parquet.html"
    },
    {
        "id": 79,
        "title": "Apache-Iceberg.html",
        "content": "{% include menu.html title=\"Apache Iceberg\" %} Apache Iceberg Apache Iceberg is a modern data management framework built to support large scale data lakes with ACID transactions and flexible partitioning, making it highly scalable and efficient for managing complex datasets. Key Features: ACID Transactions: Iceberg ensures atomic, consistent, isolated, and durable (ACID) operations on data lakes, enabling reliable updates, deletes, and upserts.",
        "url": "/data/Apache-Iceberg.html"
    },
    {
        "id": 80,
        "title": "Graph-Databases-ArgoDB-Neo4j.html",
        "content": "{% include menu.html title=\"ArgoDB\" %} ArgoDB ArgoDB is a specialized distributed database optimized for complex, large scale graph data. It is designed to handle high performance graph processing workloads, providing efficient querying and analysis of relationships within data, such as social networks, fraud detection, and recommendation systems.",
        "url": "/data/Graph-Databases-ArgoDB-Neo4j.html"
    },
    {
        "id": 81,
        "title": "GraphQL.html",
        "content": "{% include menu.html title=\"GraphQL\" %} GraphQL GraphQL is a query language for APIs, developed by Facebook in 2012, designed to make data fetching more efficient and flexible. It allows clients to request exactly the data they need, minimizing over fetching or under fetching common with REST APIs.",
        "url": "/data/GraphQL.html"
    },
    {
        "id": 82,
        "title": "Large-Scale-Data-Ingestion2.html",
        "content": "{% include menu.html title=\"Large Scale Data Ingestion Tools\" %} Large Scale Data Ingestion Tools Apache Kafka Kafka is a distributed streaming platform widely used for building real time data pipelines and streaming applications. It supports both real time and batch data ingestion, handling large amounts of event data from multiple sources.",
        "url": "/data/Large-Scale-Data-Ingestion2.html"
    },
    {
        "id": 83,
        "title": "Kafka-Producer-Consumer.html",
        "content": "{% include menu.html title=\"Apache Kafka Producer and Consumer\" %} Apache Kafka Producer and Consumer Kafka Producer from kafka import KafkaProducer import json # Initialize the Kafka producer producer = KafkaProducer(   bootstrap_servers=['localhost:9092'],   value_serializer=lambda v: json.dumps(v).encode('utf 8') ) # Send a message to the Kafka topic 'test_topic' producer.send('test_topic', {'key': 'value'}) # Ensure all messages are sent before closing the producer producer.flush() producer.close() Description KafkaProducer: Initializes the producer with a list of Kafka brokers. Here, it connects to localhost:9092 .",
        "url": "/data/Kafka-Producer-Consumer.html"
    },
    {
        "id": 84,
        "title": "sql-statements.html",
        "content": "{% include menu.html title=\"SQL Statements\" %} SQL Statements SQL Description INSERT Used to insert new rows into a table. UPDATE Used to modify existing rows in a table.",
        "url": "/data/sql-statements.html"
    },
    {
        "id": 85,
        "title": "Apache-Hudi.html",
        "content": "{% include menu.html title=\"Apache Hudi\" %} Apache Hudi Apache Hudi (Hadoop Upserts Deletes and Incrementals) is an open source data management framework that simplifies large scale data ingestion and provides ACID transaction support on data lakes. It\u2019s designed for scenarios that require efficient data upserts (updates and inserts) and deletes in big data environments, while also enabling near real time ingestion and querying of data.",
        "url": "/data/Apache-Hudi.html"
    },
    {
        "id": 86,
        "title": "sql-create-view.html",
        "content": "{% include menu.html title=\"SQL Overview\" %} SQL CREATE VIEW Statement SQL  Create a Simple View This query creates a view named employee_salaries that shows employee names and their salaries from the employees table. CREATE VIEW employee_salaries AS SELECT employee_name , salary FROM employees ; SQL  Create a View with Joins This query creates a view named department_salary_view that shows department names along with the total salaries for each department.",
        "url": "/data/sql-create-view.html"
    },
    {
        "id": 87,
        "title": "Query-Performance.html",
        "content": "{% include menu.html title=\"SQL Query Optimization\" %} SQL Query Optimization 1. Importance of SQL Query Optimization Performance Improvement  Optimized queries run faster, which is crucial when working with large datasets in data warehouses or operational databases.",
        "url": "/data/Query-Performance.html"
    },
    {
        "id": 88,
        "title": "sql_overview.html",
        "content": "Common Types of SQL The most common types of SQL (Structured Query Language) are used for managing and manipulating relational databases. SQL commands are broadly categorized based on their functionality: 1.",
        "url": "/data/sql_overview.html"
    },
    {
        "id": 89,
        "title": "Apache-Kafka.html",
        "content": "{% include menu.html title=\"Apache Kafka\" %} Apache Kafka Distributed Architecture: Kafka is designed to be distributed across multiple servers, offering high availability, fault tolerance, and scalability. Publish Subscribe Messaging System: Kafka allows multiple producers to publish messages to topics, which consumers can subscribe to, enabling decoupled communication between different parts of an application Topics and Partitions: Data is organized into topics, which are further divided into partitions.",
        "url": "/data/Apache-Kafka.html"
    },
    {
        "id": 90,
        "title": "data-warehouse-architecture.html",
        "content": "{% include menu.html title=\"Snowflake Data Warehouse Architecture\" %} Data Warehouse Architecture: Inmon (Top Down): Centralized, normalized enterprise data warehouse design for scalable and flexible data integration. Kimball (Bottom Up): Dimensional modeling approach using denormalized data marts optimized for fast querying and reporting.",
        "url": "/data/data-warehouse-architecture.html"
    }
]