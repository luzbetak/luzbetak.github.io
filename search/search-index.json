[
    {
        "id": 1,
        "title": "TF-IDF.html",
        "content": "\" \", \"machine # % 'document (term + 0.0 0.3742 0.492 1 1| 2 2| 3 4 : = [\"document [round(value ] a all and another both breakdown column content corpus data decimals distinguishing doc document document\" documents entire enumerate(docs example f\"document feature footer.html frequency high higher highest i idf import importance initialize inverse it its learning learning| limit list(feature_names low lower machine many matrix menu.html model multiple names other particular practice prettytable print(table range ranking result row rows sample score scores sklearn.feature_extraction.text specific summary table table.add_row(row) term terms text tf tfidf_array[i tfidf_matrix tfidfvectorizer that the their them theory these they this title=\"tf uniqueness upper value values vectorizer vectorizer.fit_transform(docs which word words |",
        "url": "/TF-IDF.html"
    },
    {
        "id": 2,
        "title": "Retrieval-Augmented-Generation.html",
        "content": "\"another % (document_text (retrieval ) + = =postgres [\"your a accurate algorithm an and augment augmented base bert chatbot components conn conn.commit conn.cursor contextual cur d database datasets dimensional document documents documents:\\n{retrieved_documents}\\nanswer e.g., embeddings extension external following footer.html generate generated generation generation) generator gpt gpt2lmheadmodel gpt2lmheadmodel.from_pretrained('gpt2 gpt2tokenizer high import indexing information informed insert it key knowledge large limit management menu.html model model.encode(documents model.generate(inputs['input_ids models more most multiple necessary nodes optimization order parallel performance pgvector pgvectorscale postgresql primary print(response process psycopg2 psycopg2.connect(\"dbname=test python queries query question rag references relevant repositories response responses retrieval retrieve retrieved s scale searches select sentence_transformers serial similarity solution step storage store systems table tasks text that the these title=\"rag tokenizer(prompt transformers tutorial user values vector vector(768 vectors you your {%",
        "url": "/Retrieval-Augmented-Generation.html"
    },
    {
        "id": 3,
        "title": "Tensors-Machine-Learning.html",
        "content": "# % ( (summation 0 0]) 1 1, 2 2d 3 3] 3d 4d 6 = `np.mean()` `np.sum()` `np.zeros()` a access additional all an and any array average building channels code color column complex computation computations data deep depth dimensional dimensions e.g. efficient element elements essence even example excellent footer.html foundation frameworks function functionality functions fundamental generalization gpus height high images import index indexing it key language learning library list machine matrices matrix matrix[1 mean menu.html models more multi multidimensional natural ndarrays np np.array np.array([[1 np.array([[[1 np.mean(tensor) np.sum np.sum(matrix np.sum(tensor) np.zeros((2 number numbers numerical numpy operation operations output points powerful print(\"element print(\"sum print(average print(tensor print(total print(value python pytorch row scalar simple single solid some spaces specific structure sum sum_of_matrix support syntax table tensor tensor_3d tensor_3d[0, tensorflow tensors the they this those time title=\"tensors total training us value values vector vectors videos we which width words",
        "url": "/Tensors-Machine-Learning.html"
    },
    {
        "id": 4,
        "title": "Random-Forest-Classifier-Model.html",
        "content": "\"billing \"forest \"order \"predicted \"product # % %} 'billing 'product ( * + , 100:.2f}% 30% 53.33% 70% : = [\"customer ]]) a account accuracy address all an and any approach authentication automation balance billing card case categories category charge classification classifier classifieri clf clf.fit(x_train_vec clf.predict(new_question_vec code collection columns=['question credentials cryptocurrency customer damaged data dataframe(list(dictionary.items date decision delivery depth details device df df['category dictionary different discount each either email factor feature features final footer.html forest generalization generator gift history hyperparameters i idf import incoming individual information input internal invoice it item labels leaf length lists login manual max_depth=20 maximum menu.html method methods min_samples_leaf=2 minimum missing model model\" model\u2019s my n_estimators=200 nearest new new_question node notifications number numerical options order output outstanding pandas parameter password payment pd pin plan policy predicted predicted_category prediction predictions prettytable print(table product proportion purchase python queries query question questions random random_state=42 random_state=42) randomforestclassifier(n_estimators=100 randomforestclassifier(n_estimators=100, refund remaining reproducibility reset return same samples seed set shipping sklearn.ensemble sklearn.feature_extraction.text sklearn.metrics sklearn.model_selection split step store subscription subset support support' supported table table.field_names technical test testing text textual tf tfidfvectorizer that the them these this train train_test_split training tree trees two user value various vectorizer vectorizer.fit_transform(x_train warranty what which x_test x_test_vec x_train_vec y_test y_train you {accuracy | }",
        "url": "/Random-Forest-Classifier-Model.html"
    },
    {
        "id": 5,
        "title": "Scikit-learn.html",
        "content": "# % (features , 0 1 150 2 20% 80% = a accuracy accuracy:.2f actions actual algorithms all allows analysis and any apis auc beginners best both box bunch calculate categorical classifier clf clf.fit(x_train code comparison corresponding cross data data.data dataset datasets dbscan decision dictionary dimensionality documented efficient external features few files flowers following footer.html forest forests function good grid high hyperparameter i identical import initializes integration iris it joblib key label labels later learning learning: length libraries library linear load_iris load_iris( load_iris() machine machines matplotlib menu.html metrics missing model modeling models no normalization numpy object of open other output overview pandas pca performance persistence: petal pickle points popular precision predicted predictions predicts preprocessing print(\"\\nsample print(f\"accuracy professionals python random random_state=42 randomforestclassifier(n_estimators=100 range range(5 recall reduction regression results roc sample samples science scikit scipy search selection selection: sepal set sets setup simple sklearn.datasets sklearn.ensemble sklearn.metrics sklearn.model_selection sne some source specific summary supervised svm t target test test_size=0.2 testing the this title=\"scikit tools top toy train train_test_split trained training trees unsupervised use used validation values variables variety vector well which wide widely width x_test y y_pred[i y_test y_train yes you zip(y_test {x_test[i {y_test[i]}\")",
        "url": "/Scikit-learn.html"
    },
    {
        "id": 6,
        "title": "Vector-Database.html",
        "content": "\"example \"vector # % 'faiss () (facebook (inverted .search() 1000 128 128).tolist()[0 3 5 = a abnormal accuracy ai an and angular annoy annoyindex annoyindex(f anomalies anomaly api apis applications associated audio behavior bert billions both case cases characteristics classification client clustering code collection collection.insert(data collection.search(query_vector).limit(5).to_list common connect connection content control count cpu creates critical d d)).astype('float32 data database databases datasets db db) dbscan dense detection dimension dimension=128) dimensional dimensionality distance distances documents driven each ecosystem efficiency efficient embeddings enumerate(vectors environment=\"us environments essential euclidean event example explanation external f facebook faiss fast fast, faster feature features file first flat focuses footer.html format full function gcp generated gpt gpu high hnsw hosted hosting i id ids image images import index index\" index(index_name) index.add(vectors index.add_item(i index.query(queries=[query_vector index.upsert(vectors index_name indexflatl2(d insert inserts install integration it item items its ivf just k k=5 key keywords l2 labels lancedb lancedb.connect(\"/path/to/lancedb language large latency learning library local low machine management manhattan match match['id']}, match['score meaning means memory menu.html metadata metric metrics ml models modern most natural nearest nearest_neighbors need needs neighbor neighbors new nlp nn no np np.random.rand(1 np.random.rand(128).tolist np.random.rand(128).tolist() np.random.random((1 np.random.random((1000 np.random.random(f).tolist numerical numpy of open or other outliers overview path perform performance pinecone pinecone.create_index(index_name, pinecone.delete_index(index_name pinecone.init(api_key=\"your_api_key\", pipelines points print(\"nearest print(f\"id processing production products queries query querying random randomly range range(1000 real recommend recommendation regression result result['distance results results['matches retrieval retrieve role scalability scale scales scenarios score search search) search: searches self semantic semantically services similar similarity situations small some source space specialized specified spotify steps storage store supports systems table tasks text the their these this throughput time title=\"vector top traditional transaction tree type types unusual usage use user users various vector vectors version very visual west1 where which wide workloads world you {\"i {result['id']},",
        "url": "/Vector-Database.html"
    },
    {
        "id": 7,
        "title": "Keras.html",
        "content": "\"feedforward # % ' () (a (e.g., 1) 1, 10 1d 20 28, 28x28 2d 3 = a accessible accuracy accuracy:.2f activation activation='softmax adam algorithms an analysis and any api artificial backpropagation basic batch_size=64 best biases binary blocks both building case case: channel channels churn class classes classification cnn cnns cntk code colored complex complexities complexity components computation computational connected connections consistent conv2d convolution convolutional core corners correct cpu customer cycle data dataset decision deep dense detection developers diagnosis differences different digit digits dimensional dimensions direction e.g., each edges either error example facial faster feature features feedback feedforward filters final flatten flexibility fnn footer.html forward frameworks fraud fully function functional functions generation gpu gpus grayscale grid handwriting handwritten hardware hidden hierarchies high highly image images import important information input input_dim=20 input_shape=(28 it keras kernel_size=(3 kernels key labels layer layers layers: learning level library linear linearity loops loss low lstm make_classification(n_samples=1000 many map maps max maxpooling2d mean_squared_error medical menu.html metrics minimal mnist mnist.load_data model model.add(conv2d(32 model.add(conv2d(64 model.add(dense(1 model.add(dense(10 model.add(dense(128 model.add(dense(16 model.add(dense(32 model.add(flatten()) model.add(maxpooling2d(pool_size=(2 model.fit(x_train models multi n_classes=2 n_features=20 network networks neural node nodes non number numpy object objects one open operations optimizer optimizers output overview parameters part patterns performance pixel platform pooling prediction predictions print(f\"accuracy probabilities process processing propagation random_state=42 recognition recognizing rectified reinforcement relu rgb scaler sequential sets sgd sigmoid simple size sklearn.datasets sklearn.model_selection softmax source spatial specialized stack standardscaler structure structured summary support synthetic tabular task tasks tensorflow tensorflow.keras.datasets tensorflow.keras.layers tensorflow.keras.models tensorflow.keras.utils test_size=0.2 testing text textures that the theano them these this to_categorical to_categorical(y_train top train_test_split training type unit use validation_data=(x_test variety vector video weights x x_test x_test.reshape( x_train.reshape y y_test y_train you",
        "url": "/Keras.html"
    },
    {
        "id": 8,
        "title": "Gunning-Fog-Index.html",
        "content": "\"\"\" \"aeiouy\" # % ' (average_sentence_length (nlp) (words * += , / 1 10.40 100 = [word a account algorithms all analysis applications average code complex content count count_complex_words(text count_sentences(text count_words(text creation def division document education english example exclamation explanation expressions first fog fog_index fog_index:.2f following footer.html formal formula function general given gunning gunning_fog_index(text higher implementation import index it language len(sentences len(word length levels mark marks menu.html more natural nlp num_sentences num_words number or patterns percentage percentage_complex_words period print(f\"the processing punctuation question range(1 re.findall(r'\\w+ readability readership reading regular return score sentence sentences steps syllable_count syllable_count(word syllables test text texts that the this those three various vowel vowels way which word words writing years",
        "url": "/Gunning-Fog-Index.html"
    },
    {
        "id": 9,
        "title": "PyTorch-Sentiment-Analysis-Model.html",
        "content": "\", \",# \"amazing \"awful \"best \"brilliant \"fantastic \"great \"negative\" \"positive\" \"prediction \"worst # % %} 'input_ids 'labels ( (outputs ) ): + , 0, 0.5 1, 1] 5) 80% = > [\"statement [1, [] [{epoch+1}/{num_epochs}]') ] _ a accuracy acting adam(model.parameters all_preds.extend(preds.cpu().numpy amazing analysis and attention_mask backpropagation base batch batch['attention_mask batch['input_ids batch_size=4 bcewithlogitsloss bertmodel bertmodel.from_pretrained('bert berttokenizer berttokenizer.from_pretrained('bert book both build class classification classification_report complete connected criterion criterion(outputs dataloader dataloader(train_dataset dataset decent dimension direction dtype each epoch evaluation example examples existing expanded experience extra false film footer.html forward(self fully function getitem__(self great horrible i idx if import inference init__(self input input_ids it its labels layers linear(self.bert.config.hidden_size linearity logits loop loss loss.backward() loss.item lr=3e masterpiece max_length=128, menu.html mode model model\" model(input_ids model.eval module movie negative neutral/positive nn nn.bcewithlogitsloss nn.linear(128 no non optimizer optimizer.step optimizer.zero_grad optional or output outputs outputs.squeeze(1 padding padding='max_length', performance performances plot pooled_output positive prediction predictions preds prettytable print print(\"model print(\"predictions print(classification_report(all_labels print(f'accuracy print(f'loss print(table python range(num_epochs raw report return_tensors return_tensors='pt save saved self.bert self.bert(input_ids self.fc1 self.fc2 self.texts[idx self.tokenizer sentiment sentiment_model.pth sentimentclassifier sentimentclassifier(nn sigmoid sklearn.metrics statement statements story storytelling table table.add_row([statement table.field_names test_dataloader text text, textdataset textdataset(dataset textdataset(text_data[:train_size texts the this threshold threshold).float time title=\"sentiment tokenizer tokens tokens['attention_mask'].squeeze(0 tokens['input_ids torch torch.no_grad torch.sigmoid(outputs).squeeze(1 torch.sigmoid(outputs.squeeze(1 torch.tensor(label torch.utils.data train_dataloader trained training transformers true true, truncation truncation=true, two use values waste watch worst year you { {accuracy_score(all_labels |",
        "url": "/PyTorch-Sentiment-Analysis-Model.html"
    },
    {
        "id": 10,
        "title": "Time-Complexity-Big-O-Notation.html",
        "content": "% %} (e.g., (if (worst a access access/search addition adjacency algorithm algorithm's all an array at average avl b balance balanced balancing bfs big binary black brute bst bst) bubble case case, cases certain children collisions complexities complexity computer constant data databases delete deletion deletions design dfs division double e each edges efficiency efficient element elements example execution exponential extra factor file fixed footer.html force graph hash head heap height how index indexing input insertion/deletion insertion: insertions it its keys leaf least level linear linked list logarithmic loop luzbetak matrix/adjacency max memory menu.html merge merging min minimum much n n) nature nested new nodes notation number o o(log o(n o(n) operations or other performance permutations priority problem process properties queue rebalancing red required) resizing retrieval root runtime salesman same scenario science\" search selection set size some sort sorted space splitting stack/queue step storage structure structures systems table that the their they this time traveling tree trees unbalanced) use v vertices way worst you",
        "url": "/Time-Complexity-Big-O-Notation.html"
    },
    {
        "id": 11,
        "title": "PyTorch.html",
        "content": "# % %} (10 , / = a acceleration an arrays audio automatic backward basic built class classes common compose([transforms computation computational computer connected criterion criterion(outputs crossentropyloss custom dataloader(trainset dataset datasets deep developers differentiation digits dimensional epoch epoch+1 example faster fc1 features final flexible footer.html forward forward(self fully function gpu gradients graphs here image import inputs it key labels language layer layers learning len(trainloader libraries library linear(28 logits loop loss lr=0.01 machine menu.html mnist model models module multi natural net net() net(nn network network\" networks neural nlp nn nn.linear(128 normalize((0.5 numpy open optim optimization optimize optimizer optimizer.step output outputs platform pre preprocess print(f'epoch processing pytorch range(2 relu researchers running_loss self.fc1 self.fc2 self.fc2(x sgd(net.parameters simple source speech strong super(net support tasks techniques tensors the title=\"pytorch torch.optim torch.utils.data torchaudio torchtext torchvision torchvision.datasets torchvision.transforms totensor train= training trainloader transform transform=transform true, variety various vision wide x x.view you zero",
        "url": "/PyTorch.html"
    },
    {
        "id": 12,
        "title": "Python-Syntax-Highlighting.html",
        "content": "\"fmt \"hello % %} /bin/bash 18 3) : = == > _ a add(5 add(a age b bash cd computer def desc echo fmt footer.html golang greet(\"world greet(name highlighting import int int) luzbetak main main() menu.html mkdir name new_directory order package print(f\"5 println(\"hello python science\" select sql syntax users where {",
        "url": "/Python-Syntax-Highlighting.html"
    },
    {
        "id": 13,
        "title": "Managed-External-Live-Tables.html",
        "content": "\"age \"path % ( /raw_data a actual an and automated automation aws azure batch blob blob, both can case checks clean_data cloud data databricks datasets dbfs def default delta dependencies differences dlt etl example execution external feature file files footer.html framework hdfs hdfs) id import ingestion int internal it key lifecycle live location managed management menu.html metadata monitoring my_external_table name no or output persistent pipeline pipelines processing pyspark.sql.functions quality real s3 s3, shared storage streaming string system table tables tasks temporary the time title=\"delta transformation underlying use users workflows you",
        "url": "/bricks/Managed-External-Live-Tables.html"
    },
    {
        "id": 14,
        "title": "PySpark-Coding-Examples.html",
        "content": "\"/mnt/data \"/path/to/output \"age \"alice \"alice\" \"bob \"bob\" \"category \"cathy \"csv \"department \"email \"engineering \"header \"hello \"hr \"id\" \"marketing \"name \"name_with_prefix \"parquet \"people \"salaries \"salary \"select \"text \"word # % %} 's ( () * , /data /mnt /sample.csv 0 1 1200 2 25 = [ [( [(1, a add_prefix_udf age aggregate aggregation all and any average avg(salary avg_age_df back based by categories category centralized clause code coding col column commands computer condition count count(email count_df.show csv customers d data data1 data2 databricks dataframe dataframes def default defined delta department desc df df.createorreplacetempview df.groupby df.na.fill df.show() df.write.csv( df1 df1.join(df2 df2 df2.createorreplacetempview df_duplicates.show() df_grouped.write.format df_top_categories df_with_prefix distinct duplicate each email example exercises explode, file filled_df filter filter_df filtered_df filtered_df.show footer.html format formats function greater group group_by_count_df guide i id' import inner introduction it join joined_df lake lakes let's limit loading luzbetak menu.html missing mr./ms name none number occurrences on operation order order_by_df orderby parquet people prefix price pyspark pyspark.sql pyspark.sql.functions pyspark.sql.types queries read records repository result rows salaries salary sales sample sample.csv scale science\" select show spark spark.createdataframe(data spark.createdataframe(data2 spark.sql sparksession specific split, sql sql's stringtype structured sum sum(price sum_salaries_df text than that the this top total total_count transformation transformations two udf unique unstructured user using value values various we where word word_count_df words words_df words_df.groupby( world write writing you your {",
        "url": "/bricks/PySpark-Coding-Examples.html"
    },
    {
        "id": 15,
        "title": "RDBMS-Schemas.html",
        "content": "% a central charts child clear common complex complex, complexity connections constellation data database databases denormalized dimension dimensions fact faster fewer flat footer.html foundational galaxy hierarchical integrity joins management many marts menu.html more multiple needs network non normalizes organizational parent performance queries query rdbms redundancy related relational relationship relationships schema schemas several simple simpler single small, snowflake specific star structure systems table tables that the these tree warehouses which",
        "url": "/bricks/RDBMS-Schemas.html"
    },
    {
        "id": 16,
        "title": "PySpark-Data-Streaming.html",
        "content": "% %} (rdd a abstraction an and api application averages backpressure based batch batches capabilities checkpointing checkpoints cluster code computation consistent continuous core counts data dataframes dataset datasets declarative development different discretized distributed dstream dstreams each ease ecosystems evolution failures fault flexibility flume footer.html frames handling hardware hdfs information input integrates integration kafka kinesis larger live logs loss lost menu.html metrics micro more network node nodes operations or original other overwhelming patterns period process processing processing: pyspark rate rdd rdds real resilience resilient resources running same scalability sensor sliding small sockets source sources spark spark's spark\u2019s specific sql state stateful stream streaming streaming\" streams structured support system tcp the this time title=\"pyspark tolerance tolerance: transformations trends tweets various volumes which window you",
        "url": "/bricks/PySpark-Data-Streaming.html"
    },
    {
        "id": 17,
        "title": "Databricks-Delta-Lake.html",
        "content": "\" % (cleaned a acid aggregated an analysis and approach architecture atomicity, audits availability batch big both broad bronze capabilities collaboration concurrent consistency consistency, controlled copy data databricks datasets debugging delta design durability durability) ecosystem efficient external failures fast feature flexibility flink footer.html for given handling historical isolation, it lake large management massive medallion menu.html metadata modifications need open optimized or organizations other partners platforms point presto processing protocol quality queries raw refined reliability scalable scale scenarios seamless secure sharing single streaming support the this tiers time tools transaction travel trino types unified users various versioning versions which workloads",
        "url": "/bricks/Databricks-Delta-Lake.html"
    },
    {
        "id": 18,
        "title": "RDBMS-Snowflake-Schema.html",
        "content": "\"date\" % a additional advanced all an and approach attributes central characteristics complex complex, complexity cost costs data database database's design difference differences dimension dimensions duplicated each environments example extension fact fewer footer.html higher increased indexing integrity it joins key layers level levels menu.html more multiple need normalization normalized normalizing performance piece priority process queries query reduced redundancy related relational relationships scenarios schema separate several shape simplicity single slower snowflake some space star storage structure structured sub table tables techniques that the them they this usage variation warehouses warehousing which year",
        "url": "/bricks/RDBMS-Snowflake-Schema.html"
    },
    {
        "id": 19,
        "title": "etl-pipeline.html",
        "content": "% %} (incremental 2. 3. a accuracy additional aggregation all analysis and any apis application applying averages business calculations cleaning cloud common connectivity consistency context conversion convert counts csv currencies data database databases date destination different downtime duplicates efficiently enhancing enrichment ensuring errors etl extract extract, extraction failures files footer.html format formats full incremental information integrity irrelevant issues it json lake large load loading logic menu.html missing needs new/changed of only optimization organization performance points process reference resource retries rules services sources specific structure summarizing sums system tables target that the this title=\"key transform transform, transformed usage values various volumes warehouse xml",
        "url": "/bricks/etl-pipeline.html"
    },
    {
        "id": 20,
        "title": "Databricks-PySpark.html",
        "content": "\"city \"pyspark # % %} ( (e.g., /data.csv /path : = a abstractions access acid action advantages aggregation algorithms allocation an analysis analysts analytics and apache api applications automate aws azure based big both business capabilities cases cloud cluster clusters code collaboration collaborative collect collections complexity computation computing count csv data databases databricks dataframe dataframes datasets deep delta deployment df distributed dive dive\" efficiency engineers environment etl evaluation example execution experiment extract features file footer.html for full fundamental gcp handling high import insights intelligence it iterative key lake large layer lazy learning level leverage load machine machines managed management manipulation manner members memory menu.html metadata mlflow mllib model models most multiple notebooks open operations other overview pandas parallel performance pipelines platform processing pyspark pyspark.sql pyspark\u2019s python queries rdds real resilient resource resources results scalability scalable scalable, scale scientists services session sharing source spark sparksession spark\u2019s storage stream streaming streams structure structured system tasks team that the they time title=\"pyspark tolerance top tracking transactions transform transformation use user users visualizations workflows workload workloads",
        "url": "/bricks/Databricks-PySpark.html"
    },
    {
        "id": 21,
        "title": "PySpark-Pivot-Table.html",
        "content": "\"east\" \"employee \"pivot \"region \"region\" \"sales \"west # % ( () + , 250} 400| \\ ] a agg(sum(\"sales alice alice| analytics application bob\" bob| categorical code column columns combination context data dataframe df df.groupby dictionaries digestible each east east|north| employee example explanation following footer.html format functions group import information list menu.html more multiple new north north| null| one or output overview pivot pivot(\"region pivot_df pivoted provided purposes pyspark pyspark.sql pyspark.sql.functions region region|sales| reporting sales sample something south south| spark spark.createdataframe(data spark.stop sparksession sparksession.builder sum table the this tool unique using value values west west| you {\"employee | |employee|",
        "url": "/bricks/PySpark-Pivot-Table.html"
    },
    {
        "id": 22,
        "title": "RDBMS-Star-Schema.html",
        "content": "% a advantages agegroup amounts an analysis and attributes category central commerce components customer customerid customername customername, data database date dateid dates de descriptive design details dimension disadvantages e efficient example fact facts flexibility footer.html foreign increased it its joins keys limited links location) menu.html month names normalization number optimized performance popular price product productid productname quantitative quantities quarter queries query rdbms redundancy region requirements resemblance sales schema simple simplicity star storage store storeid table tables the title=\"rdbms warehouse warehousing year",
        "url": "/bricks/RDBMS-Star-Schema.html"
    },
    {
        "id": 23,
        "title": "PySpark-SQL-Functions-Parquet.html",
        "content": "\" \"/mnt/data \"/path/to/output \"age \"alice \"alice\" \"bob \"bob\" \"category \"cathy \"csv \"department \"email \"engineering \"group \"header \"hello \"hr \"id\" \"marketing \"name \"name_with_prefix \"order \"parquet \"people \"salaries \"salary \"select \"text \"where \"word # % %} 's ( () * , /data /mnt /sample.csv 0 1 1200 2 25 = [ [( [(1, a add_prefix_udf age aggregate aggregation all and any average avg(salary avg_age_df back based by categories category centralized clause code col column commands condition count count(email count_df.show csv customers d data data1 data2 databricks dataframe dataframes def default defined delta department desc df df.createorreplacetempview df.groupby df.na.fill df.show() df.write.csv( df1 df1.join(df2 df2 df2.createorreplacetempview df_duplicates.show() df_grouped.write.format df_top_categories df_with_prefix distinct duplicate each email example explode, file filled_df filter filter_df filtered_df filtered_df.show footer.html format formats function functions greater group group_by_count_df guide i id' import inner introduction it join joined_df lake lakes let's limit loading menu.html missing mr./ms name none number occurrences on operation order order_by_df orderby parquet people prefix price pyspark pyspark.sql pyspark.sql.functions pyspark.sql.types queries read records repository result rows salaries salaries\" salary sales sample sample.csv scale select show spark spark.createdataframe(data spark.createdataframe(data2 spark.sql sparksession specific split, sql sql's stringtype structured sum sum(price sum_salaries_df text than that the this title=\"pyspark top total total_count transformation transformations two udf unique unstructured user using value values various we where word word_count_df words words_df words_df.groupby( world write writing you your {",
        "url": "/bricks/PySpark-SQL-Functions-Parquet.html"
    },
    {
        "id": 24,
        "title": "Medallion-Architecture.html",
        "content": "\"/mnt/delta/silver \"256\") \"io.delta.sql # % ( (col(\"date ) , .config(\"spark.sql.extensions .config(\"spark.sql.shuffle.partitions /mnt/delta/bronze 1 100 150 2 200 3 300 350 365 4 5 500 <= = >= \\ a accurate advanced aggregated aggregation aggregations analysts analytics and approach architecture availability avro benefits big bronze business cases category cleaned cleaning clear col(\"date column columns common complex conflicts consistency cost count(\"order_id\").alias(\"total_orders csv current_date dashboards data dataset datasets date_sub(current_date days decision delta deltacatalog description different domain duplicates each electronics end engineers errors facilitate filtering final flexibility focus footer.html form formats future gold gold_df gold_path governance grocery high implementations improvements incomplete information ingestion insights intermediate, it its json kpis lake large last latency layer layered layers learning levels lineage logic machine making medallion menu.html minimal missing model models more multiple new number optimized order_id orders organizations original overview parquet partitioning paths performance petabytes practice primary processing purpose pyspark quality query range raw raw, raw_data.repartition(1024 real records refined refining reporting result sales sales_amount scalability scale scientists separate separation sets silver silver, silver_df silver_df.groupby(\"category silver_df.write().format(\"delta\").mode(\"overwrite\").save(silver_path) silver_path sizes source sources spark spark.read().format(\"delta\").load(bronze_path spark.read().format(\"json\").load(\"/mnt/raw_data/ spark.stop sparksession.builder specific stage stages structured sum(\"sales_amount\").alias(\"total_sales systems tables teams terabytes that the this three tiers today total total_orders total_sales traceability tracking training transformation transformations two unfiltered unprocessed use users validated values various we which world {%",
        "url": "/bricks/Medallion-Architecture.html"
    },
    {
        "id": 25,
        "title": "Medallion-Architecture-Partitioning-Code.html",
        "content": "\"/mnt/delta/silver \"/mnt/raw_data/ \"category \"date \"delta \"io.delta.sql \"json \"medallion \"order_id \"spark.executor.instances \"spark.sql.catalog.spark_catalog \"spark.sql.extensions \"status \"total_orders \"total_sales\" # % %} ( (col( ) /mnt/delta/bronze 1 100 150 2 200 256 3 300 350 365 4 5 500 <= = >= \\ agg aggregated aggregation aggregations alias and architecture bronze bronze_path category cleaning cluster\" col(\"date column columns config current_date data dataset date_sub(current_date days deltacatalog description dropduplicates each electronics filter footer.html format gold gold_df gold_path grocery groupby ingestion last layer layers medallion menu.html mode new node number optimized order_id orders original partitioning pyspark raw raw_data read repartition result sales sales_amount silver silver_df silver_path spark spark.sql.shuffle.partitions spark.stop sparksession.builder sum tables the three title=\"medallion today total total_orders total_sales transformation two we write {%",
        "url": "/bricks/Medallion-Architecture-Partitioning-Code.html"
    },
    {
        "id": 26,
        "title": "Managed-External-Tables.html",
        "content": "% a an both case cases control data database delta directory external footer.html its itself lake lake's lifecycle location managed management menu.html metadata not only other physical specified storage summary system systems table the unmanaged use user you",
        "url": "/bricks/Managed-External-Tables.html"
    },
    {
        "id": 27,
        "title": "Column-Shuffle-Repartition.html",
        "content": "\"category \"customer_id # % %} ( () , 256 = a aggregations all alternatives an balance balancing case category cluster coalesce(n col column customer_id data default efficiency efficient equal example filtering following footer.html full goal grouping intention it joins key large later load logic menu.html more multiple no nodes number operations oversized parallel partition partitions perfectly points processing pyspark raw_data reasonable repartition repartition(256 repartitioned_data repartitioning rows same share shuffle shuffling smaller some_column spark specific specified splits syntax that the this title=\"repartition using value values which you",
        "url": "/bricks/Column-Shuffle-Repartition.html"
    },
    {
        "id": 28,
        "title": "Optimizing-Join-Queries.html",
        "content": "% 1. 3. 5. 6. 7. a accessed active adequate allocation amount analysis and anomalies architectures archive batch batches bottlenecks buffer buffering cache caching caching: check clause columns common complex composite condition conditions considerations constraints continuous correct cost costly criterion ctes data database database's datasets date defragmentation denormalization design different dimension disk disks efficient environment example excessive execution expensive explain expressions fact faster files filter filtering filters flexible footer.html foreign fragmentation frequently full functions hardware histograms i i/o implement incorrect increased index indexed indexes indexing infrastructure inner insights insufficient it join joins key large left legacy less limited loop maintenance management materialized memory menu.html method methods model monitoring more most multiple mysql need nested old older on ones operation operations optimization optimize or oracle order other parallel partition parts performance plan pools potential processing processing: processors proper pruning purge queried queries query redundancy redundant region regular regularly repeated resources result results retrieval rewrite right running scalability scans schema selective separate set simpler size smaller solutions specific ssds star statistics steps storage subqueries sufficient summary system table tables tasks techniques that the these they this those time tools tune type types unnecessary usage very views volume warehouse warehousing where which write",
        "url": "/bricks/Optimizing-Join-Queries.html"
    },
    {
        "id": 29,
        "title": "Relational-Databases.html",
        "content": "% : a accuracy acid advantages algebra all an analysis analytical analytics and another apis atomicity attribute attributes benefits better business caching category central centralized clear codd column columns complex compliance comprehensive concepts consistency constraints current data database databases datasets decision denormalized design dice\" dimension each edgar enforced entities etl extract extract, fact fast features files finance flat footer.html foreign functions heavy historical identifier improved indexing integrity intelligence it key keys language large load making marts menu.html more multidimensional multiple normalized nothing olap oltp one online operational operations performance predefined primary process processing properties purposes queries query querying read reads record records redundancy reference relational relations relationships reporting row rows sales schema simple single slice snowflake source sources specific sql standardized star state stores structure structured subsets table tables target techniques that the theory they time title=\"relational transaction transactional transactions transform transform, transformed trend truth unique users valid various view volumes warehouse warehouses warehousing writes }",
        "url": "/bricks/Relational-Databases.html"
    },
    {
        "id": 30,
        "title": "PySpark-Lazy-Evaluation.html",
        "content": "# % %} ( = a ability action actions actual acyclic amount an and benefits calculations cluster collect() complex computation computing concept count dag data dataframe deferred directed distributed driver efficiency efficient entire environments evaluation example examples execution existing external fewer filter filtering final footer.html graph intermediate it jobs key lambda large lazy logical manner map mapping memory menu.html more necessary new no n\u2019t one only operations optimization optimizations optimize performance pipelines pipelining plan plans point print processing program pyspark rdd rdd_filtered resource resources result results saveastextfile sc.parallelize scalable scale sequence several small some spark spark\u2019s steps storage subset that the their them these they this title=\"lazy transformations unnecessary usage what you your",
        "url": "/bricks/PySpark-Lazy-Evaluation.html"
    },
    {
        "id": 31,
        "title": "Delta-Live-Tables.html",
        "content": "\"age \"country \"path \"raw_data \"user_count # % %} ( .agg(count /raw_data 18) : > @dlt.expect a acid analytics and automate automatic batch both cases checks clean clean_data cleaned col complex constraints creation data databricks declarative def delta delta's dependencies dependency development dlt ease efficiency enforce enforcement entire etl example execution failure flexibility footer.html framework handling health immediate import incremental ingestion integration invalid it lake learning lifecycle lineage live machine management manual menu.html monitoring new observability only operational optimizations orchestration output overhead performance pipeline pipelines processing pyspark.sql.functions quality raw raw_data real reliable, return rows scalable, schema simple simplified simplifies sources spark.read streaming supports syntax tables the time title=\"delta tools top tracking transactions transformation transformations travel use valid visibility way workflow workloads",
        "url": "/bricks/Delta-Live-Tables.html"
    },
    {
        "id": 32,
        "title": "PySpark-Handling-Missing-Data.html",
        "content": "\"imputation # % %} ' , : = =true).over(window_spec a action algorithms all an analysis analytics any apache api applications backward big binary building cluster col collections column column2 columns common computer computing constant data database dataframes datasets developers df.dropna(axis=1 df.dropna(subset=['column1 df.fillna('unknown') df.fillna({'column1 df_cleaned df_filled different distributed downstream drop dropna() e.g., evaluation execution fault features fill filling fillna() filter flag footer.html for forward forward/backward framework function functions fundamental github import imputation imputer(inputcols=['column1 indicator instance it key language large last lazy learning library lost luzbetak machine machines map mean_value median memory menu.html method methods missing missingness mllib mode model modeling models named nature null numeric objects observation open operations optimizations or other outputcols=['column1_imputed pages\" parallel performance plan planning popular powerful preprocessing processing programming pyspark pyspark.sql.functions pyspark.sql.window pyspark\u2019s python queries query rdds regression relational remove requirements resilient rows scale several single some source spark specific specified sql string structure structured subset summary sys.maxsize system table task techniques that the these they this tolerance tools transformations true).over(window_spec valid value values various when(df['column1'].isnull window window.orderby('date_column').rowsbetween window.orderby('date_column').rowsbetween(0 window_spec you your",
        "url": "/bricks/PySpark-Handling-Missing-Data.html"
    },
    {
        "id": 33,
        "title": "PySpark-Questions-Answers.html",
        "content": "% %} (resilient = == a action actions algorithms an and answers apache api apis big catalyst cluster clusters collection columns computation computing concept control count data database databases dataframe dataframes dataset datasets df1.id df1.join(df2 df2.id difference distributed driver each easier efficient element elements entire evaluation example execution existing external features filter flatmap flatmap() flow footer.html for function fundamental github high higher inner input interface it iterative join join() joins key large lazy level lineage list logical low luzbetak machines manipulation map map() memory menu.html more most multiple named new objects one open operations optimizer pages\" pandas parallel parallelism parallelized physical plan plans processing program pyspark python questions rdd rdds relational resilient results rules scalable, scale semi single source spark storage structure structured system table tables tasks that the them they tolerance transformation transformations two various ways what which you",
        "url": "/bricks/PySpark-Questions-Answers.html"
    },
    {
        "id": 34,
        "title": "menu.html",
        "content": "data databricks devops github include.title learning machine programming search { }}",
        "url": "/_includes/menu.html"
    },
    {
        "id": 35,
        "title": "footer.html",
        "content": "kevin luzbetak \u00a9",
        "url": "/_includes/footer.html"
    },
    {
        "id": 36,
        "title": "debugging-kubernetes-performance.html",
        "content": "% (hpa / /o 4. 5. 6. 7. : < <container <namespace> <node <pod <pvc a agent an and anomalies another api application applications approach autoscaler bottlenecks c calico capacity cause cilium cli clues cluster cluster's clusters code collection combination communication components conclusion congestion container containers cpu cpu/memory crashes dashboard dashboards debugging degradation dependencies deployments describe description detailed details different disk displays distributed each entire environment error errors events experience external failed features fio flow flows footer.html functions go grafana graphical health high historical horizontal hpa htop hubble i i/o indicators information interactions iops iotop issue issues istio it java journalctl jvm key kubectl kubelet kubernetes latencies latency leaks level limits liveness/readiness load logs loss measure memory menu.html messages metrics misconfigurations monitoring mounts n name namespace native nature network networking node nodes o object observability oomkilled open operations overview packet performance persistentvolumeclaim persistentvolumeclaims planning plugins pod pods pods, policies policy pressure primary probes problems profiler profiling prometheus pvc py python rates real related reliable request requests resource resources restarts root running runtime scaling service slow smooth snapshot solution solutions source space specific spy ssh stack status steps storage swapping symptoms systematic testing tests that the these they this throttling time tool tools top traces tracing traffic trends troubleshoot u ui unusually usage use variety vertical visualization vpa warnings wasted weave web what you your",
        "url": "/devops/debugging-kubernetes-performance.html"
    },
    {
        "id": 37,
        "title": "github.html",
        "content": "% %} (cd) (rbac) (repo .github/workflows/ a access account actions additionally, admin all an another answer answers applications asked authentication authorization automate automated based best blogs board branch bugs building changes ci ci/cd cloud code codebase coding collaboration comment common community conclusion conflicts content continuous contributions contributor contributors control copy core deployment developer developers development directory documentation each environments experience feature features files followers footer.html forked forks git github global group history integrates integration interview interviews issue issues it its job kanban key level lines local machine main maintainer management members menu.html merge method model modern modifications most oauth offers open organization organizations original other others overview overview\" own owner pages party permissions personal pipelines platform platforms points popular powerful practices process production project projects providers pull quality questions range rbac remote repositories repository request requests resources review reviews revisions role roles saml separate services site sites social software source stars static style target tasks team teams testing tests that the their them these they third this those to tool tools tracking users version web websites what which wide wikis workflow workflows yaml you your",
        "url": "/devops/github.html"
    },
    {
        "id": 38,
        "title": "general-101.html",
        "content": "\"city \"pyspark # % %} ( (cd) (ci/cd (e.g., (elastic (iam (relational (repo .gitlab /data.csv /path 1 101\" 2. 3. : = a ability abstractions acceptable access accuracy acid action acyclic adheres advantages aggregation airflow airflow\u2019s alerts algebra algorithms aligns all allocation amazon amounts an analysis analysts analytical analytics and another any apache api apis applications atomicity attribute attributes automate automated automatic automating automation aws azure based basis batch benefits best better big both branch branches branching bug build building business caching capabilities capacity cases category cd central centralized certain chance change changes checks ci ci.yml ci/cd clear cloud cloudformation cloudwatch cluster clustering clusters codd code codebase collaboration collaborative collect collection collections column columns commit common complex complexity compliance comprehensive computation compute computing concepts conditions configuration configurations conflicting conflicts considerations consistency consistent constraints container containerized continuous control control: count cross csv current custom cyclic dag dags data database databases databricks datadog dataframe dataframes dataset datasets date dates decision deep delay delta denormalized dependencies deployment deployment) design developers development df diagnosing dice\" different dimension directed distributed dive docker documentation downstream driven duplicated duplication each ec2 edgar efficiency elastic elt encryption end enforced engineering engineers ensure ensures entire entities environment environments error errors etl etl/elt evaluation event events every example execution expectations expected experiment extract extract, fact failed failure failures fast feature features field fields file files finance fixes flat flexibility focus footer.html for foreign formal formats full functions fundamental gcp general git github gitlab graceful graphs great handling heavy high historical iac idempotency idempotent identifier identifiers identity implement import improved incoming inconsistent indexing infrastructure insights instances integrates integration integration/continuous integrity intelligence intervals intervention issue issues it iteration iterative its jenkins key keys kubernetes lake lambda language large layer lazy learning level leverage load location logging logic long luigi machine machines main making managed management manipulation manner manual marts mechanisms members memory menu.html merge merging metadata mlflow mllib model models modern monitor monitoring more most mr multidimensional multiple mysql new no normalized notebooks nothing null null/empty numbers numeric object olap oltp one online open operational operations or orchestrated orchestration orchestrator orchestrators order other others outcome overview pandas parallel parallelism part parts performance pipeline pipelines platform platforms point popular post postgresql pr practices predefined prefect previous prices primary process processes processing product production proper properties protection provisioning pull purposes pyspark pyspark.sql pyspark\u2019s python quality queries query querying ranges rapid raw, rdds rds read reads real record records recovery redundancy reference regular relational relations relationships reliability repeatability reporting repositories repository request requests resilient resizable resource resources response result results retries retry reviews row rows rules running s3 sales same scalability scalable scalable, scale scaling scheduled scheduling schema scientists secure security semi sensitive separation sequence server serverless servers service service) services session setup shared sharing simple simplicity single slice small snapshot snowflake source sources spark sparksession spark\u2019s specific specified sql stable stages standardized standards star state states step storage stores strategies strategy stream streaming streams structure structured structured, subsets system systems table tables target task tasks team teams techniques technologies temporary terraform test testing tests that the their theory they this those thresholds time title=\"pyspark tolerance tool tools top traceability track tracking transaction transactional transactions transform transform, transformation transformations transformed travel trend trigger triggering troubleshooting truth two types unique uniqueness unit updates upstream use user users valid validated validates validation validations value values various version versions view virtual visualizations volumes warehouse warehouses warehousing way web which with workflows workload workloads writes you your",
        "url": "/devops/general-101.html"
    },
    {
        "id": 39,
        "title": "apache-nifi.html",
        "content": "% a ability aggregation an analysis and apache apis approach audit authentication authorization automation based both cases cloud common complex compliance control csv custom data databases debugging delivery destinations detailed detection developers devices different diverse drag drop efficient encryption enrichment environments external features filtering finance flow flows footer.html formats formatting fraud friendly ftp government healthcare high http industries ingestion integration interface iot it its json kafka key lakes large lineage logs manner mechanisms menu.html monitoring multi nifi nifi\" non open organizations other overview pipelines plugins premises privacy processing processors programming protocols provenance range real reliable, routing scalability scalable, scenarios seamless secure services source sources ssl system systems tasks technical telecommunications tenant that the throughput time tool trails transfer transformation transformations use user users various visual volumes warehouses web wide xml",
        "url": "/devops/apache-nifi.html"
    },
    {
        "id": 40,
        "title": "docker.html",
        "content": "% a access all an and any application applications base center cloud commands compose consistency container containerization containerized containers control creation data dependencies developers development different docker docker\" dockerfile each ecosystem environment environments faster features file files filesystem footer.html host hub image images increased instructions isolated it its kernel key lightweight, load local machine machine\" machines management menu.html multi multiple my networking only orchestration os other own points portability portable, previous problem process processes production repository reproducibility scalability scaling scope security series startup swarm swarm/kubernetes system system's templates testing text that the they this times title=\"key tools variables vast version versions virtual vms vulnerabilities which",
        "url": "/devops/docker.html"
    },
    {
        "id": 41,
        "title": "apache-airflow.html",
        "content": "% a acyclic airflow amounts an analysis apache architecture author based bashoperator celery changes cloud code communication complex configurable connection correct custom dag dags data databases debugging delays dependencies devops different directed distributed dynamic each engineering errors etl extensibility external extract, failure features flow footer.html generation graph graphs hooks integrates integration integrations interface intervals it key kubernetes large learning limits load load) logging machine managing mechanism mechanisms menu.html modular monitoring multiple needs node notifications number open operators orchestrate order pipeline pipelines platform platforms plugins points processes programmatically progress python pythonoperator relationships retry rich scalability schedule scheduling science seamless sensors services set small source specific specified sql step systems task tasks technologies that the their they transform, triggers troubleshooting types ui updates user various versioning web which workers workflow workflows xcoms you your",
        "url": "/devops/apache-airflow.html"
    },
    {
        "id": 42,
        "title": "Kubernetes.html",
        "content": "# % (hpa (pvc (rbac (role (smallest , 0.5 0.5\" 1 2 2. 3 4 500 500m : a access active actual address address: amount an and answer answers any api apiversion app application application's applications approach appropriate apps/v1 architecture architectures asked at automate automated autoscalers autoscaling availability balancing based basic both capabilities capacity case cases certificates changes charts claims cloud cluster cluster) clusters cncf command communication community complex complex, component component: components computational computing conclusion conditions configmaps configuration conflicts constraints container containerized containers control controller controllers core count coupled cpu cpus creation custom data databases declarative default definition dependencies deployable deployment deployment's deployment.yaml deployments desired difference different distributed docker downtime each easy ecosystem efficiency elk environment environments etcd example execution experience external f failed features field file following footer.html foundation fractional functionality given google granular half hand healing health healthy helm hierarchy high highest horizontal host hosting how hpa http https hub identity image images impact individual information infrastructure ingress instance integrations interview interviews ip isolated isolation issues it its job k8s kernel key kind kube kubectl kubelet kubernetes labels large, least level levels libraries lifecycle lightweight lightweight, limit limits load loads logging logs lowest machines main management manager master matchlabels maximum mechanism memory memory, menu.html metadata metrics microservices millicores minimal monitoring more most much multi multiple my name namespace namespaces native nature necessary network new nginx nginx:1.21.6 node node's nodes number object observed old one ones open operations operators or orchestration other overconsuming overview package parts passwords performance persistent physical place plane platform pod pod's pod. pods pod\u2019s points policies port portable ports powerful previous private process processes progress proxy pv questions rate rbac registries reliable replicas replicaset replicasets request requests requests.cpu resource resources restarts reusable revision rich risk role rollback rollbacks rolling rollout rollouts routing running runtime same scalability scalable scale scaling scheduler scheduling secrets secure security select selector self sensitive server services set settings shared simple simplest simplifies single smallest snapshots some source spec specific specification specifications specified ssl stack standards state stateful statefulset statefulsets stateless status storage store strategy sufficient summary system system's systems tasks template termination than that the their them these they this those three tightly time title=\"kubernetes tls to tokens tool tools traditional traffic turn two types typically understanding undo unhealthy unique unit units up update updated updates usage use users utilization v1 value varying vcpu version versions virtual volume volumes vulnerabilities way what which worker workloads yaml you your",
        "url": "/devops/Kubernetes.html"
    },
    {
        "id": 43,
        "title": "software-delivery.html",
        "content": "\" % (ci/cd (elasticsearch 1. 2. 3. 4. 5. 6. 7. a actions an and another ansible application applications artifact artifactory artifacts aspects assurance automating automation aws binaries bitbucket building categories centralized chef ci ci/cd cloud cloudformation code configuration containerization containerized containers continuous control data delivery deployment development different distributed docker elk entire environment environments footer.html formats frequent functionality functions git github gitlab grafana hosted industry infrastructure initial integration integration/continuous it jenkins key kibana kubernetes legacy lifecycle logging logs logstash management manager menu.html metrics modern monitoring most multiple nexus open orchestration other package performance pipelines platform platforms popular practices process production projects prometheus prometheus: providers puppet quality recipes reliability reliable repositories repository running scalability scaling server service shipping software some source sources stack staging subversion svn system task tasks terraform testing that the this tool tools top tracking travis unit universal updates use users various version writing",
        "url": "/devops/software-delivery.html"
    },
    {
        "id": 44,
        "title": "bashrc.html",
        "content": "\"$archive_dir \"*.py\" # $archive_dir $cmd $target_file % '${1}'\" '${keyword 's/kevinluzbetak/kluzbetak/g ) * *\" +\"%y%m%d /kevinluzbetak/kluzbetak/g 1 \\ \\*.py ]]; a all an argument at c3 clear cmd+= cmd=\"rg cp date=$(date dir='.vim/' dir='anaconda3/ echo else eval exec extension f fi file filename=$(basename files first footer.html function grep h%m i kevinluzbetak/kluzbetak/g' keyword l least linux local ls max menu.html mkdir name one open os p py python replace rg rnw s/${1}/${2}/g search search() second shift spy t${2} text the then tpy two type { {% {} }",
        "url": "/devops/bashrc.html"
    },
    {
        "id": 45,
        "title": "Python-Search-Algorithms.html",
        "content": "# % %} + += 4] <= = > [3, [] ] a algorithm algorithms algorithms\" an and another arr arr[0 arr[1 array base binary binary_search(arr, both case containing current def each element elements first footer.html function half halves high if index intersection it iteration left left[0] length list list2: list_intersection(list1: lists menu.html merge merge_sort(arr mid middle output pivot print(list_intersection([1 process python quick quick_sort(right result result.append(left.pop(0 result.append(right.pop(0 return right right: right[0]: same search second set(list2 single sort target the them they two we while x",
        "url": "/programming/Python-Search-Algorithms.html"
    },
    {
        "id": 46,
        "title": "Python-String-Algorithms.html",
        "content": "\"\"\" \"abcbdab\" \"bdcab\" \"buzz \"flaw\" \"hello\" \"karolin\" \"kathrin\" \"kitten\" \"lawn\" \"silent\" \"world\" # % 'flaw 'kathrin 'lawn 'silent' 'world' '{str1a '{str1b}' '{str2 '{str2a '{str2b}' (string + 1 1, 1] 1][j 2 2d 3 = ] _ a actual aeiouaeiou' algorithm algorithms all an anagram anagram(str1a anagrams analysis args argument array base binary boolean both c1 c2 cases char character characters check checking common comparison correction corresponding cost count_vowels(s counts.get(char counts[char def deletion deletions detection distance distances dna dp dp[i dp[i][j dynamic each edits element elements elif equal error example false first first_non_repeated_char(s fizz fizzbuzz fizzbuzz(n footer.html function hamming hamming_distance(str1 he i if import indices input insertion insertions int integer is_palindrome(str it itself j karolin language last lcs lcs_length lcs_sequence len(str1 len(str1) len(str2 length levenshtein levenshtein_distance(str1 list longest longest_common_subsequence m max(dp[i maximum measure menu.html metric min(dp[i minimum n n. natural no non none np.zeros((m number numpy one operation order original output pair palindrome positions print(f\"the print(is_palindrome(\"121 print(reverse_string(\"hello processing programming python range(1 range(len(str1 range(m range(n repeated representation result result2 return returns reverse reverse_string(s row s same second sequence sequences similarity single size sorted spell str str1 str1[i str2 str2a str2b string strings subsequence substitutions sum(1 sum(c1 table that the them they this true two usage value valueerror(\"strings values versions vowels w\" we which world zip(str1 {% {lcs_length {result {str1",
        "url": "/programming/Python-String-Algorithms.html"
    },
    {
        "id": 47,
        "title": "Python-Coding-Exercise-Algorithms.html",
        "content": "\"hello # % & 'aeiouaeiou' 1] : < = == > ] a algorithms bool char character coding count_vowels(s counts.get(char def duplicates enumerate example exercise fib_seq fibonacci fibonacci(n first first_non_repeated_char(s footer.html i int intersection is_palindrome(n: j largest len(list1 len(list2 list list(set(list1 list(set(list1) list2: list_intersection(list1: lists menu.html merge merged.extend(list2[j non none num_map[num number nums olleh output palindrome print(is_palindrome(121 print(is_palindrome(123 print(reverse_string print(two_sum([2 problem range remove_duplicates(nums repeated return reverse_string(s s sequence set(list2 sorted str string sum sum(1 target the true two usage vowels w while world",
        "url": "/programming/Python-Coding-Exercise-Algorithms.html"
    },
    {
        "id": 48,
        "title": "101.html",
        "content": "\"\"\" \"abcbdab\" \"bdcab\" \"buzz \"flaw\" \"hello\" \"karolin\" \"kathrin\" \"kitten\" \"lawn\" \"silent\" \"two \"world\" # % 'flaw 'kathrin 'lawn 'silent' 'world' '{str1a '{str1b}' '{str2 '{str2a '{str2b}' (i.e., (string + += 1 1, 1] 1][j 2d 3 4] 5 : <= = > [0, [3, [] [i, [num_map[complement], ] _ a actual aeiouaeiou' algorithm algorithms all an anagram anagram(str1a anagrams analysis and another args argument arr arr[0 arr[1 array base binary binary_search(arr, boolean both c1 c2 case cases char character characters check checking common comparison complement containing correction corresponding cost count_vowels(s counts.get(char counts[char current def deletion deletions detection distance distances dna dp dp[i dp[i][j duplicates dynamic each edits element elements elif enumerate(nums equal error example false fibonacci fibonacci(n first first_non_repeated_char(s fizz fizzbuzz fizzbuzz(n footer.html function half halves hamming hamming_distance(str1 he high i i] if import index indices input insertion insertions int integer intersection is_palindrome(str it iteration its itself j j] karolin language last lcs lcs_length lcs_sequence left left[0] len(arr len(list1 len(str1 len(str1) len(str2 length levenshtein levenshtein_distance(str1 list list(set(nums list2: list_intersection(list1: lists longest longest_common_subsequence loop m max(dp[i maximum measure menu.html merge merge_sort(arr merged.extend(list2[j metric mid middle min(dp[i minimum n n. natural nested no non none np.zeros((m num_map[num number numbers numpy nums one operation order original other output pair palindrome pivot positions print(f\"the print(is_palindrome(\"121 print(list_intersection([1 print(remove_duplicates([1 print(reverse_string(\"hello problem process processing programming python quick quick_sort(right range(1 range(i+1 range(len(arr range(len(str1 range(m range(n recursive remaining remove remove_duplicates(nums: repeated representation result result.append(left.pop(0 result.append(right.pop(0 result2 return returns reverse reverse_string(s right right: right[0]: row s same search second sequence sequences series set(list2 similarity single size sort sorted specific spell str str1 str1[i str2 str2a str2b string strings subsequence substitutions sum sum\" sum(1 sum(c1 table target that the their them these they this true two two_sum two_sum(arr, two_sum(nums usage value valueerror(\"strings values versions vowels w\" we which while world x zip(str1 {lcs_length {result {str1",
        "url": "/programming/101.html"
    },
    {
        "id": 49,
        "title": "Python-Programming-Language.html",
        "content": "% (oop) . 4. 5. 9. _ a access algorithms and anonymous arguments async asynchronous asyncio attributes await behavior break cases classes closures code collection complexity comprehensions conditional context control counting cprofile csv custom data debugging decorators def default defining dependencies development dictionaries different django dunder each efficient encapsulation environments error exceptions explicitness expressions file files flask flexible flow fly footer.html frameworks functions garbage generator generators gil guide handling higher idiomatic implications importing inheritance inline iter iterators itertools json key lambda language language\" libraries library like list lists loops management managers managing math matplotlib memory menu.html methods methods: mock modules multiprocessing mutable numpy object objects optimization order oriented packages pandas parameters pdb pep performance polymorphism popular practices principles private profiling program programming properties public python python's pythonic readability readable reading reference requests results scikit sequence sets simplicity special standard statements str structures style sys systems test testing text that the their time timeit tools tuples tuples, types understand understanding unit unpacking use values venv virtual virtualenv ways web writing yield zen",
        "url": "/programming/Python-Programming-Language.html"
    },
    {
        "id": 50,
        "title": "mysql-lag-function.html",
        "content": "% %} ( () (date) (order >= a analysis any average avg avg_daily_increase binance.klines_1d chronological clause column curdate current d daily daily_increase data date dates day day's days decimal difference differences dt1 explanation filtering four function increase increases inner interval it lag lag(price last menu.html mysql null only order outer overview places previous price price_changes purpose query result round row rows same select sequential series sql that the this those time title=\"mysql valid value values where window you",
        "url": "/programming/mysql-lag-function.html"
    },
    {
        "id": 51,
        "title": "Python-Function-OOP-Data-Structure.html",
        "content": "# % ' 'meow' ( () (): (1, (2) (greet (i) (item (row (square(5 (value ) ): * , 1 16 2 2, 25 3, 30) 3} 4, 5) 5] 5} = [1, [row __name a age alice alice! an and animal anonymous another apply_func argument array async async_print_numbers asynchronous asyncio asyncio.run attribute attributeerror author await bark basic bob book buddy call cat() class classes closures code collection common concurrency csv data data_dict data_list) data_tuple) decorator_func decorators def defining dictionaries dictionary different dog dunder dynamic each element elements encapsulation example expression file footer.html func func(value func(value) function functions gen_exp generator generators george greet header headers hello hi higher i immutable import individual inheritance it item iterator iterators key key, keys lambda line list lists memory menu.html meow methods msg multiple multiprocessing multithreading my_cat my_cat.speak my_dict my_dog my_dog.bark my_dog.name my_generator my_list my_set my_tuple name numbers object objects order oriented orwell output pair person person.get_name polymorphism print print(person.__name print_numbers print_numbers_mp private process processes program programming python r range range(4 range(5 reader reading return row rows same sample say_hello say_hello() self self.author}' separate sequence set sets simple single spaces speak square structures target that the these this thread threading threads times_two times_two(5 tuple tuples tuples, unique unordered value values woof wrapper x yield yields {1,",
        "url": "/programming/Python-Function-OOP-Data-Structure.html"
    },
    {
        "id": 52,
        "title": "Sudoku-Board-Verification.html",
        "content": "\" \"window # % %} (i.e., (window ) 3x3 9x9 = ] _ a above all args block board bool checks col column columns completed conditions def definition each example expected explanation footer.html function given grid inner int it list lists logic main matrix matrix1 menu.html module numbers output parameters pp.pprint(matrix1 pprint pretty prettyprinter print(result prints provided purpose puzzle python\u2019s range(0 range(start result return returns row rows rules script specified standard start steps sub sudoku sum sums the this total true value verification verification\" verify_sudoku_board verify_sudoku_board(board, which",
        "url": "/programming/Sudoku-Board-Verification.html"
    },
    {
        "id": 53,
        "title": "Python-Algorithms.html",
        "content": "# % (i.e., : = [i, a algorithm an argument array both complement def each element elements fibonacci fibonacci(n footer.html function i if indices integer it its j j] len(arr len(list1 list list(set(nums lists loop menu.html merged.extend(list2[j n nested number other output print(remove_duplicates([1 range(i+1 range(len(arr recursive remaining remove_duplicates(nums result return series sorted sum target that the their two two_sum(arr, value values we {%",
        "url": "/programming/Python-Algorithms.html"
    },
    {
        "id": 54,
        "title": "algorithms2.html",
        "content": "\" # (string ) + 1 1, 1][j = args both c1 c2 character collections comparison counter def deletions distance dp dp[i dp[i][j dynamic each first hamming hamming_distance(str1 i if import insertions int j len(str1 levenshtein libraries m min(dp[i n necessary np.zeros((m number numpy programming range(1 returns second str1 str1[i str2 string strings substitutions the two zip(str1",
        "url": "/programming/algorithms2.html"
    },
    {
        "id": 55,
        "title": "algorithms.html",
        "content": "\"buzz \"fizz \"fizzbuzz (i.e., ) + , 1 12 1] 1][ 1][j 4. 5. 7. <= = > [ [:: [] _ a algorithm algorithms all an anagram anagram(str1, anagrams and another applications argument arr arr[0 arr[1 array article backtracker backtracking base binary binary_search(arr bits bitwise boolean both bubble calculate case characters check code coding coins combinatorics common complement computational computer conclusion containing cryptography current data def desired different dijkstra dijkstra's distance dp dp[i dp[i][j dynamic each edges effective, efficient, element elements elif engineering examples explanation fibonacci fibonacci(n first five fizz fizzbuzz fizzbuzz(n following formula function gcd given graph half halves hamming hash high i if important index indices information insertion integer integers interviews it iteration its j key knapsack last left left[0] len(arr len(str1 len(str2 length levenshtein linear list longest longest_common_subsequence(str1 longest_common_subsequence(str1, loop m matrices matrix max(dp[i maximum merge mid middle most multiples multiplication n n. negative nested networks neural nth number numbers operations optimization order original other pair palindrome palindrome_check(str parts passwords paths pivot popular possibilities powerful print problem problems process programming python queens quick range range(1 range(i+1 range(len(arr range(len(str1 range(m real recursion recursive regression remaining representation result result.append(left.pop(0 result.append(right.pop(0 return reverse right: right[0]: row salesman same science search second selection series set shortest single size smaller solution some sort sorted sorting specific stairs steps str str1 str1[i str2 string strings structures subproblem subproblems subsequence sudoku sum tables target tasks term that the their them these they this three tools traveling trees two types user value values various ways we weight which while wide world x",
        "url": "/programming/algorithms.html"
    },
    {
        "id": 56,
        "title": "Fibonacci-Generator.html",
        "content": "\" # % %} , 10 = _ a an b code current def definition execution fib_gen fibonacci fibonacci_generator first footer.html function function's generator github infinite it iteration its loop luzbetak menu.html next numbers old pages print(next(fib_gen python range(10 sequence state statement sum that the this two value values yield",
        "url": "/programming/Fibonacci-Generator.html"
    },
    {
        "id": 57,
        "title": "Amazon-RDS.html",
        "content": "% a advanced amazon an analytical and api application application's applications aurora automated availability aws az backend backup backups calls capabilities cases choice cli clicks cloud cloudwatch common compute console consuming costs credentials data database databases database\u2019s demand demands deployments development dms drivers easy encryption engine engines enterprise environment environments example excellent failover features few flexible footer.html grade hardware high import ingestion instance instances integration isolation it just key kms layers load managed managed, management mariadb menu.html microsoft migration mobile models multi multiple mysql native network operation oracle organizations other patching patterns performance point popular postgresql pricing provisioning rds rds\" recovery recovery: relational reliable reliable, replicas reserved resources rest scalability scalable, scale security server service services setup several smaller snapshots solution sql ssl ssl/tls standard storage tasks testing testing: that the time title=\"amazon tools transit usage use vpc warehouses warehousing web workflow you your zone",
        "url": "/aws/Amazon-RDS.html"
    },
    {
        "id": 58,
        "title": "AWS-CloudWatch.html",
        "content": "% a access actions activity alarms alerts amazon an and anomalies anomaly application applications automate automated automation aws balancers behavior build cases certain changes cloud cloudwatch collection common comprehensive conditions configure cpu custom customizable, dashboards data defined detection ec2 ecosystem efficiency enable entire environment environments errors essential eventbridge events example failure features filters footer.html functions gain health immediate infrastructure insights insights: instances interface issues it key lambda learning load log logs machine management: memory metric metrics monitor monitoring notifications now observability operational operations other part patterns performance premises problems rds real related reliability resource resources response responses s3 security service services single specific system's that the these thresholds time tool unauthorized unusual usage use utilization view visibility visual web workflow you your",
        "url": "/aws/AWS-CloudWatch.html"
    },
    {
        "id": 59,
        "title": "Amazon-S3.html",
        "content": "% (identity 99.99% a access accessed amazon amount amounts an analysis analytics analytics: and any api applications archival archive archiving athena automated availability aws backup backups big bucket buckets cases cdn centers class classes cloud cloudfront common compliance compliance: computing content controls cornerstone cost costs custom data datasets deep delivery different disaster dispersed distribution diverse documents durability effectiveness emr encryption features flexibility footer.html for frequently geographically glacier high hours ia iam identity images infrequent infrequently intelligent internet it key lakes lambda languages large layers lifecycle long lower lowest management many menu.html minutes multiple needs number object objects one option options other patterns policies pricing processing programming range rapid reasons recovery recovery: redshift regions reliability replication requests rest restful retrieval s3 s3\" scalability scalable scale sdks security serverless service services simple single sources specific spectrum sql standard static storage storing term that the them tiering tiers times title=\"aws tools transfer transit transition two unlimited usage use users variety various versioning versions videos virtually web which who wide you your zone",
        "url": "/aws/Amazon-S3.html"
    },
    {
        "id": 60,
        "title": "ETL-Pipeline-AWS.html",
        "content": "% & (extract 2. 4. 5. 6. 7. a acceleration access activity alarms amazon an analysis analytical and another any apache api apis archival auditing auto automation aws batch big buckets calls catalog cleaned cloudtrail cloudwatch complex compliance config create data database databases datasets define dependencies dynamodb each ec2/emr elastic emr encrypt etl events external failures flow footer.html frameworks fully functions further glue hadoop health iam identity ingestion inspector internal issues job jobs key kinesis kms lake lambda large load loading log logging logs manage managed management mapreduce menu.html metadata monitor monitoring more needs nosql option or orchestration overall party performance pipeline pipeline's policies processing purposes queries raw rds real redshift regulations relational reporting resources rest roles s3 scale scaling schedules schema security sequence service services simple sources spark specific step storage store streams that the third time track transfer transform transformation transformations transit trigger use various warehouse workflow you your",
        "url": "/aws/ETL-Pipeline-AWS.html"
    },
    {
        "id": 61,
        "title": "AWS-EMR.html",
        "content": "% : a access advanced algorithms amazon amounts an analysis analytics apache aws batch big businesses cases cloud cluster common complex configuration control costs data datasets dynamodb ecosystem effective elastic emr encryption environment etl example features flink footer.html framework frameworks further hadoop hbase hive iam infrastructure instances integration isolation it job jobs key languages large layers learning machine managed mapreduce menu.html mllib multiple necessary needs network options or other others pig platform presto presto, pricing processed processing provisioning python raw real redshift resources rest results running s3 scala scalability scale security service services spark spot sql storage store streaming tasks termination that the time title=\"aws transformations transit underlying use vast warehousing workflow you your",
        "url": "/aws/AWS-EMR.html"
    },
    {
        "id": 62,
        "title": "AWS-Glue-Workflow.html",
        "content": "% %} a amazon an analytics arrival automation aws aws\" bucket cases catalog cloudwatch common completion complex comprehensive conditional configuration configure console coordinate correct crawler crawlers custom data define dependencies destinations different driven each editor error etl even event events example execution extract, failure feature features flexibility flow flowchart footer.html glue handling ingestion integrated integration interface issues it job jobs lake load load) loading logging logic management menu.html metadata monitor monitoring multiple necessary orchestrate orchestration order other parallel part path paths pipelines pipelines: predefined previous processes processing progress real redshift s3 scale scheduled schedules sequence set solution sources status success tasks that the them they time title=\"amazon transform, transformation transformations trigger triggers troubleshoot upstream use various visual warehouse warehousing workflow workflows you your",
        "url": "/aws/AWS-Glue-Workflow.html"
    },
    {
        "id": 63,
        "title": "AWS-Kinesis-Data-Streams.html",
        "content": "\"aws \"extract\" % a amazon an analytics analyzing and any applications aws cases centralized common data distributed etl event features fly footer.html full further ingestion insights integration interactions it key kinesis lambda large log logs menu.html metrics monitoring name other personalization phase pipeline process processing real redshift s3 same scalability service services sources streaming streams streams\" the time title=\"aws tracking use user various volume you",
        "url": "/aws/AWS-Kinesis-Data-Streams.html"
    },
    {
        "id": 64,
        "title": "AWS-Step-Functions.html",
        "content": "% %} a access amazon an and another api application approvals automate automation aws batch business call calls cases common completion complex console coordination data days decision defined deployment design different distributed dynamodb each easy ecs editor end environment error errors etl evaluation even example executes execution external extraction failed fault features final flowchart footer.html function functions functions\" further glue handling human iam infrastructure input integration interface it its jobs json key lambda language learning loading long machine making manage management management: menu.html microservices model monitoring months multiple orchestrate orchestration other output path pipelines policies preparation process processes processing redshift reliability reliable roles running s3 scalability sdk security serverless servers service services start state states step steps storage systems task tasks that the they title=\"aws tolerance training transformation transitions trigger use validation various visual which workflow workflows you your",
        "url": "/aws/AWS-Step-Functions.html"
    },
    {
        "id": 65,
        "title": "AWS.html",
        "content": "% %} (iam) a access accurate ad all amazon analytics and any assets athena auditability automatic aws cases catalog catalog\" cataloged centralized common component configuration control core crawlers data databases date define different discovery emr environment etl evolution example extract, fast features flow footer.html glue governance history hoc iam identity integrates integration it job jobs key lake lakes landscape lineage load) loading manage management manual menu.html metadata monitor or partitions pipelines place policies powerful process processes queries query redshift reference repository schema schemas security services single sources spectrum storage stores strategy structures tables the time tool track tracking transform, transformations transforming transparency updates use users various versioning warehouse warehouses workflow you your",
        "url": "/aws/AWS.html"
    },
    {
        "id": 66,
        "title": "AWS-Config-Inspector.html",
        "content": "% %} a account action actionable actions adherence alerts all an analysis and any applications approach assessment assessments audit auditing automate automated aws basis best better breaches capabilities cases change changes checks ci ci/cd clear cloudtrail common compliance compliant config configuration configurations configure continuous controls current define defined demand dependencies deployments desired detailed deviations devops disaster dss each enable environment evaluation example execute execution exposed external features findings footer.html generate governance historical iam identified impacts improvement incident incidents industry insecure inspector inspector\" integration internal intervention inventory issues it key lambda logs management manual menu.html monitoring monitoring: non notifications ongoing operational organizational other part patches pci pipeline planning policies ports posture potential powerful practices predefined process recommendations recommended recorded recovery regular regulations regulatory relationships remediate remediation report reports requirements resource resources response review risks rules schedule scheduled secure security service services set sns software standards state status targets templates that the these this thorough time title=\"aws track troubleshooting unpatched use view vulnerabilities vulnerability way workflow workflow: you your",
        "url": "/aws/AWS-Config-Inspector.html"
    },
    {
        "id": 67,
        "title": "KMS-Key-Management-Service.html",
        "content": "% %} (cmk a access activities additional all an and api application applications approach audit auditing authenticity automatic aws behalf best cases centralized cli cloud cloudtrail cmk code common communications compliance configuration consistent console control created cryptographic custom customer customers data define deletion digital dynamodb ebs encrypt encryption environment example feature features footer.html for granular hardware highly hsms iam industry information integrated integrity it key keys kms lambda layer level lifecycle manage managed management managing master menu.html minimal modules ongoing operations others permissions policies practices protection providers range rds regulations regulatory repository requirements rest robust, rotation s3 saas scalable, sdk security sensitive service service\" services signing signing: single solution specific standards store that the their title=\"kms transit unified usage use users various which who wide workflow you your",
        "url": "/aws/KMS-Key-Management-Service.html"
    },
    {
        "id": 68,
        "title": "Lambda-Serverless-Computing.html",
        "content": "% %} (iam) a access actions ai aliases amazon and any api apis application applications assistants automated automatic automatically automation availability aws backend behavior c cases changes charges chatbots code common complex comprehend compute computing cost custom data database databases design development devices different dynamodb effective efficiency environment environments etl even event events executes execution external extract file files fine flexibility footer.html function functions gateway generating grained handle high http identity images incoming information infrastructure input instances integration intervention invocations iot it java lakes lambda lambda\" languages levels log logic management managing manual many menu.html message messages milliseconds multiple new no node.js notifications number orchestration other overhead own packaging performance permissions persistent power pricing process processing processing: production programming provisioning python queues range rds real records rekognition requests resizing resource resources response roles ruby runtime runtimes s3 scalability scaling scenarios secure server serverless servers service services several shifts sns sources specific sqs staging state stateless storage streams system tasks that the their them they this thumbnails time title=\"aws traffic transform transformation trigger triggers underlying usage use user variables variety various varying versioning versions videos voice web what which wide workflows you your",
        "url": "/aws/Lambda-Serverless-Computing.html"
    },
    {
        "id": 69,
        "title": "Auto-Scaling-EC2-EMR.html",
        "content": "% %} (e.g., a action activities all amazon amount an and application applications auto automated availability aws balancing batch big build capacity cases changes checks ci ci/cd cloudwatch cluster clusters common complexity configure consistent continuous cost costs cpu current custom data day demand deployment desired devops disaster dynamic ec2 ec2/emr effective effectiveness efficiency elastic elb emr environment environments events example failed features flexible footer.html group health high increases instances integration intensive it job jobs key levels load mapreduce memory menu.html metrics monitor necessary new nodes number ones operations optimal optimization optimize or other patterns performance pipelines policies powerful predefined predictable processes processing quick real recovery recovery: resource resources response review right scale scaling scheduled schedules seamless service services set size solution specific target test that the they this time timely times tracking traffic unhealthy unpredictable usage use utilization way web week when workflow workload workloads you your",
        "url": "/aws/Auto-Scaling-EC2-EMR.html"
    },
    {
        "id": 70,
        "title": "AWS-CloudWatch-Events.html",
        "content": "AWS-CloudWatch-Events.html % %} a action actions adjustments alerting alternate an and api application applications architectures auditing audits automate automated automation aws backups building calls cases change changes checks cloud cloudwatch common complex compliance compliance: component components conditions corrective corresponding criteria cron custom data detailed driven ec2 ecosystem efficiency environment error errors event events events\" example execution failed features filtering flexible footer.html function functions handle handling health infrastructure instance instances intervals it key lambda load log logging logic logs mechanisms menu.html monitor monitoring near needs notification only operations other performance periodic powerful processing real regular relevant reliability resources response responses retry review robust rotation rule s3 scheduled scripts security service services sns source specific state step stream system target tasks that the their these time title=\"aws trigger triggering triggers troubleshooting unnecessary use way workflow workflows you your",
        "url": "/aws/AWS-CloudWatch-Events.html"
    },
    {
        "id": 71,
        "title": "AWS-Glue-ETL-Service.html",
        "content": "\", \"age \"age\" \"s3://your # % %} (e.g., (etl) ) , 30 = =filtered_data =lambda [\"s3://your ], a actual amazon an analysis analytics and another any apache args argument automatically aws awsglue.context awsglue.job awsglue.transforms awsglue.utils basic batch both bucket bucket/input cases catalog cleaning code commit common completion connection_options={\"path connection_options={\"paths context crawlers create_dynamic_frame.from_options custom data database databases definitions desired different discovery distributed either environment etl example explanation extract features field filter.apply(frame=input_data filtered_data filtering footer.html format frame fully getresolvedoptions(sys.argv glue glue\" gluecontext gluecontext(sc gluecontext.spark_session gluecontext.write_dynamic_frame.from_options hood import infrastructure initialization initialize input input_data integration interface intervals intervention it job jobs json key kinesis lakes less load loading location managed managed, manual mapping menu.html metadata name object only optimal output part paths performance preparation process processing provisions pyspark.context python range rds real records redshift relational required resources result results s3 s3://your scala scalability scalable, scale scales scenarios schema script seamless serverless service services simple simplifies sources spark sparkcontext specific step storage store stores streaming studio table target than that the these this those time title=\"aws transform transformation transformation: transformations transformed transforming types usage use using variety various visual warehouses web which wide work workflow workload write_dynamic_frame.from_options you your",
        "url": "/aws/AWS-Glue-ETL-Service.html"
    },
    {
        "id": 72,
        "title": "S3-Transfer-Acceleration.html",
        "content": "% (iam a accelerated acceleration access additional all amazon amazon's amounts an and any applications assets aws backups benefits bucket bucket's budget business case cases changes clients closest cloudfront cloudfront's cloudwatch common console content control costs critical cross data datasets deadlines desired distance distances distant easy, edge effective enable endpoint example existing expected factor fast, faster feature features file files footer.html geographically global identity improved infrastructure it key large latency location locations long management media menu.html metrics nearest need network new no operations optimized or paths performance processing region regions reliability requirements review s3 scale scenarios secure security sensitive settings solution speed speeds ssl standard streaming that the time transfer transfers transit upload uploads use users users' video which workflow workflows world your",
        "url": "/aws/S3-Transfer-Acceleration.html"
    },
    {
        "id": 73,
        "title": "IAM-Identity-Access-Management.html",
        "content": "% %} (iam (iam) a access access: account actions active activities added addition additional all an and another api application applications attach audit authentication aws best calls cases cloud cloudtrail common compliance compliant consultants controls create credentials critical cross custom define directory ec2 enable enhance environment exactly example external factor feature features federation footer.html form foundational google granular group groups iam identity individual it json key lambda least limited long management management\" management: menu.html mfa microsoft monitor multi needs one organization other parties password patterns permissions policies policy powerful practices principle privilege protection provider regulatory requirements resources role roles second secure security service services specific step strict stronger temporary term that the their them these they third title=\"iam tool unauthorized use user users web what workflow you your",
        "url": "/aws/IAM-Identity-Access-Management.html"
    },
    {
        "id": 74,
        "title": "AWS-Glue-Data-Catalog.html",
        "content": "% %} (iam) a access accurate ad all amazon analytics and any assets athena auditability automatic aws cases catalog catalog\" cataloged centralized common component configuration control core crawlers data databases date define different discovery emr environment etl evolution example extract, fast features flow footer.html glue governance history hoc iam identity integrates integration it job jobs key lake lakes landscape lineage load) loading manage management manual menu.html metadata monitor or partitions pipelines place policies powerful process processes queries query redshift reference repository schema schemas security services single sources spectrum storage stores strategy structures tables the time tool track tracking transform, transformations transforming transparency updates use users various versioning warehouse warehouses workflow you your",
        "url": "/aws/AWS-Glue-Data-Catalog.html"
    },
    {
        "id": 75,
        "title": "AWS-CloudTrail.html",
        "content": "% %} a access account actions activate activity alarms all amazon an analysis analyze and anomalies api athena audit auditing automated aws bucket call calls cases change changes cloud cloudtrail cloudtrail\" cloudwatch command common complete compliance comprehensive configurations configure conjunction console dashboards data detailed detecting durable enable environment essential event events example external features files footer.html forensic governance historical history incident industry infrastructure insights internal issues it key line log logs long management menu.html monitoring notifications operational other party patterns potential problem record regions regulations relevant requirements resources responses retention risk roles s3 sdks secure security sequence service services specific standards storage store support term that the third those threats time tools track trail troubleshooting unauthorized unusual use user users visibility who workflow you your",
        "url": "/aws/AWS-CloudTrail.html"
    },
    {
        "id": 76,
        "title": "AWS-Redshift.html",
        "content": "% %} 160 a access amazon an analysis analytical analytics and aws bi big business businesses cases central cloud cluster clusters columnar commands common complex compression control copy costs data databases datasets decision demand destination destinations driven dynamodb ecosystem effective emr encryption etl example export export: fast features footer.html fully gb glue grained high iam ideal ingestion insights instances integration intelligence isolation it jobs key language large load looker making managed, menu.html multi network new node organizations other parallel performance petabyte petabytes pricing processed processing purposes queries query quicksight real redshift redshift\" reporting reports reserved resources rest results s3 scale security: semi service services single solution sources sql standard storage structured support tableau that the their time title=\"aws tools transformation transit use users various visualizations vpc warehouse warehousing workflow workload workloads you your",
        "url": "/aws/AWS-Redshift.html"
    },
    {
        "id": 77,
        "title": "Apache-Parquet.html",
        "content": "% /o a access amount analytic analytical analytics apache architectures aws azure based big bigquery cases changes cloud columnar columns common compatibility compression costs data datasets dictionary distributed easy efficient encoding engines environments etl evolution existing expenses fast faster features file files footer.html format format\" frameworks google hadoop hive i impala improved integration interoperability it key lakes large length lower many massive menu.html modern multiple necessary new olap only operations optimized parallel parallelism parquet parquet's parts performance performance: pipelines platforms processing queries query read redshift reduced retrieval rle scalability scale schema services size spark specific splitting storage stored stores support synapse system systems techniques that the them this time use warehousing which wide workloads",
        "url": "/data/Apache-Parquet.html"
    },
    {
        "id": 78,
        "title": "Apache-Iceberg.html",
        "content": "\"time % (acid) a acid analytics and apache atomic, cases: changes cloud columns complex consistent, data dataset datasets deletes durable dynamic efficiency entire evolution existing features flexible footer.html framework frequent hidden historical iceberg iceberg\" isolated, it key lakes large management manual menu.html metadata model models modern need needs object operations partition partitioning partitions petabyte previous queries query reliable scalability scale schema storage supports the time transactions travel travel\" updates upserts use users versioning versions",
        "url": "/data/Apache-Iceberg.html"
    },
    {
        "id": 79,
        "title": "Graph-Databases-ArgoDB-Neo4j.html",
        "content": "\"\"\" \"bob\", \"friends \"knows\" \"neo4j # $name $person2 $relation % %} (neo4j (person ) , :person = @staticmethod [:knows a a) a.name acid alice all analysis analytics and apis arangodb argodb argodbexample argodbexample(\"bolt://localhost:7687 auth=(user b billions breakdown case cases class close(self code complex complex, compliance concept connection consistency create create_node create_node(self create_node(tx, create_relationship create_relationship(self cypher data database databases datasets db db.close() db.create_node(\"alice\") db.create_node(\"bob db.create_relationship(\"alice db.find_friends(\"alice deep def detection different distributed document driver edges efficient ensures example excels explanation fast features find_friends find_friends(self footer.html fraud friend friends function given graph graphdatabase graphdatabase.driver graphdatabase.driver(uri handles high import init__(self insights integrates integration it key language languages large massive match matching menu.html method methods model multi name name=name neo4j neo4j's networks new node nodes or other password pattern people performance person person2 person2= platforms popular print(f\"{name processing python queries query querying rapid recommendation record related relation relationship relationships reliability result scalability scale self.driver self.driver.session session session.write_transaction(self._create_node session.write_transaction(self._create_relationship sessions setup social specialized storage syntax systems that the their this title=\"argodb\" transactions traversal two tx.run(query uri usage use user value way where which workloads you {name }\")",
        "url": "/data/Graph-Databases-ArgoDB-Neo4j.html"
    },
    {
        "id": 80,
        "title": "GraphQL.html",
        "content": "% . /access_token /accounts /adaccounts /ads /ads_insights /campaigns /events /messages /thread_settings /v1/contacts /v1/messages a access accessing accounts acquisition ad administrators ads advertisers all amount analytics and api api's apis applications apps audiences authentication automated based benefits budgets business businesses campaigns changes chatbots clients comments communication complex content control conversational creator credentials customer d data description developers different direct ecosystem efficiency efficient email end endpoint endpoints engagement environments events exactly facebook facebook's features fetching fields flexibility flexible footer.html friends front functionality graph graphql group groups i id id}/comments id}/insights id}/media id}/members id}/posts info information insights instagram interactions interfaces it key language likes list lists lot managing marketers marketing media members menu.html messages messaging messenger metrics microservices more multiple name names network notifications one only other page pages part party performance permissions photos platforms play posts precise predictability primary professional profile profiles programmatic public queries query real redundant relationships replies reports request requests required response rest schema server service sets single specific strongly structure structures subscriptions support teams that the their them they third this time title title=\"graphql titles transfer typed types updates usage user user(id users user\u2019s videos way websites whatsapp which who {",
        "url": "/data/GraphQL.html"
    },
    {
        "id": 81,
        "title": "Large-Scale-Data-Ingestion2.html",
        "content": "% a acid additional advanced amazon amounts an analytics and apache application applications automation aws azure based batch beam big both capabilities cloud cluster compliance computation confluent connectors data databricks dataflow delta distributed efficient end engineering enterprise event events extension features flink flow footer.html framework fully google grade high hubs ingestion iot it kafka key kinesis lake large latency logs low managed management manner massive menu.html message messaging millions movement multi multiple nifi parallel pipelines platform powerful processing pulsar ready real registry role scalability scale schema security service sources spark storm stream streaming streams structured, system systems telemetry tenancy that the throughput time tool tools tools\" transformation variety versioning wide",
        "url": "/data/Large-Scale-Data-Ingestion2.html"
    },
    {
        "id": 82,
        "title": "Kafka-Producer-Consumer.html",
        "content": "# % 'value = a all and apache bootstrap_servers=['localhost:9092 brokers consumer consumer\" consumer.close earliest footer.html format import it json json.dumps(v).encode('utf json.loads(x.decode('utf kafka kafkaconsumer kafkaproducer lambda list localhost:9092 menu.html message messages offset producer producer.close producer.flush producer.send('test_topic specified test_topic test_topic' the them topic true, value_deserializer=lambda",
        "url": "/data/Kafka-Producer-Consumer.html"
    },
    {
        "id": 83,
        "title": "sql-statements.html",
        "content": "\" \"192% % 'low' 'medium' ( ) , 100 2 50000 ; = a add address addresses alias all alter alumni an and any as at autoincrement average avg below both brown by cartesian case check clause column columns combine combines common conditional conditions count create cross cte customer_name customers d d. data date default delete department department_a department_b department_id departments departmenttotals desc different distinct doe drop dropping duplicate duplicates e e. e.department_id e1 e2 each either else email emails employee employee_count employee_id employee_name, employees end equality example examples existence existing exists explanation expression filter find first first_name following footer.html from full group groups having hierarchical high high' id if in index inner insert integer ip it itself jane join key keywords languages last_name least left limit limit/offset limits list location m.department_id main manager manager_id managers match matched matching medium menu.html michael minimum more name names new no none null number on one online_customers or order other outer output p p.product_id pagination part partition price prices primary product product_name products programming queries query ranges record records related removes rename result results retrieval retrieve right row rows s.email salary salary_level sales sales_amount second select self set sets side smith snowflake specified speed sql standard statement statements store_customers structure student student_name students subquery subquery_table subquery_table. sum sum_price table tables temporary that the their then this timestamp title=\"sql top total total_sales truncate two union unique unique_products update value values varchar view virtual when where which who with york' you {% | }",
        "url": "/data/sql-statements.html"
    },
    {
        "id": 84,
        "title": "Apache-Hudi.html",
        "content": "% (hadoop a acid an analytics apache architectures background batch big capabilities changed compaction compression conclusion cost data datasets deletes effective efficient environments features file footer.html footprint framework frequent fresh gap guarantees historical hudi hudi\" incremental incrementals indexes indexing ingestion inserts it key lake lakes large latency low management menu.html modern near older only open overall performance pipelines previous processing processing: queries query querying real scalable scale scenarios sizes source storage support systems techniques that the time top traditional transaction transactional transactions travel updates upserts users versions which with you",
        "url": "/data/Apache-Hudi.html"
    },
    {
        "id": 85,
        "title": "sql-create-view.html",
        "content": "% a conditions create d d.department_id d.department_name department department_salary_view departments e e.department_id each employee employee_salaries employees footer.html group high_salary_employees join joins menu.html names on overview query salaries salary select simple sql statement sum that the their this title=\"sql total view where with",
        "url": "/data/sql-create-view.html"
    },
    {
        "id": 86,
        "title": "Query-Performance.html",
        "content": "% ( (e.g., (use (vacuum * 4. 5. a a.name access advanced age aggregations amount analysis analyze and automatic aws b.salary batch benefit best better bigquery bottlenecks by caching city clauses cloud clustering clusters column columns complex compute conditions consumption cost costs count customers data database databases dataset datasets date decimal decisions denormalization denormalize desc disk distribution efficiency efficient employees ensure executed execution explain explicit fast faster filtering footer.html for frequently full group grouped hash help id importance improvement index indexes indexing inefficiencies inserts int join joins key keys large leverage limit maintenance materialized memory menu.html merge multiple mv_sales_summary name names need nodes number offset olap on only operational optimization optimized optimizer optimizing order partition partitioned partitioning performance plan practices precomputed price product_id products proper queries query range redshift reducing regular regularly resource result results rows run sale_date sales scalable scan scans segments select single size smaller snowflake snowflake's sort space specific sql statistics stores suboptimal subset sum(sales systems table tables tasks techniques that the them times title=\"sql tools total_sales types updates use using utilize view views virtual warehouses where which you }",
        "url": "/data/Query-Performance.html"
    },
    {
        "id": 87,
        "title": "sql_overview.html",
        "content": "(dcl) (pl/sql (tcl) , 3. 4. 5. a ability access additional all an analysis analytical and by category changes characteristics columns command commands common commonly complex control cube current data database databases dcl ddl definition delete dml dql drop elements existing extensions functionality functions grant group index indexes insert inserting integrity isolation it its language level modifies more most new object objects one or oracle other own pl/sql point privileges procedural purpose query rank records relational removes retrieval roles rollback rollup row_number rows schemas select server significance specific specifies sql structure structured structures t table tables tcl that the their transaction transaction's transactions types update used users variants which window you",
        "url": "/data/sql_overview.html"
    },
    {
        "id": 88,
        "title": "Apache-Kafka.html",
        "content": "% a aggregation an analytics and apache application architecture availability brokers cases cluster communication compaction confluent connect consumers data decoupled delivery different disk durability each easy ecosystem exactly external fault footer.html hardware high how immutable integration integrations it its kafka kafka's key large latency latency: latest library load log long low menu.html messages messaging minimal more multiple once only options ordered, partition partitions parts policies processing producers publish range real records regist requirements retention scaling schema semantics sequence servers sinks sources storage stream streams subscribe system systems that the throughput time tolerance tools topics use value volumes which wide you",
        "url": "/data/Apache-Kafka.html"
    },
    {
        "id": 89,
        "title": "data-warehouse-architecture.html",
        "content": "% %} (bottom (top and approach architecture architecture\" centralized data denormalized design dimensional down enterprise flexible footer.html inmon integration kimball marts menu.html modeling scalable up warehouse",
        "url": "/data/data-warehouse-architecture.html"
    }
]
