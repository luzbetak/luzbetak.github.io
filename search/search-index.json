[
    {
        "id": 1,
        "title": "TF-IDF.html",
        "content": "absent across add also another appear appears array beyond breakdown column common content convert corpus data decimals depending distinguishing doc docs document documents either entire enumerate equally even example f feature fit frequency frequent frequently get go high higher highest highly however idf import importance important include indicates initialize inverse key learning less limit list low lower machine making many matrix meaningful model multiple names normalized particular practice present prettytable print python range ranges ranking reduced reduces reducing relevant result round rounded row rows sample score scores see shared since slightly somewhat specific stay still summary table term terms text tf tfidfvectorizer theory though transform twice typically unique uniqueness upper value values vectorizer word words",
        "url": "/TF-IDF.html"
    },
    {
        "id": 2,
        "title": "Retrieval-Augmented-Generation.html",
        "content": "accurate across algorithm allowing another answers augment augmented base based bert build building builds cases chatbot combines components conn contextual create cur customer database datasets dimensional distribute document documents embedding embeddings enables engines enhances ensure etc extension external f find finds following generate generated generation generator gpt high id import include indexing inform information informed inputs insert install key knowledge large like limit making management minilm model models multiple necessary need nodes optimization order outputs parallel performance pgvector pgvectorscale postgresql primary print process prompt provide pt python queries query question rag references relevant repositories response responses retrieval retrieve retrieved retriever retrieving scale search searches select sentencetransformer serial setting similarity solution step storage store suitable support synthesize systems table tasks text tokenizer transformers tutorial use user uses using values vector vectors via",
        "url": "/Retrieval-Augmented-Generation.html"
    },
    {
        "id": 3,
        "title": "Tensors-Machine-Learning.html",
        "content": "access additional allow also array average building calls channels code color column commonly complex computation computations compute create creating data deep depth dimensional dimensions efficient efficiently element elements embedded enable essence even example excellent fill foundation frameworks function functionality functions fundamental generalization gpus heavily height high images import include index indexing key language learning library like list machine manipulate manipulating matrices matrix mean models multi multidimensional natural note np number numbers numerical numpy operation operations output perform points powerful print process provides python pytorch rely remember represent row scalar similar simple single solid spaces specific starts structure sum summation support syntax table tensor tensorflow tensors think time total training us used using value values vector vectors videos width words working",
        "url": "/Tensors-Machine-Learning.html"
    },
    {
        "id": 4,
        "title": "Random-Forest-Classifier-Model.html",
        "content": "access accordingly account accuracy address adjusted apply approach arbitrary authentication automation averaging balance based belongs bill billing build cancel card case categories categorize category change charge charged classification classifier classifieri classify clf code collection consists contact convert corresponds create credentials cryptocurrency customer damaged data date decision declined delivery depth details device df dictionary different discount display dispute either email enable ensure ensures etc evaluate every example exchange expected expires f factor feature features final find follows forest forgot generalization generator get gift given going happen happens helps higher history hyperparameters idf import improves include incoming individual information input internal invoice item labels leaf length list lists login made make manual maximum method methods minimum missing model nearest new node notifications number numerical occurs omit options order output outstanding overfitting pandas parameter particularly password pay payment pd pin plan policy predict predicted prediction predictions prettytable print product proportion purchase purchased python queries query question questions random randomforestclassifier receive redeem reduce refund remaining report represent reproducibility required reset return route run samples see seed set sets shipping ships specifies split step store subscription subset support supported suspended table target task technical test testing text textual tf tfidfvectorizer time track train trained training tree trees troubleshoot twice two understand unsubscribe update upgrade use used useful user using value various vectorize vectorized vectorizer verify view want warranty",
        "url": "/Random-Forest-Classifier-Model.html"
    },
    {
        "id": 5,
        "title": "Scikit-learn.html",
        "content": "access accessible accuracy actions actual algorithms allows analysis apis auc automatically available beginners behaves best box built bunch calculate calculates categorical classifier clf clustering code comparison contains corresponding cross data dataset datasets dbscan decision dictionary dimensionality directly documented download efficient encoding evaluating explain explaining external f features fetches files flowers following forest forests function good grid handling helps high highly holds hyperparameter identical import include included initialize initializes integration iris joblib k key label labels later learn learning length libraries library like linear load loads machine machines make manually match matplotlib means metrics missing model modeling models need needed normalization numpy object offers one open output overview pandas pca perfectly performance performing performs persistence petal pickle points popular precision predicted predictions predicts preprocessing print professionals provided providing python random randomforestclassifier range recall reduction regression results right roc sample samples saving scaling science scikit scipy search see selecting selection sepal set sets setup several show simple sne source specific split splits structured summary supervised support svm target test testing tools top toy train trained training trains trees tuning unsupervised use used using validation values variables variety vector via vs well wide widely width works x yes zip",
        "url": "/Scikit-learn.html"
    },
    {
        "id": 6,
        "title": "Vector-Database.html",
        "content": "abnormal accuracy acts add affects ai allowing angular annoy annoyindex anomalies anomaly api apis applications approximate associated assumes audio based behavior bert billions build built capture case cases characteristics classification client clustering code collection common connect connection connects content control cosine could count cpu create creates critical data database databases datasets db dbscan deleted deleting dense designed detecting detection developed dimension dimensional dimensionality directory display distance distances documents driven easy ecosystem efficiency efficient efficiently embedding embeddings encoding enumerate environments essential euclidean event example explanation extensively external f facebook faiss fast faster feature features file find finding first fit flat focuses format friendly full function gcp generate generated gives gpt gpu handle handling high highly hnsw hosted hosting id ideal identify ids image images import include index indices initialization initialize initializes insert inserted inserts install integrate integrates integration inverted item items ivf k key keywords labels lancedb language large latency learning library like limit local locally longer low machine making management manhattan match meaning means memory metadata metric metrics ml models modern natural nearest need needed needs neighbor neighbors new nlp nn np numerical numpy offers often open optimized oriented outliers overview path perform performance performing performs pinecone pip pipelines play points print processing production products providing python queries query querying random randomly range rather real recommend recommendation regression remote represent representing required result results retrieval retrieve retrieves role run scalability scale scales scenarios score search searches self semantic semantically services similar similarity since situations small source space specialized specifically specified specify spotify steps storage store stored storing supports systems table tasks techniques text throughput time top trade traditional transaction tree type types unusual upserting us usage use used user users uses using various vec vector vectors version visual want well wide willing within without workloads world",
        "url": "/Vector-Database.html"
    },
    {
        "id": 7,
        "title": "Keras.html",
        "content": "abstracting accessible accuracy across activation adam adjusting algorithms allowing allows analysis api apply artificial backed backpropagation based basic best biases binary blocks build building calculating called case channel channels churn class classes classification classifying cnn cnns cntk code colored combining compile complex complexities complexity components computation computational connected connections consistent convolution convolutional core corners correct corresponds cpu create customer cycle data dataset decision deep default define dense detect detection developers diagnosis differences different digit digits dimensional dimensions ease edges effective efficient either enabling encode error etc evaluate eventually example excellent extensible f facial faster feature features fed feedback feedforward filters final fit flatten flattened flattening flexibility flexible flows fnn form forward frameworks fraud friendly fully function functional functions generate generation gpu gpus grayscale grid handwriting handwritten hardware helping hidden hierarchies high highlights highly hot ideal image images import important include including information input introduce involves keras kernels key labels layer layers learn learning learns level library like linear linearity load loops loss low lstm make making many map maps matter max medical metrics minimal minimize mnist model models modular multi network networks neural node nodes non normalize np number numpy object objects occurs offers one open operations optimizer optimizers output overview parameters part particularly passes patterns performance pixel pixels platform pooling powerful prediction predictions predicts primarily print probabilities process processing produces propagation provides quickly receives recognition recognizing rectified reduce reducing reinforcement related relu reshape retain reusing rgb run runs scaler sequential sets sgd sigmoid similar simple size softmax source spatial spatially specialized split stack standardscaler structure structured suitable summary support synthetic tabular takes task tasks tensorflow test testing text textures theano top train training type typically unit use used user uses using variety vector versatile video weights x",
        "url": "/Keras.html"
    },
    {
        "id": 8,
        "title": "Gunning-Fog-Index.html",
        "content": "account aeiouy algorithms analysis analyzing applications average avoid based calculated calculates calculation code complex considered content count counted counts creation def defined difficult division document e education else ending english estimates example exclamation explanation expressions f first fog following formal formula function general given gunning higher implementation import include index language len length levels mark marks natural needed nlp number one optimal output patterns percentage period print processing provides punctuation python question r range rank read readability readership reading regular requiring return score sentence sentences splitting steps suggests syllables takes test text texts three understand usage used useful using various vowel vowels way word words years zero",
        "url": "/Gunning-Fog-Index.html"
    },
    {
        "id": 9,
        "title": "PyTorch-Sentiment-Analysis-Model.html",
        "content": "absolutely accuracy acting add amazing analysis apply awful backpropagation base batch bcewithlogitsloss bertmodel berttokenizer best book boring brilliant build class classification complete compute connected convert criterion dataloader dataset debugging decent def define dimension direction disliked else ensure epoch evaluation ever every example examples existing expanded experience extra false fantastic film forward fully function get good great hated horrible idx import include inference initialize input inspect int label labels layers len linearity load loaded logits loop loss love loved masterpiece minute mode model movie negative never nn non optimizer optional output outputs padding performance performances plot positive prediction predictions preds prepare preprocess prettytable print python quite range raw really remove report return run save saved second seen self sentiment sentimentclassifier set sets sigmoid skip split squeeze statement statements story storytelling super superb table terrible test testing text textdataset texts threshold time tokenize tokenizer tokens torch train trained training transformers true truncation two uncased use using values want waste watch worst x year",
        "url": "/PyTorch-Sentiment-Analysis-Model.html"
    },
    {
        "id": 10,
        "title": "Time-Complexity-Big-O-Notation.html",
        "content": "access accessing achieved addition adjacency adjusted algorithm array automatically average avl b balance balanced balancing bfs big binary black brute bst bubble case cases certain change children collisions common complexities complexity computer constant critical data databases delete deletion deletions describe design dfs division done double doubles due e edges efficiency efficient element elements ensures ensuring example except execution exponential expresses extra factor factorial factorially file fixed focusing force generating graph grows hash head heap height include increases index indexing inherently input insert inserted insertion insertions kept kevin keys leaf least level like linear linearithmic linked list log logarithmic loop low luzbetak maintain maintaining max memory merge merging min minimum much n nature necessary needed nested new nodes notation number operations optimize particularly performance performs permutations possible priority problem process properties proportionally quadratic quadratically queue quicksort rebalancing red reduces remain remains require required resizing retrieval root runtime salesman scenario science search selection self set size solving sort sorted space splitting step storage structure structures suited systems table time traveling traversing tree trees unbalanced use used using v vertices way well worst",
        "url": "/Time-Complexity-Big-O-Notation.html"
    },
    {
        "id": 11,
        "title": "PyTorch.html",
        "content": "acceleration among apply arrays audio autograd automatic backward basic build builds built class classes classifying comes common computation computational compute computer connected contains create criterion custom dataset datasets debug deep def define developers differentiation digits dimensional dynamic dynamically easier epoch example extensibility faster features final finished flatten flexible forward fully function gpu gradients graphs image implement import include includes initialize inputs key labels language layer layers learning len libraries library load logits loop loss machine making mnist model models modify multi natural net network networks neural nlp nn numpy open optim optimization optimize optimizer output outputs pass platform popular pre preprocess print processing provides python pytorch range relu researchers return self similar simple source speech strong super support tasks techniques tensors torch torchaudio torchtext torchvision training trainloader trainset transform transforms use used uses variety various vision wide x zero",
        "url": "/PyTorch.html"
    },
    {
        "id": 12,
        "title": "Python-Syntax-Highlighting.html",
        "content": "add age b bash cd computer def desc echo f fmt func golang greet hello highlighting import include int kevin luzbetak main mkdir name order package print python result return science select sql syntax users world",
        "url": "/Python-Syntax-Highlighting.html"
    },
    {
        "id": 13,
        "title": "Managed-External-Live-Tables.html",
        "content": "actual age allow automated automates automation aws azure batch blob building case checks cloud col controlled create creating data databricks datasets dbfs def default deleted delta dependencies designed differences dlt drop dropped etl example execution external externally feature file files framework fully handling hdfs id import include ingestion int internal key lifecycle live location manage managed management manages managing metadata monitoring name optimizing output persistent pipeline pipelines processing quality real remains return shared storage store stored streaming string system table tables tasks temporary time transformation underlying use users using within workflows",
        "url": "/bricks/Managed-External-Live-Tables.html"
    },
    {
        "id": 14,
        "title": "PySpark-Coding-Examples.html",
        "content": "add age aggregate aggregation alice allows apply ascending average avg back based bob calculate categories category cathy centralized clause code coding col column commands computer condition count create csv customers data databricks dataframe dataframes deep def default define defined delta department desc descending df distinct dive duplicate email engineering example exercises explode false file fill filter find finds format formats function great greater group guide handling header hello hr id import include inner introduction join kevin lake lakes let limit load loading luzbetak manage marketing might missing name names none number occurrences operation order orderby parquet people perform prefix price processed pyspark queries read records repository result retrieves return rows salaries salary sales sample save scale science select show similar spark sparksession specific split sql start store stringtype structured sum text top total transformation transformations true two udf unique unstructured use user using value values various within word words world write writing",
        "url": "/bricks/PySpark-Coding-Examples.html"
    },
    {
        "id": 15,
        "title": "RDBMS-Schemas.html",
        "content": "allows based central charts child clear common complex complexity connected connections constellation contains data database databases denormalized derived dimension dimensions directly due fact faster fewer flat following foundational galaxy hierarchical include including integrity involves joins like make management many marts multiple needs network non normalizes offers often organizational organizes parent performance queries query rdbms reduces redundancy related relational relationship relationships require schema schemas several share similar simple simpler single small snowflake specific star structure systems table tables tree typically used utilized warehouses without",
        "url": "/bricks/RDBMS-Schemas.html"
    },
    {
        "id": 16,
        "title": "PySpark-Data-Streaming.html",
        "content": "abstraction add adjust allowing allows also analyzing api application apply averages avoid backpressure based batch batches capabilities changing checkpointing checkpoints cluster code computation consistent continuous core counts crucial data dataframes dataset datasets declarative designed development different discretized distributed divided dstream dstreams ease ecosystems enables ensure ensuring evolution fails failures fault flexibility flume frames handle handles handling hardware hdfs horizontally include including information ingest ingested input integrates integrating integration kafka kinesis larger like live logs loss lost maintain meaning means metrics micro network node nodes operations original overwhelming patterns period periodically process processed processing provides providing pyspark rate rdd rdds real recomputed recovering represents resilience resilient resources restarting running save scalability scale seamlessly sensor simplifying sliding small sockets source sources spark specific sql state stateful stream streaming streams structured support supports system tcp time tolerance tolerant tracking transformations trends tweets update use useful using various volumes window within without",
        "url": "/bricks/PySpark-Data-Streaming.html"
    },
    {
        "id": 17,
        "title": "Databricks-Delta-Lake.html",
        "content": "acid across aggregated allowing allows analysis approach architecture atomicity audits availability batch beneficial big broad bronze capabilities categorizing cleaned collaboration concurrent connector consistency controlled copy crucial data databricks datasets debugging delta design duplicate durability ecosystem efficient enables ensures ensuring even external failures fast feature flexibility flink follows given gold handling historical improving include integrates involving isolation lake large lets like management massive medallion metadata modifications moves need offers open optimized organizations partners platforms point presto processing protocol provides quality queries query querying raw refined reliability reproducing requires scalable scale scenarios seamless secure securely share sharing silver single streaming streamlines support supports tiers time tools transaction transactions travel trino types unified users using various versioning versions without workloads",
        "url": "/bricks/Databricks-Delta-Lake.html"
    },
    {
        "id": 18,
        "title": "RDBMS-Snowflake-Schema.html",
        "content": "additional advanced approach attributes become beneficial breaking broken central characteristics comes complex complexity connected cost costs critical crucial data database date day denormalized design difference differences dimension dimensions directly due duplicated easier easy eliminate ensuring environments even example extension fact faster fewer forming handle harder help higher however important improve include increased indexing integrity involves joins key layers lead leads level levels like maintain maintaining make makes may meaning means might minimizing mitigated modeling month multiple navigate need normalization normalized normalizes normalizing often organizing overall perform performance piece priority process queries query querying reduced reduces reducing redundancy related relational relationships relatively require required result resulting retrieve save scenarios schema separate several shape simple simpler simplicity since single slower snowflake space star storage stored straightforward structure structured sub summary table tables techniques tend typically understand usage use used useful using variation warehouses warehousing year",
        "url": "/bricks/RDBMS-Snowflake-Schema.html"
    },
    {
        "id": 19,
        "title": "etl-pipeline.html",
        "content": "according accuracy accurately adding additional aggregation analysis apis application applying averages begins business calculating calculations choosing cleaning cloud common connectivity consistency context conversion convert counts crucial csv currencies data database databases date destination different downtime duplicates efficiently enhancing enrichment ensure ensuring error errors etc etl extract extracted extraction failures files filtering format formats full grouping handle handling include incremental information integrity involves irrelevant issues joining json key lake large like load loading logic managing minimize missing monitoring needs occur optimization organization performance points prepare process reference reloading removing resource retries rules services sources specific standardizing structure summarizing sums system tables target transform transformed usage values various volumes warehouse xml",
        "url": "/bricks/etl-pipeline.html"
    },
    {
        "id": 20,
        "title": "Databricks-PySpark.html",
        "content": "abstracting abstractions abstracts access acid across action advantages age aggregation algorithms allocation allow allowing among analysis analysts analytics apache api applications automate automatically aws azure based big build built business called capabilities cases city cloud cluster clusters code collaborate collaboration collaborative collect collections complexity computation computed computing count csv data databases databricks dataframe dataframes datasets deep delta deployment designed df distributed distributing dive efficiency efficient enables enabling engineers environment etl evaluated evaluation example execution experiment extract fault features file fosters full fundamental gcp handling high highly import improving include includes initialize inside insights integrates integration intelligence iterative key lake large layer lazily lazy learning level leverage library load machine machines making managed management manages manipulation manner meaning members memory metadata mlflow mllib model models multiple notebook notebooks often open operated operations optimization optimizes optimizing overview pandas parallel perform performance performs pipelines platform process processing provide provides provisions pyspark python queries rdds real resilient resource resources results runs scalability scalable scale scales scientists seamlessly services session sharing show similar source spark sparksession storage stream streaming streams structure structured supports system tasks team time tolerance tolerant top tracking transactions transform transformation true use user users using versioning visualizations work workflows working workload workloads",
        "url": "/bricks/Databricks-PySpark.html"
    },
    {
        "id": 21,
        "title": "PySpark-Pivot-Table.html",
        "content": "agg aggregate aggregates aggregating alice allowing analytics application average based bob categorical code column columns combination context count create creates data dataframe demonstrates df dictionaries digestible east employee especially etc example explanation following format functions github group groupby groups import include information kevin like list look luzbetak multiple new north one original output overview pages per pivot pivoted provided purposes pyspark region reporting reshaping sales sample show something south spark sparksession sum summarizing summing table tool transforms unique used useful using value values west would",
        "url": "/bricks/PySpark-Pivot-Table.html"
    },
    {
        "id": 22,
        "title": "RDBMS-Star-Schema.html",
        "content": "advantages agegroup amounts analysis attributes best category central commerce components consider contains customer customerid customername data database date dateid dates de descriptive design details dimension disadvantages e easy efficient example fact facts flexibility foreign include increased joins keys lead limited links location month named names needed normalization number optimized performance popular price product productid productname quantitative quantities quarter queries query rdbms reduces redundancy region related requirements resemblance sales schema simple simplicity star storage store storeid storename suited surrounded table tables understand used warehouse warehousing year",
        "url": "/bricks/RDBMS-Star-Schema.html"
    },
    {
        "id": 23,
        "title": "PySpark-SQL-Functions-Parquet.html",
        "content": "add age aggregate aggregation alice allows apply ascending average avg back based bob calculate categories category cathy centralized clause code col column commands condition count create csv customers data databricks dataframe dataframes deep def default define defined delta department desc descending df distinct dive duplicate email engineering example explode false file fill filter find finds format formats function functions great greater group guide handling header hello hr id import include inner introduction join lake lakes let limit load loading manage marketing might missing name names none number occurrences operation order orderby parquet people perform prefix price processed pyspark queries read records repository result retrieves return rows salaries salary sales sample save scale select show similar spark sparksession specific split sql start store stringtype structured sum text top total transformation transformations true two udf unique unstructured use user using value values various within word words world write writing",
        "url": "/bricks/PySpark-SQL-Functions-Parquet.html"
    },
    {
        "id": 24,
        "title": "Medallion-Architecture.html",
        "content": "accommodate accurate adopting advanced aggregated aggregating aggregation aggregations aiding allowing amounts analysts analytics applying approach architecture availability available avro based benefits big bronze business cases category cleaned cleaning clear col column columns common complex compute conflicts consistency consistent containing contains cost count csv dashboards data dataset datasets date days decision delta denormalized description designed different directly divides domain duplicates efficiently electronics end engineers ensure ensuring errors facilitate filter filtering final finance flexibility focus form formats fully future gold governance grocery grouped handles handling helps high highly implementations implemented improvements improves improving include incomplete information ingested ingestion insights intermediate involves joining json kpis lake large last latency layer layered layers learning levels leveraging like lineage logic machine made make making managing may medallion might minimal minimize missing model models moves multiple new number often operations optimize optimized orders organizations original overview overwrite parquet partitioning paths perform performance performing petabytes practice pre primary processed processing progresses provides providing purpose pyspark quality query range raw ready real records reducing referred refined refining removing reporting representing represents result sales scalability scalable scale scientists separate separation sets silver sizes source sources spark specific stage stages status store stored structured sum supporting systems tables teams terabytes three tiers today total traceability tracking training transformation transformations transformed transforming two typically undergoes unfiltered unprocessed usable use used users validated values various without work world",
        "url": "/bricks/Medallion-Architecture.html"
    },
    {
        "id": 25,
        "title": "Medallion-Architecture-Partitioning-Code.html",
        "content": "agg aggregated aggregation aggregations alias amounts appname architecture bronze category cleaning cluster col column columns compute config containing count data dataset date days delta description dropduplicates electronics filter format getorcreate gold grocery groupby grouped include ingestion isnotnull json last layer layers load medallion mode new node number optimized orders original overwrite partitioning paths perform performing pyspark raw read repartition reporting result sales save silver spark status sum tables three today total transformation two write",
        "url": "/bricks/Medallion-Architecture-Partitioning-Code.html"
    },
    {
        "id": 26,
        "title": "Managed-External-Tables.html",
        "content": "automatically case cases cleaning cleans control controlled controls create data database deleted delta determines directory dropped dropping external hdfs ideal include including inside intact lake lifecycle location longer manage managed management manages metadata needed often outside path physical remains remove removed resides retain shared specified storage stored suitable summary system systems table tables typically unmanaged use user want",
        "url": "/bricks/Managed-External-Tables.html"
    },
    {
        "id": 27,
        "title": "Column-Shuffle-Repartition.html",
        "content": "across aggregations allows alternatives among assigns attempts balance balancing based call case category cluster coalesce col column data default depend distribute distribution efficiency efficient ensures ensuring equal especially even evenly example filtering following full goal grouping guarantee helps include intention joining joins key large later like load logic multiple n nodes number operations optimized optimizing oversized parallel partition partitions perfectly performs placed points possible processing pyspark randomly reasonable redistribute redistributes redistributing reduces repartition repartitioning repartitions rows share shuffle shuffling skewed smaller spark specific specified specifying splits syntax typically unlike use used useful using value values without working works writing",
        "url": "/bricks/Column-Shuffle-Repartition.html"
    },
    {
        "id": 28,
        "title": "Optimizing-Join-Queries.html",
        "content": "access accessed active adding address adequate allocated allocation allows amount analysis analyzing anomalies apply architectures archive avoid based batch batches bottlenecks breaking buffer buffering cache caching check clause clauses columns common complex composite concurrently condition conditions consider considerations constraints continuous correct cost costly creating criterion crucial ctes data database datasets date dealing defragmentation degrade denormalization denormalizing depending design different dimension disk disks done early efficient enable ensure ensuring environment especially evaluate example excessive executed executing execution expensive explain expressions fact faster files filter filtering filters first flattened flexible foreign fragmentation frequently full functions given handle handling hardware hash help histograms however identify impact implement include incorrect increased index indexed indexes indexing infrastructure inner insights insufficient involves join joined joins kept key large lead left legacy less like limited longer look loop maintain maintenance management materialized memory merge method methods might mindful model monitor monitoring move multiple mysql need needed nested normalized old older ones operation operations optimization optimize optimizing oracle order parallel partition partitioning parts perform performance plan pools possible potential precompute present prevent processing processors proper properly pruning purge queried queries query ranges rather real rebuild reduce reduces reducing redundancy redundant region regular regularly remains reorganize repeated resources responsive result results retrieval rewrite rewriting rewritten right run running scalability scanned scans schema segmenting selective separate served set significantly simpler size slow slowing smaller solutions specific speeds ssds star start statistics steps storage store stored struggles subqueries sufficient summary supported supports system table tables tasks techniques time together tools tune tuning type types typically understand unnecessary updating usage use used using views volume warehouse warehousing whether write",
        "url": "/bricks/Optimizing-Join-Queries.html"
    },
    {
        "id": 29,
        "title": "Relational-Databases.html",
        "content": "accepted accuracy acid advantages aggregated algebra allowing allows analysis analytical analytics analyze another apis around atomicity attribute attributes based benefits better brings business caching category central centralized cleaned clear codd collecting column columns committed complex compliance comprehensive concepts connected consistency consolidates constraints current data database databases datasets decision denormalized design designed dice differences dimension durability edgar enabling enforced ensures ensuring entities establishing etl executed extract extracted fact fast features files finance flat foreign frequently functions guarantee guarantees heavy historical holds identifier improve improved include independently indexing integrity intelligence interacting interactively isolation key keys language large load loaded mainly making managing marts match minimize multidimensional multiple normalized nothing often olap oltp one online operational operations optimized organized partitioning performance performing permanently predefined primary process processing properties proposed provides providing purpose purposes queries query querying rarely read reads ready record recorded records reducing redundancy reference refers relational relations relationships reliable reporting represent represents row rows sales schema simple single slice snowflake source sources specific specifically sql standardized star state store stores structure structured subsets suited supporting table tables target techniques theory time transaction transactional transactions transform transformed trend truth unique unlike updated updating used users using valid various view volumes warehouse warehouses warehousing well widely writes",
        "url": "/bricks/Relational-Databases.html"
    },
    {
        "id": 30,
        "title": "PySpark-Lazy-Evaluation.html",
        "content": "ability absolutely across action actions actual acyclic allows amount applied apply avoids benefits build building builds calculations called causing chain cluster collect collected complex computation compute computing concept count create dag data dataframe deferred define delay delaying directed distributed driver efficiency efficient efficiently entire environments etc evaluated evaluation example examples execute executed execution existing external faster fewer filter filtering final flexibility fundamental general graph groupby immediately include instead intermediate jobs key lambda large lazily lazy like logical manner map mapping meaning means memory minimize minimizes necessary need needed new occurred occurs one operations optimization optimizations optimize optimized optimizing outlines perform performance performed pipelines pipelining plan plans point points print process processed processing program pyspark rdd reducing refers reordering required resource resources result results return run saveastextfile scalable scale select sequence several show shuffled simply small spark steps storage subset transformations trigger triggered triggers ultimately unnecessary usage vs without worrying write x",
        "url": "/bricks/PySpark-Lazy-Evaluation.html"
    },
    {
        "id": 31,
        "title": "Delta-Live-Tables.html",
        "content": "abstracting acid age aggregate analytics automate automatic away batch benefits building built cases checks clean cleaned col complex constraints count country creating creation data databricks declarative def define delta dependencies dependency development dlt ease efficiency enforce enforcement ensure enters entire especially etl example execution failure features filtering flexibility framework handle handles handling health immediate import improving include including incremental ingest ingestion integration invalid key lake learning lifecycle lineage live machine managed management manages managing manual monitoring much new observability operational optimizations orchestration output overhead performance pipeline pipelines processed processing provide providing quality raw real reduce reliable return rows rules scalable schema simple simplified simplifies since sources streaming support supports syntax tables time tools top tracking transactions transformation transformations travel use valid visibility way workflow workloads",
        "url": "/bricks/Delta-Live-Tables.html"
    },
    {
        "id": 32,
        "title": "PySpark-Handling-Missing-Data.html",
        "content": "across action algorithms allow allows also analysis analytics apache api applications automatically backward based big binary build building built chosen cluster col collect collections column columns combined common computer computing constant contain create data database dataframes datasets designed developers df different distributed downstream drop dropna dropping easier enables etc evaluated evaluation exceeds executed execution fault features fill filling fillna filter flag flagging forward framework function functions fundamental github handle handling however immutable implemented import imputation imputer imputing include including indicating indicator instance kevin key known language large last lazily lazy learning library lost luzbetak machine machines making manage map mean meaning median memory method methods missing missingness mllib mode model modeling models named nature null numeric objects observation offering open operations optimizations optimize organized pages parallel performance performed performing plan planning popular powerful predict preprocessing process processed processing programming provide provides pyspark python queries query rdds recovering regression relational remove replace replacing requirements resilient rows save scale several similar single sometimes source spark specific specified sql string structure structured subset suitable summary supports system table task techniques tolerance tools transformations use useful using valid value values various whether window within work write",
        "url": "/bricks/PySpark-Handling-Missing-Data.html"
    },
    {
        "id": 33,
        "title": "PySpark-Questions-Answers.html",
        "content": "across action actions algorithms allow allowing allows analyzes answers apache api apis applies automatically big builds called catalyst cluster clusters collect collection columns computation computed computing concept control count create created data database databases dataframe dataframes dataset datasets difference distributed distributing driver easier efficient element elements enables entire evaluated evaluation example execute executed execution existing explain external fault features filter flatmap flattens flow function fundamental github handle high higher highly immediately immutable include inner input instead interface iterative join joins kevin key large lazily lazy level lineage list loading logical low luzbetak machines making manipulation map meaning memory multiple named new objects one open operations optimize optimizer optimizing organized pages pandas parallel parallelism parallelized parallelizing perform performed performs physical plan plans processed processing produce program programming provide provides pyspark python questions rdd rdds relational resilient results return returns rules runs scalable scale semi similar single source spark stands storage structure structured system table tables tasks tolerance tolerant transformation transformations trigger triggered two uses using various ways whereas working",
        "url": "/bricks/PySpark-Questions-Answers.html"
    },
    {
        "id": 34,
        "title": "menu.html",
        "content": "aws data databricks devops github learning machine programming search",
        "url": "/_includes/menu.html"
    },
    {
        "id": 35,
        "title": "footer.html",
        "content": "kevin luzbetak",
        "url": "/_includes/footer.html"
    },
    {
        "id": 36,
        "title": "debugging-kubernetes-performance.html",
        "content": "adjust affect affecting agent also analyze anomalies another api application applications approach appropriately attach autoscaler based benchmark bottlenecks c calico capacity cause check cilium cli clues cluster clusters code collection collects combination communication complex components conclusion conditions congestion container containers cpu crashes crashing create dashboard dashboards debugging degradation dependencies deploy deployments describe description detailed details different disk displays distributed due effectively ensure ensuring entire environment error errors events exhaustion experience external f failed features fetches fio flow flows frequently functions get go grafana graphical handle health help high historical horizontal hotspots hpa htop hubble identify include including indicate indicators information inside inspect install installed integrate interactions involved iops iotop issue issues istio java journalctl jvm key kubectl kubelet kubernetes latencies latency lead leaks level like load logs look loss measure memory mesh meshes messages metrics might misconfigurations monitor monitoring mounts n name namespace namespaces native nature network networking node nodes object objects observability offer oomkilled open operations overview packet performance persistentvolumeclaim persistentvolumeclaims pinpoint planning plugins pod pods policies policy pprof pressure primary probes problems profile profiler profiling prometheus provide provides provisioned pvc py python rates real related reliable request requests requires resolve resource resources responding restarts root run running runtime scale scaled scaling scrape search server service set slow slowly smooth snapshot solution solutions source space specific spy ssh stack status steps storage stores swapping symptoms systematic testing tests throttling throughput time tool tools top traces tracing traffic trends troubleshoot u ui understanding unusually usage use used using variety vertical visualization visualize vmstat vpa warnings wasted weave web",
        "url": "/devops/debugging-kubernetes-performance.html"
    },
    {
        "id": 37,
        "title": "github.html",
        "content": "access account actions additionally admin allow allowing allows also another answer answers applications asked assign assigned authentication authorization automate automated automatically back based become best blogs board branch branching bugs build building built cd changes ci clone cloud code codebase coding collaborate collaboration comment commit common community complete conclusion conducted configured conflicts content continuous contribute contributing contributions contributor contributors control copy core create created creates defined deploy deploying deployment developer developers development directly directory discover discuss documentation enabling engage ensuring environments essential experience feature features files focusing followers fork forked forking forks git github global group handle history host hosting include includes including incorporated integrated integrates integration interview interviews issue issues job kanban key level like lines local machine main maintain maintainer make manage management master members merge merged merging method model modern modifications oauth offers often one open organization organizations original others overview owner pages party permissions personal pipelines platform platforms points popular powerful practices prepared primarily process production project projects propose provide providers provides pull push pushed quality questions range rbac ready related remote repo repositories repository request requests resolving resources review reviewing reviews revisions role roles run saml separate services site sites social software source stars static stored stores style submit submitting suggest supports target tasks team teams testing tests third tool tools track tracking understanding update use used users uses using version web websites wide wikis within work workflow workflows yaml",
        "url": "/devops/github.html"
    },
    {
        "id": 38,
        "title": "general-101.html",
        "content": "ability abstracting abstractions abstracts acceptable accepted access account accuracy acid across action acts acyclic added adheres advantages age aggregated aggregation aid airflow alerts algebra algorithms aligns allocation allow allowing allows altering amazon among amounts analysis analysts analytical analytics analyze another apache api apis applications around arranged atomicity attempt attribute attributes automate automated automates automatic automatically automating automation aws azure based basis batch benefits best better big branch branches branching brings bug build building built business caching called capabilities capacity cases category cause cd central centralized certain chance change changes checking checks ci city cleaned clear cloud cloudformation cloudwatch cluster clustering clusters codd code codebase collaborate collaborated collaboration collaborative collect collecting collection collections column columns commit commits committed common commonly completed complex complexity compliance comprehensive computation compute computed computing concepts conditions configuration configurations conflicting conflicts connected considerations consistency consistent consists consolidates constraints contain container containerized contains continue continuous control count critical cross crucial csv current custom cyclic dag dags data database databases databricks datadog dataframe dataframes dataset datasets date dates decision deep define defined defines delay deleted delta demand denormalized depend dependencies deploy deployed deploying deployment design designed detect developers development df diagnosing dice differences different dimension directed distributed distributing dive docker documentation downstream driven due duplicated duplication durability easy edgar efficiency efficient efficiently elastic empty enables enabling encryption end enforced engineering engineers ensure ensures ensuring entire entities environment environments equipped error errors essential establishing etl evaluated evaluation event events every example executed execution exist expectations expected experiment extract extracted fact fail failed fails failure failures fall fast fault feature features field fields file files finance fixes flat flexibility flows focus foreign formal formats fosters frequently full functions fundamental gcp general git github gitlab going graceful gracefully graphs great guarantee guarantees handle handled handling heavy help helps high highly historical holds hotfix iac iam idempotency idempotent identifier identifiers identity implement import improve improved improving include includes including incoming inconsistent increasing independent independently indexing infrastructure initialize inside insights instances integrated integrates integrating integration integrity intelligence interacting interactively intervals intervention introducing involves isolation issue issues iteration iterative jenkins key keys kubernetes lake lambda language large layer lazily lazy learning level leverage library like live load loaded location logging logic long luigi machine machines made main mainly maintain maintaining making manage managed management manages managing manipulation manner manual marts match may meaning mechanisms meet members memory merge merging metadata minimize mlflow mllib model models modern modified monitor monitoring mr multidimensional multiple must mysql necessary need new normalized notebook notebooks nothing notify null numbers numeric object occur often olap oltp one online open operated operational operations optimization optimized optimizes optimizing orchestrate orchestrated orchestrating orchestration orchestrator orchestrators order organized others outcome overview pandas parallel parallelism part partitioning parts perform performance performing performs permanently pipeline pipelines platform platforms point popular positive post postgresql pr practices predefined prefect prevent previous prices primary process processes processing produces product production proper properties proposed protection provide provided provides providing provisioning provisions pull purpose purposes push pyspark python quality queries query querying quickly range ranges rapid rarely rather raw rdds rds read reads ready real record recorded records recovery reducing redundancy reference refers regular regularly relational relations relationships reliability reliable reliably repeatability repo reporting repositories repository represent represents request requests rerun resilient resizable resource resources response result results retried retries retry revert review reviews row rows rules run running runs sales scalability scalable scale scales scaling schedule scheduled scheduling schema scientists seamlessly secure security semi sensitive separation sequence server serverless servers service services session set setup shared sharing show similar simple simplicity simultaneously single slice small snapshot snowflake source sources spark sparksession specific specifically specified specifies sql stable stages standardized standards star start state states step storage store stored stores storing strategies strategy stream streaming streams structure structured subsets successfully suited supporting supports system systems table tables target task tasks team teams techniques technologies temporary terraform test testing tests theory thresholds time times tolerance tolerant tool tools top trace traceability track tracked tracking transaction transactional transactions transform transformation transformations transformed travel trend trigger triggering triggers troubleshooting true truth two types typically unique uniqueness unit unlike updated updates updating upstream use used user users using usually valid validated validates validation validations value values various version versioning versions view virtual visualizations volumes warehouse warehouses warehousing way web well widely within without work workflows working workload workloads writes",
        "url": "/devops/general-101.html"
    },
    {
        "id": 39,
        "title": "apache-nifi.html",
        "content": "ability accessible across adopted aggregation allowing allows analysis analyzing apache apis approach audit authentication authorization automate automates automation based cases cloud common complex compliance control create csv custom data databases debugging delivery design designed designing destinations detailed detection developers devices different diverse drag drop efficient efficiently encryption enrichment ensure ensuring environments extensibility extensible external features filtering finance flexibility flow flows formats formatting fraud friendly ftp government handle handling healthcare high highly horizontally http include includes including industries ingest ingestion integrate integration interface iot json kafka key lakes large like lineage log logs making manage manner mechanisms monitor monitoring move moves multi need nifi non offers often open organizations overview particularly perform pipelines plugins premises privacy processing processors programming protocols provenance provides providing range real reliable routing scalability scalable scale scenarios seamless secure securely security services source sources ssl suitable supports system systems tasks technical telecommunications tenant throughput time tool tracks trails transfer transformation transformations use used useful user users valuable various visual volumes warehouses web wide widely xml",
        "url": "/devops/apache-nifi.html"
    },
    {
        "id": 40,
        "title": "docker.html",
        "content": "access across additionally allowing application applications assemble back base build built bundle center cloud commands compared compose consistency consistently container containerization containerized containers contains control create created creation data defining dependencies detect developers development different docker dockerfile ecosystem efficiency eliminates enables enhances ensures environment environments faster features file files filesystem handle host hub image images include included including increased instructions interfere isolated isolation kernel key lightweight like limiting load local lower machine machines maintaining managed management multi multiple needed networking orchestrated orchestration os overhead particularly points portability portable previous problem process processes production provides read repository reproducibility results roll run running runs scalability scaled scaling scanning scope security series share simplifies specifies startup supports swarm system templates testing text times together tools typically useful using variables vast version versioned versions virtual vms vulnerabilities whether within works",
        "url": "/devops/docker.html"
    },
    {
        "id": 41,
        "title": "apache-airflow.html",
        "content": "accessed across acyclic airflow alerting allow allowing allows amounts analysis apache architecture author automate based bashoperator celery changes cloud code communication complex configurable connection correct created custom dag dags data databases debugging define defined delays dependencies details devops different directed distributed dynamic easier enabling engineering ensuring errors etl exchange execute executed execution extensibility extensible external extract fail failure features flow generation graph graphs highly hooks horizontally include including independently integrated integrates integration integrations interface intervals key kubernetes large learning like limits load logging logs machine making manage managing mechanism mechanisms modular monitor monitoring multiple needs node notifications number open operators orchestrate order overview perform pipeline pipelines platform platforms plugins points processes programmatically progress provides python pythonoperator relationships represented represents retry rich run scalability scale scaling schedule scheduling science seamless send sensors services set small source specific specified sql step succeed support supports systems task tasks technologies tracking transform triggers troubleshooting types ui updates used user using various versioning web widely within workers workflow workflows xcoms",
        "url": "/devops/apache-airflow.html"
    },
    {
        "id": 42,
        "title": "Kubernetes.html",
        "content": "abbreviated abstracts access across active actual add added address adjust adjusting allocated allow allowing allows also always amount answer answers api apiversion app application applications apply applying approach appropriate architecture architectures around asked automate automated automates automatically autoscalers autoscaling availability available back balancing based basic beyond called capabilities capacity case cases categorized certificates changes changing charts claims cloud cluster clusters cncf command common communicate communicating communication community compared complex component components computational computing conclusion conditions configmaps configurable configuration configure configured configuring conflicts consistently constraints consume contain container containerized containerport containers contains control controller controllers controls core correctly count coupled cpu cpus create created creates creating creation custom data databases de declarative default define definition deleted dependencies deploy deployable deployment deployments designed desired destroyed dev developed difference different directly discuss distributed distributing divide docker downtime easier easily easy ecosystem efficiency efficient efficiently elk enabling encapsulate enhance enhanced enhances ensure ensures ensuring environment environments ephemeral equals equivalent especially essential etcd example exceed execution expected experience explain expose extend extensibility extensible external f facilitate facto fail failed fails features field file focusing following foundation fractional functionality get gets given google gradually granular greater guaranteed half hand handle handles healing health healthy helm hierarchy high highest highly horizontal horizontally host hosting hpa http https hub ideal identity image images impact implementing important include includes including incrementally individual information infrastructure ingress injected inside instance integrates integrations interview interviews ip isolated isolation issues job keeping kernel key kind kube kubectl kubelet kubernetes labels large least level levels libraries lifecycle lightweight like limit limiting limits listen load loads logging logs lowest machines main maintain maintained maintaining make making manage managed management manager manages managing master match matches matchlabels maximum means mechanism meet memory metadata metrics microservices millicores minimal monitored monitoring much multi multiple name named namespace namespaces native nature necessary need needed needs network new next nginx node nodes number object observed offering often old one ones onto open operate operations operators orchestrate orchestration originally overconsuming overview package parts passwords perform performance performed persist persistent physical place placed plane platform pod pods points policies port portable ports powerful prepared preserved preventing prevents previous private proceed proceeds process processes production progress prometheus provide provides providing proxy pv pvc questions quickly rate rbac reduces registries regularly related reliable remains remove removed replace replaces replacing replicas replicaset replicasets represents request requesting requests require requires resource resources responsible restarted restarts reusable revert reverting revision rich risk role roll rollback rollbacks rolling rollout rollouts rolls routing run running runs runtime scalability scalable scale scales scaling schedule scheduled scheduler schedules scheduling secrets secure securing security select selector self sensitive server services set setting settings share shared simple simplest simplifies simply single smallest snapshots source spec specific specification specifications specified specify ssl stack staging standard standards state stateful statefulset statefulsets stateless status storage store stored strategy successfully sufficient summary support system systems tasks template terminated termination three throttle tightly time tls together tokens tool tools track traditional traffic tries turn two types typically understanding undo unhealthy unique unit units update updated updates updating usage use used users uses using utilization value varying vcpu version versions virtual volume volumes vulnerabilities want way within without work worker workloads would yaml yes",
        "url": "/devops/Kubernetes.html"
    },
    {
        "id": 43,
        "title": "software-delivery.html",
        "content": "across actions another ansible application applications artifact artifactory artifacts aspects assurance automate automates automating automation available aws binaries bitbucket build building categories centralized chef ci circleci cloud cloudformation code coding configuration containerization containerized containers continuous control correctly data define defining delivery deploying deployment designed developing development different directly distributed docker efficiently elasticsearch elk enabling encompasses end ensure ensuring entire environment environments fall finally formats frequent functionality functions git github gitlab grafana hosted include including industry infrastructure initial inside integrated integrates integration intended issue jenkins key kibana kubernetes legacy lifecycle like logging logs logstash making managed management manager manages metrics modern monitoring moving much multiple nexus observing offering often open operates orchestration package paired performance pipelines platform platforms popular practices process production projects prometheus providers provides provisioning puppet quality recipes refers reliability reliable repositories repository retrieves running scalability scaling searching server service shipping software source sources stack staging still stores subversion supports svn system task tasks terraform testing tool tools top tracking travis unit universal updates use used users using various version visualize widely within writing",
        "url": "/devops/software-delivery.html"
    },
    {
        "id": 44,
        "title": "bashrc.html",
        "content": "archive argument basename bashrc clear cmd code copied cp date done e echo edit else eq error eval exclude exec exist extension f fi file filename files find first function grep h include keyword l least linux local ls max mkdir name one open optionally os p path please printf provide py python replace return rg rnw search second sed shift spy text tpy two type xargs",
        "url": "/devops/bashrc.html"
    },
    {
        "id": 45,
        "title": "Python-Search-Algorithms.html",
        "content": "algorithm algorithms another arr array back base binary call case compare considered containing current def discard dividing element elements elif else equal find first found function greater half halves high include index intersection iteration left len length less list lists low merge merging mid middle one output partitioning pivot print process python quick recursively remains repeat repeatedly result return returned right search second selecting set similarly single sort sorted sorting target together two works x",
        "url": "/programming/Python-Search-Algorithms.html"
    },
    {
        "id": 46,
        "title": "Python-String-Algorithms.html",
        "content": "abcbdab access actual algorithm algorithms anagram anagrams analysis another args argument array backtracks base bdab bdcab binary boolean build buzz calculate calculates cases change char character characters check checking checks common compare comparing comparison correction corresponding cost count counts create def defining deleting deletion deletions detection differ distance distances divisible dna dp dynamic edits element elements elif else equal error example f false fill filling finally find first fizz fizzbuzz flaw function hamming hello import include indicating indices initialize input inserting insertion insertions int integer iterate j karolin kathrin kitten language last lawn lcs len length levenshtein list listen longest max maximum measure measuring metric min minimum must n natural needed non none np number numpy olleh one operation order original otherwise output pair palindrome positions print processing programming python raise range reconstruct repeated representation required result return returning returns reverse row second sequence sequences set silent similarity single sitting size slicing sort sorted sorting spell store str string strings subsequence substitution substitutions sum swiss table take takes tracks true two typically usage use used uses value valueerror values versions vowels w whether works world x zip",
        "url": "/programming/Python-String-Algorithms.html"
    },
    {
        "id": 47,
        "title": "Python-Coding-Exercise-Algorithms.html",
        "content": "algorithms bool char character check coding complement count counts def duplicates else enumerate example exercise false fibonacci find first hello include int intersection j largest len list lists max merge merged n non none num number nums olleh output palindrome print problem python range remove repeated return reverse sequence set sorted str string sum swiss target true two usage vowels w world",
        "url": "/programming/Python-Coding-Exercise-Algorithms.html"
    },
    {
        "id": 48,
        "title": "101.html",
        "content": "abcbdab access actual add adds algorithm algorithms anagram anagrams analysis another args argument arr array back backtracks base bdab bdcab binary boolean build buzz calculate calculates call case cases change char character characters check checking checks common compare comparing comparison complement considered containing correction corresponding cost count counts create current def defining deleting deletion deletions designed detection differ discard distance distances dividing divisible dna dp duplicates dynamic edits element elements elif else enumerate equal error example f false fibonacci fill filling finally find finding first fizz fizzbuzz flaw found function greater half halves hamming hello high import include index indicating indices initialize input inserting insertion insertions int integer intersection iterate iterating iteration j karolin kathrin kitten language last lawn lcs left len length less levenshtein list listen lists longest loop low max maximum measure measuring merge merged merging metric mid middle min minimum must n natural needed nested non none np num number numbers numpy nums olleh one operation order original otherwise output pair palindrome partitioning pivot positions print problem process processing programming python quick raise range reconstruct recursive recursively remaining remains remove repeat repeated repeatedly representation required result return returned returning returns reverse right row search second selecting sequence sequences series set silent similarity similarly single sitting size slicing solve sort sorted sorting specific spell store str string strings subsequence substitution substitutions sum swiss table take takes target together tracks true two typically usage use used uses value valueerror values versions vowels w whether works world x zip",
        "url": "/programming/101.html"
    },
    {
        "id": 49,
        "title": "Python-Programming-Language.html",
        "content": "access algorithms analyzing anonymous arguments async asynchronous asyncio attributes await behavior break cases classes closures code collection complexity comprehensions concurrency conditional context continue control counting cprofile create creating csv custom data datetime debugging decorators def default defining dependencies designing development dictionaries differences different django dunder effectively efficient elif else encapsulation environments error etc except exceptions explicitness expressions file files finally flask flexible flow fly frameworks functions garbage generate generator generators gil guide handling higher idiomatic immutable implications importing include inheritance inline iter iterators itertools json key lambda language lazily learn libraries library like list lists loops management managers manages managing math matplotlib memory methods mock mocking modifying modules multiprocessing multithreading mutable next numpy object objects oop open optimization order organizing oriented os packages pandas parameters pass pdb pep performance polymorphism popular practices principles private produce profiling program programming properties public pytest python pythonic read readability readable reading reference requests results return reusing scikit sequence sets similar simplicity simulate special standard statements structures style sys systems test testing text time timeit tools try tuples types understand understanding unit unittest unpacking use using values venv virtual virtualenv ways web write writing yield zen",
        "url": "/programming/Python-Programming-Language.html"
    },
    {
        "id": 50,
        "title": "mysql-lag-function.html",
        "content": "access allows analysis average avg based calculate calculated calculates calculating chronological clause column comparing curdate current daily data date dates day days decimal difference differences ensure ensures excludes explanation filtering filters four function giving include increase increases inner interval lag last mysql null order outer overview particularly places previous price processed purpose query result retrieves round rounds row rows select sequential series set sql time useful valid value values window within",
        "url": "/programming/mysql-lag-function.html"
    },
    {
        "id": 51,
        "title": "Python-Function-OOP-Data-Structure.html",
        "content": "age alice animal anonymous another argument array async asynchronous asyncio attribute attributeerror author await b bark basic bob book buddy c call cat class classes closures code collection common concurrency creates csv data decorators def defining demonstrates dictionaries dictionary different dog dunder dynamic element elements encapsulation example explanation expression f file func function functions generator generators george greet header headers hello hi higher holds immutable import include individual inheritance inline item iterate iterated iterates iterating iterator iterators key keys lambda like line list lists load main memory meow methods msg multiple multiplier multiprocessing multithreading n name next numbers object objects one open order oriented orwell output pair passing person polymorphism print printing private process processes program programming python r raise range read reader reading represents return returns row rows sample self separate sequence set sets simple single skipping sound spaces speak square structures target thread threading threads title tuple tuples unique unordered use using value values within woof would wrapper writing x yield yields",
        "url": "/programming/Python-Function-OOP-Data-Structure.html"
    },
    {
        "id": 52,
        "title": "Sudoku-Board-Verification.html",
        "content": "according also args block board bool calls check checking checks col column columns completed conditions def define definition easy equal equals example execute expected explanation false finishing function given grid grids import include indicating inner int known list lists logic main make matches matrix module numbers otherwise output parameters pass pp pprint pretty print prints provided purpose puzzle python range represented representing represents result return returns row rows rules script since specified standard start steps sub sudoku sum sums total true used using valid value verification verifies verify visualize whether window",
        "url": "/programming/Sudoku-Board-Verification.html"
    },
    {
        "id": 53,
        "title": "Python-Algorithms.html",
        "content": "add adds algorithm algorithms argument arr array check complement def defining duplicates element elements else equal fibonacci finding function include indices integer iterate iterating j len list lists loop merge merged n nested number nums otherwise output print python range recursive remaining remove result return series set sorted sum takes target two use value values works",
        "url": "/programming/Python-Algorithms.html"
    },
    {
        "id": 54,
        "title": "algorithms2.html",
        "content": "args calculate character collections comparing comparison convert counter def deletions distance distances dp dynamic else first hamming import insertions int j len levenshtein libraries lowercase math min n necessary needed np number numpy programming range return returns second string strings substitutions transform two using zip",
        "url": "/programming/algorithms2.html"
    },
    {
        "id": 55,
        "title": "algorithms.html",
        "content": "access add adds algorithm algorithms also anagram anagrams another applications argument arr arrange array article ascending asked back backtracker backtracking base based bellman binary bits bitwise boolean breadth breaking broken bubble buzz calculate call case characters check climb code coding coins combinatorics common commonly compare comparing complement computational computer conclusion considered containing counting cryptography current data def defining descending desired different dijkstra discard discussed distance dividing divisible dp dynamic easier edges effective efficient element elements elif else engineering equal examples explanation factorial fail false fibonacci finally find finding first five fizz fizzbuzz following ford formula found function gcd given graph greater half halves hamming hash high implement implements important include index indicating indices information initialize insertion instead integer integers interviews iterate iterating iteration j key knapsack last left len length less levenshtein lexicographically like linear list longest loop low make manageable matrices matrix max maximum merge merging mid middle multiples multiplication multiply n negative nested networks neural nth number numbers one operations optimization order original otherwise pair palindrome partitioning parts passwords paths perform pivot popular possibilities powerful print problem problems process programming provided python queens quick quickly range real recursion recursive recursively regression related remaining remains repeat repeatedly representation result retrieve return returned returning reverse right row salesman science search second selecting selection series set shortest similarly single size slicing smaller solution solve solving sort sorted sorting specific stairs steps store storing str string strings structures subproblem subproblems subsequence sudoku sum tables take takes target tasks term three together tools traveling traverse traversing trees true trying two types use used user using value values various ways weight whether wide widely works world x",
        "url": "/programming/algorithms.html"
    },
    {
        "id": 56,
        "title": "Fibonacci-Generator.html",
        "content": "b code current def defined defines definition example execution explained fibonacci first function generate generator github include indefinitely infinite iterating iteration kevin loop luzbetak next numbers old pages pauses preserving print produces python range sequence state statement sum takes true two updated updating usage used value values yield yielding",
        "url": "/programming/Fibonacci-Generator.html"
    },
    {
        "id": 57,
        "title": "Amazon-RDS.html",
        "content": "advanced allowing allows amazon analytical api application applications aurora automated automates availability aws az backend backup backups based building calls capabilities cases choice choose cli clicks cloud cloudwatch common commonly compute configure connect console consuming cost costs credentials data database databases demand demands deployments development dms drivers easily easy effective encryption engine engines ensure ensuring enterprise environment environments especially example excellent failover features flexible focus grade hardware high import include including ingestion instance instances integrates integration isolation key kms layers like load looking making managed management mariadb meet microsoft migration mobile models monitor monitoring multi multiple mysql native necessary needed network offers operation optimize oracle organizations patching patterns performance point popular postgresql pricing protect protected provided provides provisioning rds read recoverable recovery relational reliable replicas require reserved resources rest restore running scalability scalable scale scaling seamlessly security server service services set setup several simplifies smaller snapshots solution sql staging standard storage suitable supports tasks testing time tools transit usage use used using vpc warehouses warehousing web workflow zone",
        "url": "/aws/Amazon-RDS.html"
    },
    {
        "id": 58,
        "title": "AWS-CloudWatch.html",
        "content": "access across actions activity alarms alerts allowing allows amazon analyze analyzing anomalies anomaly application applications automate automated automatically automation aws balancers based behavior build cases certain changes cloud cloudwatch collect collected collecting collection collects common comprehensive conditions configure cpu create custom customizable dashboards data databases defined designed detect detection display easier ecosystem efficiency enable enabling entire environment environments errors essential eventbridge events example failure features filters functions gain health help helping identify immediate improving include including infrastructure insights instances integrates integration interface invoking issues key lambda learning load log logs machine maintain making management managing memory met metric metrics monitor monitoring needed notifications observability operational operations optimize optimizing part patterns performance premises problems provide provided provides rds real related reliability resource resources respond response responses restarting running scale scaling seamlessly security send sending service services set setting single specific supports system thresholds time tool track trigger triggering troubleshoot unauthorized unusual usage use using utilization view visibility visual visualize visualizes web well workflow",
        "url": "/aws/AWS-CloudWatch.html"
    },
    {
        "id": 59,
        "title": "Amazon-S3.html",
        "content": "access accessed accessible across allows also amazon amount amounts analysis analytics anywhere api apis applications archival archive archiving athena automated automatically availability available aws backup backups based big bucket buckets cases cdn centers centralizing change class classes cloud cloudfront common compliance computing content control controls cornerstone cost costs custom data datasets deep delivery depending designed different directly disaster dispersed distribution diverse documents durability easily easy effectiveness emr enable encryption end ensuring features fine flexibility frequently geographically glacier grained handle high hosting hours ia iam identity images include including infrequent infrequently integrate integrates integration intelligent internet keep key lakes lambda languages large layers less lifecycle like long lower lowest made make making management many minutes moves multiple must needed needs number object objects offering offers one optimize optimized option options patterns policies pricing processed processing programming provided provides querying range ranging rapid rarely reasons recovery redshift regions reliability replicate replicating replication requests requires rest restful retained retrieval retrieved retrieving scalability scalable scale scaling sdks seamlessly security serverless service services serving simple single sources specific spectrum sql standard static storage stored storing suitable term tiering tiers times tools transfer transit transition two unlimited usage use users using variety various versioning versions videos virtually web wide zone",
        "url": "/aws/Amazon-S3.html"
    },
    {
        "id": 60,
        "title": "ETL-Pipeline-AWS.html",
        "content": "acceleration access across activity aggregate alarms alerts allowing amazon analysis analytical another apache api apis archival arrives auditing auto automatically automation aws back based batch big buckets building calls catalog clean cleaned cloudtrail cloudwatch collect come complex compliance config control create data database databases datasets define defining degrades dependencies dynamodb elastic emr encrypt enrich ensure etl events example executed external extract fails failures flow format frameworks fully functions glue hadoop health iam identity include ingestion inspector internal issues job jobs keep key kinesis kms lake lambda large like load loaded loading log logging logs manage managed management mapreduce metadata monitor monitoring needs nosql option orchestrate orchestration overall party performance pipeline policies process processing purposes queries raw rds real redshift regulations relational reporting requiring resources rest roles run scale scaling schedules schema security sequence service services set simple sources spark specific speed step storage store streams third time track transfer transform transformation transformations transformed transit trigger triggering use various warehouse within workflow",
        "url": "/aws/ETL-Pipeline-AWS.html"
    },
    {
        "id": 61,
        "title": "AWS-EMR.html",
        "content": "access across adjust advanced algorithms allowing allows amazon amounts analysis analytics analyze apache aws back based batch big businesses cases cloud cluster common complete complex configuration control cost costs data datasets dynamodb easily ecosystem effective effectively elastic emr enabling encryption environment etl example features flexible flink framework frameworks freeing hadoop handles hbase hive iam ideal include including infrastructure instances integrates integration isolation job jobs key languages large launch layers learning like machine managed managing mapreduce mllib multiple necessary need needs network optimize options others pay perform performing pig platform presto pricing process processed processing provides provisioning python querying quickly raw real redshift reduce resources rest results run running save scala scalability scalable scale seamlessly security service services shut spark spot sql storage store streaming supported tasks termination time transformations transit tuning underlying use using utilize vast warehousing workflow written",
        "url": "/aws/AWS-EMR.html"
    },
    {
        "id": 62,
        "title": "AWS-Glue-Workflow.html",
        "content": "adding allowing allows amazon analytics arrival automate automatically automation aws based bucket build cases catalog cataloging checking cloudwatch common completion complex comprehensive conditional configuration configure connecting console coordinate correct crawler crawlers create custom data define defining dependencies design destinations different drive driven easier editor enabling ensure error errors etl even event events example execute execution extract fails failure feature features flexibility flow flowchart glue handle handling helps include ingestion integrated integrates integration interface involve issues job jobs key lake like load loading logging logic maintain making manage management metadata monitor monitoring multiple necessary orchestrate orchestration order organize parallel part path paths pipelines predefined previous processes processing progress provides providing real redshift retry run scale scheduled schedules seamlessly sequence set simplifies solution sources specifying start status success tasks tightly time track transform transformation transformations trigger triggered triggers troubleshoot upstream use using various visual warehouse warehousing within workflow workflows",
        "url": "/aws/AWS-Glue-Workflow.html"
    },
    {
        "id": 63,
        "title": "AWS-Kinesis-Data-Streams.html",
        "content": "allowing allows amazon analytics analyze analyzing applications arrives aws cases centralized collect collecting common data designed distributed easily etl event extract features fly full generate handle include ingestion insights integrated integration interactions key kinesis lambda large like log logs metrics monitoring name personalization phase pipeline process processing provide real redshift refer scalability scales service services sources storing streaming streams time tracking use used user various volume",
        "url": "/aws/AWS-Kinesis-Data-Streams.html"
    },
    {
        "id": 64,
        "title": "AWS-Step-Functions.html",
        "content": "access according across allowing allows amazon another api application approvals arranging automate automatically automating automation aws batch build business call calls cases common completes completion complex console control coordinate coordination data days decision define defined defining definition deployment design different distributed dynamodb easily easy ecs editor enables end ensuring environment error errors etl evaluation even example executes execution external extraction failed fails fault features final flowchart function functions glue gracefully handle handling hours human iam ideal include including infrastructure input integrates integration interact interacting interface invoking involve jobs json key lambda language learning like loading long machine making manage management manages microservices model monitoring months multiple orchestrate orchestrates orchestrating orchestration output path perform performs pipelines policies preparation process processes processing provides redshift reliability reliable requiring retries roles run running scalability sdk seamlessly security sent sequence serverless servers service services simplifies span specifying start state states step steps storage stored successfully supports systems task tasks tolerance training transformation transitions trigger use using validation various via visual waiting within without workflow workflows",
        "url": "/aws/AWS-Step-Functions.html"
    },
    {
        "id": 65,
        "title": "AWS.html",
        "content": "access accurate across ad allowing amazon analytics analyze assets athena auditability authorized automatic automatically aws backbone cases catalog cataloged centralized centrally changes common component configuration continuously control controlling core cornerstone crawlers data databases date define designed different directly discover discovery easier easy emr enabling enhances ensuring environment etl evolution example extract extracting fast features flow flows glue governance history hoc iam identity implement include infer information integrates integration interact job jobs keeps key lake lakes landscape leveraging like lineage load loading make making manage management managing manual metadata monitor organize organizing partitions perform pipelines place policies populate powerful process processes queries query redshift reference remains repository scan schema schemas seamlessly search security serves services showing simplifies simplifying single sources spectrum storage stored stores strategy streamline structures tables time tool track tracking transform transformations transforming transforms transparency understand update updates use users using utilizing various versioning warehouse warehouses workflow",
        "url": "/aws/AWS.html"
    },
    {
        "id": 66,
        "title": "AWS-Config-Inspector.html",
        "content": "account across action actionable actions adapting addressing adherence aiding alerts allowing allows analysis applications applying approach assess assesses assessment assessments audit auditing automate automated automatically aws back basis best better breaches bring capabilities cases change changes checks clear cloudtrail common compliance compliant comply config configuration configurations configure consistent continuous continuously controls create current define defined defining demand demonstrate dependencies deployed deployments desired detailed deviations devops disaster documented dss either enable enables enabling enhance ensure ensuring environment evaluate evaluation example execute execution exposed external features findings following generate generated generates governance helping helps highlight historical iam identified identify impacts implementing improve improvement incident incidents include industry insecure inspector integrate integrates integration internal intervention inventory investigate issues key lambda led like logs maintain manage management manual may misconfigurations mitigate monitor monitoring monitors non notifications ongoing operational organizational part patches pci performed pipeline planning policies ports posture potential powerful practices predefined prevent process provide provides providing quickly receive recommendations recommended recorded recording records recovery regular regularly regulations regulatory relationships remains remediate remediation report reporting reports represent requirements resource resources response review risks rules run scanning scans schedule scheduled secure security service services set shows sns software specify standards start state status take targets templates thorough time track troubleshooting understand unpatched updating use view vulnerabilities vulnerability way workflow",
        "url": "/aws/AWS-Config-Inspector.html"
    },
    {
        "id": 67,
        "title": "KMS-Key-Management-Service.html",
        "content": "access across activities additional allowing allows api application applications approach audit auditing authenticity authorized automatic aws behalf best cases centralized cli cloud cloudtrail cmk code common communications compliance compliant configuration consistent console control create created creating cryptographic custom customer customers data define deletion digital directly disrupting dynamodb easier ebs enables enabling encrypt encryption enhance ensure ensuring environment example feature features granular hardware helps highly hsms iam include including industry information integrated integrates integration integrity key keys kms lambda layer level leveraging lifecycle like logging making manage managed management managing master meet minimal modules monitor offering offers ongoing operations others periodically permissions policies practices processed protect protected protection provide providers provides providing range rds regularly regulations regulatory repository requirements rest robust rotating rotation saas scalable sdk seamlessly secure secured securely securing security sensitive service services set signing single solution specific specifying standards store stored storing supports transit unified usage use used users using various verify wide without workflow",
        "url": "/aws/KMS-Key-Management-Service.html"
    },
    {
        "id": 68,
        "title": "Lambda-Serverless-Computing.html",
        "content": "access across actions additionally aggregating ai aliases allows also amazon analyzing api apis application applications arrives assistants automated automatic automatically automation availability aws backend based behavior benefits bring build building c cases changes charges chatbots code combining come common complex comprehend compute computing configure connect consume control cost create custom customize data database databases deploy design development devices different driven dynamically dynamodb easier easily easy effective efficiency efficient enabling ensures ensuring environment environments especially etl even event events executes execution external extract features file files fine flexibility function functions gateway generating go grained handle handling high highly http iam identity images include including incoming information infrastructure input instances integrates integration intervention invocations iot java key lakes lambda languages levels like load log logic maintain makes making manage management managing manual many meaning measured message messages milliseconds modifying multiple need new notifications number orchestrating orchestration overhead packaging parallelize pay per performance permissions persistent point power pricing process processing production programming provided provisioning python queues quickly range rds real records rekognition requests requires requiring resizing resource resources respond response role roles ruby run running runtime runtimes scalability scalable scales scaling scenarios secure sending server serverless servers service services several shifts sns sources specific sqs staging state stateless storage streams support supports system tasks thumbnails tightly time traffic transcoding transform transformation trigger triggered triggers underlying updates updating uploaded usage use used user using variables variety various varying versatile versioning versions videos voice web wide widely without workflows worry",
        "url": "/aws/Lambda-Serverless-Computing.html"
    },
    {
        "id": 69,
        "title": "Auto-Scaling-EC2-EMR.html",
        "content": "action activities adjust adjusting adjusts allocated always amazon amount application applications auto automated automatically availability aws balancing based batch big build capacity cases changes checks cloudwatch cluster clusters common complexity configure consistent continuous cost costs cpu create current custom data day decrease decreases define demand deployment desired devops dictate disaster dynamic dynamically effective effectiveness efficiency efficiently elastic elb emr ensure ensures ensuring environment environments events example failed features flexible fluctuates group groups handle health helps high include increase increases instances integrates integration intensive job jobs key levels like load low maintain maintaining manage mapreduce match maximum memory metrics minimizing minimum monitor necessary needed new nodes number ones operations optimal optimization optimize optimizing patterns pay performance pipelines policies powerful predefined predictable processes processing provides providing quick real recovery reducing regularly replace replacing resource resources response review right run running scale scaling scheduled schedules seamless service services set size solution specific specifying target test time timely times tracking traffic unhealthy unpredictable usage use utilization way web week workflow workload workloads",
        "url": "/aws/Auto-Scaling-EC2-EMR.html"
    },
    {
        "id": 70,
        "title": "AWS-CloudWatch-Events.html",
        "content": "across action actions adjusting adjustments alerting allowing allows alternate api application applications architectures audit auditing audits automate automated automatically automating automation aws backups based building calls cases change changes checks cloud cloudwatch common complex compliance component components conditions configure corrective correctly corresponding create criteria cron custom data define delivers describe detailed driven ecosystem efficiency enables enabling ensure environment error errors event events example execution expected failed features filter filtering flexible function functions generated handle handling health implement improve improving include infrastructure instance instances integrate integrated integrates integration intervals invoking key lambda like load log logging logic logs make making manage mechanisms monitor monitoring near needs notification occur operations orchestrate perform performance periodic powerful processing provides providing real reducing regular regularly relevant reliability resources respond responding response responses restarting retry review robust rotation rule running scaling scheduled scripts seamlessly security sending service services set sns source specific specifies starting state step stream system tailor target tasks time tool trigger triggered triggering triggers troubleshooting unnecessary use useful using via way within workflow workflows",
        "url": "/aws/AWS-CloudWatch-Events.html"
    },
    {
        "id": 71,
        "title": "AWS-Glue-ETL-Service.html",
        "content": "across actual age allowing allows also amazon analysis analytics analyze another apache applies apply args argument aurora automate automates automatically aws back based basic batch bucket build built cases catalog centralize cleaning code commit committed common completion connect context crawlers create creation custom data database databases define definitions demonstrating designed desired different discover discovers discovery distributed easier easy either enabling enriching ensuring environment etl example execution explanation extract features field filter filtering filters format fully getresolvedoptions glue gluecontext helps hood import include includes including infrastructure initialization initialize initialized initializes input integrate integrated integration interact interface intervals intervention involved job jobs joins json keeping key kinesis lakes less like load loaded loading location making manage managed manages manual mapping meaning metadata might moving much name near needed object older optimal output part passed path paths performance preparation prepare preparing process processing provided provides providing provisions python querying range rds reads real records redshift relational replace required resources result results run runs save sc scala scalability scalable scale scales scenarios schedule scheduling schema script seamless serverless service services shows signal simple simplifies simplify sources spark sparkcontext specific step storage store stored stores streaming studio suitable supports sys table target time transform transformation transformations transformed transforming types usage use used using variety various visual warehouses warehousing web wide without work workflow workload write writes writing written x",
        "url": "/aws/AWS-Glue-ETL-Service.html"
    },
    {
        "id": 72,
        "title": "S3-Transfer-Acceleration.html",
        "content": "accelerate accelerated acceleration access additional aligns also amazon amounts anywhere applications around assets associated aws backups benefit benefits bucket budget business case cases changes choice clients close closest cloudfront cloudwatch common compared console content control cost costs critical cross data datasets deadlines deliver desired directly distance distances distant distributed easy edge effective enable enabled enables enabling encrypted end endpoint ensure ensuring especially example existing expected factor far fast faster feature features file files geographically global globally go iam ideal identity impacts improve improved include incurs infrastructure integrates key large latency leverages leveraging location locations long making management media meet metrics monitor nearest need needs network new operations optimized particularly paths performance processing providing quickly reach reduce reducing region regions regularly reliability remains requirements review route routes routing scale scenarios secure security sensitive settings significantly simple solution speed speeding speeds ssl standard streaming time transfer transferring transfers transit turning upload uploaded uploading uploads use users uses using video workflow workflows world",
        "url": "/aws/S3-Transfer-Acceleration.html"
    },
    {
        "id": 73,
        "title": "IAM-Identity-Access-Management.html",
        "content": "access account across actions active activities added addition additional allow allowing allows analyze another api application applications applying assign assumed attach attached audit authenticated authentication authorized aws based best calls cases cloud cloudtrail common compliance compliant consultants continuously control controls create credentials critical cross custom define deny directory enable enables enabling enforce enhance ensure ensuring environment exactly example external factor feature features federation fine form foundational google grained grant granular group groups helping helps iam identity implement implementing include individual integrates interact key lambda least like limited log long made maintain manage management managing meet mfa microsoft monitor multi need needed needing needs one organization organize parties password patterns perform permissions policies policy powerful practices principle privilege protection provide provider providers regulatory related require requirements requiring resources role roles saml second secure securely securing security service services set share specific specify step strict stronger supports tailored temporary term third tool unauthorized use useful user users uses using web without workflow write",
        "url": "/aws/IAM-Identity-Access-Management.html"
    },
    {
        "id": 74,
        "title": "AWS-Glue-Data-Catalog.html",
        "content": "access accurate across ad allowing amazon analytics analyze assets athena auditability authorized automatic automatically aws backbone cases catalog cataloged centralized centrally changes common component configuration continuously control controlling core cornerstone crawlers data databases date define designed different directly discover discovery easier easy emr enabling enhances ensuring environment etl evolution example extract extracting fast features flow flows glue governance history hoc iam identity implement include infer information integrates integration interact job jobs keeps key lake lakes landscape leveraging like lineage load loading make making manage management managing manual metadata monitor organize organizing partitions perform pipelines place policies populate powerful process processes queries query redshift reference remains repository scan schema schemas seamlessly search security serves services showing simplifies simplifying single sources spectrum storage stored stores strategy streamline structures tables time tool track tracking transform transformations transforming transforms transparency understand update updates use users using utilizing various versioning warehouse warehouses workflow",
        "url": "/aws/AWS-Glue-Data-Catalog.html"
    },
    {
        "id": 75,
        "title": "AWS-CloudTrail.html",
        "content": "access account across actions activate activity alarms allows amazon analysis analyze analyzing anomalies api athena audit auditing automated automatically aws based bucket call calls cases change changes cloud cloudtrail cloudwatch command common complete compliance comprehensive configurations configure conjunction console continuously create dashboards data deliver demonstrate detailed detect detecting durable effectively enable enables enabling enhance ensuring environment essential event events examining example external features files forensic gaining governance helping helps historical history identify incident include including industry infrastructure insights integrates integration internal investigate issues key leading like line log logging logs long made maintaining manage management meet monitor monitoring needed notifications operational party patterns perform potential problem provides providing query quickly record recording records regions regulations related relevant requirements resources respond responses retain retention reviewing risk roles sdks secure security sequence service services set specific standards start storage store support taken term third threats time tools track trail troubleshooting unauthorized understand unusual use used user users via visibility within workflow",
        "url": "/aws/AWS-CloudTrail.html"
    },
    {
        "id": 76,
        "title": "AWS-Redshift.html",
        "content": "access across adjust aggregate allowing allows amazon analysis analytical analytics analyze aws back based bi big business businesses cases central clean cloud cluster clusters collected columnar commands common commonly complex compression control copy cost costs data databases datasets decision deliver demand designed destination destinations driven dynamodb easily easy ecosystem effective efficiently emr enables enabling encryption etl even example export familiar fast features fine fully gain gb generate glue grained handle high iam ideal include ingestion insights instances integrates integration intelligence isolation jobs key language large learning like load loaded looker making managed multi needed needing network new node offers organizations parallel pausing performance petabyte petabytes power pricing processed processing purposes queries query querying quickly quicksight real redshift reporting reports reserved resources rest results resuming run save scalable scale seamlessly security semi service services single solution sources sql standard start storage stored structured support supports tableau time tools transform transformation transformed transit use used users uses using various visualizations vpc warehouse warehousing well within without workflow workload workloads",
        "url": "/aws/AWS-Redshift.html"
    },
    {
        "id": 77,
        "title": "Apache-Parquet.html",
        "content": "access across adding aggregating allow allowing amount analytic analytical analytics apache applies architectures aws azure based benefits big bigquery bit breaking cases changes cloud columnar columns common compatibility compress compression costs cutting data datasets designed dictionary distributed easily easy efficient efficiently encoding engines ensures environments especially etl evolution existing expenses fast faster features file files filtering format frameworks go google hadoop highly hive ideal impala improved include ingested integration interoperability key lakes large leading length like lower makes making many massive minimizing modern multiple necessary need new olap operations optimized packing parallel parallelism parquet parts performance pipelines platforms processed processing queries query read reading redshift reduced reduces require retrieval rle run scalability scale schema selecting services size spark specific split splitting storage stored stores suited support supported supports synapse system systems techniques time transformed use using warehousing well wide without workloads",
        "url": "/data/Apache-Parquet.html"
    },
    {
        "id": 78,
        "title": "Apache-Iceberg.html",
        "content": "access acid allowing analytics apache atomic automatically breaking built cases changes cloud columns complex consistent data dataset datasets deletes designed durable dynamic efficiency efficient efficiently eliminating enables enabling ensures entire evolution evolve evolving existing features flexible framework frequent handle hidden highly historical iceberg improving include internally isolated key lakes large making management manages managing manual metadata model models modern need needs object operations optimizes partition partitioning partitions petabyte possible previous queries query querying reliable rewriting scalability scalable scale schema smoothly storage support supports time transactions travel updates upserts use users versioning versions without",
        "url": "/data/Apache-Iceberg.html"
    },
    {
        "id": 79,
        "title": "Graph-Databases-ArgoDB-Neo4j.html",
        "content": "according acid across adapt adds alice analysis analytics apis arangodb argodb argodbexample b based billions bob bolt breakdown case cases class close code complex compliance concept connection consistency create creates cypher data database databases datasets db deep def deliver delivering designed detection different distinct distributed document driver edges effectively efficient ensures ensuring establishes even example excels explanation f fast features find finds fraud friend friends function given graph graphdatabase handle handles handling high ideal import include insights integrates integration key knows language languages large making massive match matching method methods model multi n name networks new node nodes optimized password pattern people performance person platforms popular print processing providing python queries query querying rapid recommendation record referring related relation relationship relationships reliability remains requiring result return scalability scale scaling self session sessions setup shows similar social specialized staticmethod storage syntax systems transactions traversal two tx uri usage use user using value vs way within work workloads would",
        "url": "/data/Graph-Databases-ArgoDB-Neo4j.html"
    },
    {
        "id": 80,
        "title": "GraphQL.html",
        "content": "access accessing accounts acquisition ad administrators ads advertisers allowing allows amount analytics api apis applications apps ask asks audiences authentication automate automated based benefits budgets build building built business businesses campaigns changes chatbots clients comments common commonly communication complex consumed content control conversational create creator credentials customer data define defines description designed developed developers different direct ecosystem efficiency efficient email enable enables end endpoint endpoints engagement environments especially etc event events exactly exposes facebook features fetching fields flexibility flexible friends front functionality generate get gives graph graphql group groups helps id ideal improving include including info information insights instagram instead integrate integrated interact interactions interfaces key language like likes list lists log login lot mainly make makes making manage managing marketers marketing media members messages messaging messenger metrics microservices minimizing mostly multiple name names need network notifications often one optimize page pages part party performance permissions photos platform platforms play posts precise predictability primary professional profile profiles programmatic provide provides public pushing queries query reach read real receive receiving reduces redundant related relationships replies reports request requests require required response rest retrieve run schema sending serve server service sets setting single specific specifically specifying strongly structure structures subscriptions support supports teams third time title titles track transfer typed types unlike update updated updates usage used useful user users uses using validating via videos want way websites whatsapp write",
        "url": "/data/GraphQL.html"
    },
    {
        "id": 81,
        "title": "Large-Scale-Data-Ingestion2.html",
        "content": "acid across additional advanced allows amazon amounts analytics apache application applications automation aws azure based batch beam big building built capabilities capable cloud cluster compliance computation confluent connectors data databricks dataflow delta designed distributed efficient end engineering ensures enterprise event events extension features flink flow framework fully google grade handle handling high hubs include ingestion integrates iot kafka key kinesis lake large latency like logs low manage managed management manner massive message messaging millions movement multi multiple nifi parallel per pipelines platform plays powerful processes processing provides pulsar ready real receiving registry role scalability scale schema second security service sources spark storm stream streaming streams structured supports system systems telemetry tenancy throughput time tool tools transformation used variety versioning via wide widely",
        "url": "/data/Large-Scale-Data-Ingestion2.html"
    },
    {
        "id": 82,
        "title": "Kafka-Producer-Consumer.html",
        "content": "apache automatically brokers close closes closing commits connect connects consumer continuously description deserializes done earliest ensure ensures f flush format import include initialize initializes json kafka kafkaconsumer kafkaproducer list message messages offset print prints producer read reading reads received send sending sends sent serializes sets specified start topic v x",
        "url": "/data/Kafka-Producer-Consumer.html"
    },
    {
        "id": 83,
        "title": "sql-statements.html",
        "content": "add adding address addresses alias allows already also alter altering alumni ascending assuming autoincrement average avg based belong brown calculates cartesian case changes check clause clauses column columns combine combines common conditional conditions count counts create creates creating cross cte customers data date default define delete deletes demonstrates department departments departmenttotals desc descending description different distinct doe drop dropping duplicate duplicates e earn either eliminating else email emails employee employees end equality example examples exceeds existence existing exists explanation expression filter find finds first following full greater group groups hierarchical high id improve include includes including index inner insert integer ip jane john join joined key keywords known languages least left like limits list lists location low main manager managers match matched matching medium met michael minimum modify name named names new none null number omitted one optional order outer output p pagination part partition perform price prices primary product products programming queries query ranges record records referenced related remains remove removes rename result results retains retrieval retrieve retrieves return returned returns right row rows salary sales sample second select selects self set sets side similar smith snowflake sort specified speed sql standard starts statement statements structure student students subquery sum table tables temporary test timestamp top total truncate truncating two types union unique update used useful using value values varchar view virtual within without york",
        "url": "/data/sql-statements.html"
    },
    {
        "id": 84,
        "title": "Apache-Hudi.html",
        "content": "accelerate access acid allowing allows also analytics apache architectures available background batch big bridges brings building capabilities cases changed clean compaction compression conclusion cost data datasets deletes designed effective efficient enabling ensures environments features file footprint framework frequent fresh gap guarantees hadoop historical hudi ideal improving include incremental incrementals indexes indexing ingestion inserts key lake lakes large latency low maintaining maintains making management managing modern near necessary older open optimizes optimizing overall perform performance pipelines powerful previous processed processing provides queries query querying real reducing require scalable scale scenarios simplifies sizes source storage support supports systems techniques time tool top traditional transaction transactional transactions travel updates upserts use users using versions",
        "url": "/data/Apache-Hudi.html"
    },
    {
        "id": 85,
        "title": "sql-create-view.html",
        "content": "along conditions create creates department departments e employee employees greater group include join joins named names overview query salaries salary select shows simple sql statement sum table total view",
        "url": "/data/sql-create-view.html"
    },
    {
        "id": 86,
        "title": "Query-Performance.html",
        "content": "access across advanced age aggregations allowing amount analysis analyze automatic avoid aws b based batch benefit best better bigquery bottlenecks caching city clauses cloud clustering clusters column columns complex compute conditions consumption cost costs count create crucial customers data database databases dataset datasets date decimal decisions denormalization denormalize desc disk distribute distribution early efficiency efficient efficiently employees ensure etc evenly execute executed execution explain explicit fast faster filtering find frequently full group grouped hash help helping id identify importance improve improvement improves improving include index indexed indexes indexing inefficiencies inserts int interprets join joins key keys large later leverage like limit limiting maintenance make materialized memory merge minimizing multiple name names necessary need nodes number offset olap operational optimization optimize optimized optimizer optimizing order partition partitioned partitioning performance physically plan practices precomputed price processed products proper providing queries query range reclaim redshift reduce reduces reducing regular regularly resource restrict result results returned rows run salaries sales scalability scalable scale scan scans segments select single size smaller snowflake sort space specific speed splits sql statistics store stores suboptimal subset sum systems table tables tasks techniques times tools types understand update updates use using utilize vacuum view views virtual warehouses well working writes",
        "url": "/data/Query-Performance.html"
    },
    {
        "id": 87,
        "title": "sql_overview.html",
        "content": "ability access adding additional adds alter analysis analytical back based broadly categorized category changes characteristics columns command commands commit common commonly complex concerned conditionals considered control create creates cube current data database databases dcl ddl define definition delete deletes deleting dml dql drop due elements ensuring etc existing extensions functionality functions grant granted group includes index indexes insert inserting integrity introduce isolation language later level like loops made manage managing manipulating manipulation modifies modify modifying new object objects often one oracle part permanently point previously primarily privileges procedural provides purpose query querying rank records relational remains removes retrieval retrieves retrieving revoke roles roll rollback rollup rows savepoint saves schemas select server set sets significance specific specifies sql structure structured structures table tables tcl technically transaction transactions truncate types undoes update updating used users variants window within",
        "url": "/data/sql_overview.html"
    },
    {
        "id": 88,
        "title": "Apache-Kafka.html",
        "content": "across adding aggregation allows analytics apache appends application architecture availability brokers cases cluster communication compaction confluent connect consumers data decoupled define delivery designed different disk distributed distributes divided duplicated durability durable easy ecosystem enables enabling ensuring event exactly external fault flows handle hardware high ideal immutable include includes integrates integration integrations kafka keep kept key large latency latest library like load log long lost low making messages messaging minimal multiple neither offering options ordered organized partition partitions parts policies processing producers publish range real records reducing regist replicated retention scalability scaling schema semantics sequence servers sinks sources sourcing storage stream streams subscribe supports system systems throughput time tolerance tolerant tools topics transforming use value volumes wide written",
        "url": "/data/Apache-Kafka.html"
    },
    {
        "id": 89,
        "title": "data-warehouse-architecture.html",
        "content": "approach architecture bottom centralized data denormalized design dimensional enterprise fast flexible include inmon integration kimball marts modeling normalized optimized querying reporting scalable snowflake top using warehouse",
        "url": "/data/data-warehouse-architecture.html"
    }
]