---
---
{% include menu.html title="PySpark SQL Functions" %}

<body>

    <h1>PySpark Spark SQL</h1>

    <h2>1. Select Unique Records</h2>
    <pre class="python">
        <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession

        <span class="comment"># Sample data</span>
        data = [(1, <span class="string">"Alice"</span>, 25), (2, <span class="string">"Bob"</span>, 30), (3, <span class="string">"Alice"</span>, 25)]
        df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>])

        df.createOrReplaceTempView(<span class="string">"people"</span>)

        <span class="comment"># Select distinct names</span>
        unique_names_df = spark.sql(<span class="string">
                        "SELECT DISTINCT name"
                        "FROM people"
                        </span>)

        <span class="comment"># Show result</span>
        unique_names_df.show()
    </pre>

    <h2>2. Count Records</h2>
    <pre class="python">
        <span class="comment"># Count the number of rows in the DataFrame</span>
        count_df = spark.sql(<span class="string">
                        "SELECT COUNT(*) as total_count"
                        "FROM people"
                        </span>)

        <span class="comment"># Show result</span>
        count_df.show()
    </pre>

    <h2>3. Group By and Count</h2>
    <pre class="python">
        <span class="comment"># Group by name and count occurrences</span>
        group_by_count_df = spark.sql(<span class="string">
                       "SELECT name, COUNT(*) as name_count"
                       "FROM people"
                       "GROUP BY name"
                       </span>)

        <span class="comment"># Show result</span>
        group_by_count_df.show()
    </pre>

    <h2>4. Group By with Aggregation (SUM)</h2>
    <pre class="python">
        <span class="comment"># Sample data</span>
        data = [(<span class="string">"Alice"</span>, 1000), (<span class="string">"Bob"</span>, 1500), (<span class="string">"Alice"</span>, 2000)]
        df2 = spark.createDataFrame(data, [<span class="string">"name"</span>, <span class="string">"salary"</span>])

        df2.createOrReplaceTempView(<span class="string">"salaries"</span>)

        <span class="comment"># Group by name and sum salaries</span>
        sum_salaries_df = spark.sql(<span class="string">
                          "SELECT name, SUM(salary) as total_salary" 
                          "FROM salaries"
                          "GROUP BY name"
                          </span>)

        <span class="comment"># Show result</span>
        sum_salaries_df.show()
    </pre>

    <h2>5. Group By with Aggregation (Average)</h2>
    <pre class="python">
        <span class="comment"># Group by name and calculate average salary</span>
        avg_salaries_df = spark.sql(<span class="string">
                          "SELECT name, AVG(salary) as avg_salary"
                          "FROM salaries"
                          "GROUP BY name"
                          </span>)

        <span class="comment"># Show result</span>
        avg_salaries_df.show()
    </pre>

    <h2>6. Filter Records with WHERE Clause</h2>
    <pre class="python">
        <span class="comment"># Filter records where salary is greater than 1200</span>
        filter_df = spark.sql(<span class="string">
                    "SELECT * "
                    "FROM salaries"
                    "WHERE salary > 1200"
                    </span>)

        <span class="comment"># Show result</span>
        filter_df.show()
    </pre>

    <h2>7. Order Records by a Column</h2>
    <pre class="python">
        <span class="comment"># Order records by salary in descending order</span>
        order_by_df = spark.sql(<span class="string">
                      "SELECT * "
                      "FROM salaries"
                      "ORDER BY salary DESC"
                      </span>)

        <span class="comment"># Show result</span>
        order_by_df.show()
    </pre>


    <p><hr>

    <h1>PySpark SQL Functions</h1>

    <h2>1. Count the Number of Occurrences of Each Word</h2>
    <pre class="python">
        <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession
        <span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> explode, split, col

        <span class="comment"># Sample data</span>
        data = [(<span class="string">"Hello world"</span>,), (<span class="string">"Hello PySpark"</span>,), (<span class="string">"Spark is great"</span>,)]
        df = spark.createDataFrame(data, [<span class="string">"text"</span>])

        <span class="comment"># Split the text into words</span>
        words_df = df.select(explode(split(col(<span class="string">"text"</span>), <span class="string">" "</span>)).alias(<span class="string">"word"</span>))

        <span class="comment"># Count occurrences of each word</span>
        word_count_df = words_df.groupBy(<span class="string">"word"</span>).count()

        <span class="comment"># Show result</span>
        word_count_df.show()
    </pre>

    <h2>2. Filter Data Based on a Condition</h2>
    <pre class="python">
        <span class="comment"># Sample data</span>
        data = [(1, <span class="string">"Alice"</span>, 25), (2, <span class="string">"Bob"</span>, 30), (3, <span class="string">"Cathy"</span>, 22)]
        df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>])

        <span class="comment"># Filter rows where age > 25</span>
        filtered_df = df.filter(col(<span class="string">"age"</span>) > 25)

        <span class="comment"># Show result</span>
        filtered_df.show()
    </pre>

    <h2>3. Join Two DataFrames</h2>
    <pre class="python">
        <span class="comment"># Sample data for DataFrame 1</span>
        data1 = [(1, <span class="string">"Alice"</span>), (2, <span class="string">"Bob"</span>), (3, <span class="string">"Cathy"</span>)]
        df1 = spark.createDataFrame(data1, [<span class="string">"id"</span>, <span class="string">"name"</span>])

        <span class="comment"># Sample data for DataFrame 2</span>
        data2 = [(1, <span class="string">"HR"</span>), (2, <span class="string">"Engineering"</span>), (4, <span class="string">"Marketing"</span>)]
        df2 = spark.createDataFrame(data2, [<span class="string">"id"</span>, <span class="string">"department"</span>])

        <span class="comment"># Inner join on 'id'</span>
        joined_df = df1.join(df2, on=<span class="string">"id"</span>, how=<span class="string">"inner"</span>)

        <span class="comment"># Show result</span>
        joined_df.show()
    </pre>

    <h2>4. Group By and Aggregate Data</h2>
    <pre class="python">
        <span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> avg

        <span class="comment"># Sample data</span>
        data = [(<span class="string">"Alice"</span>, <span class="string">"HR"</span>, 25), (<span class="string">"Bob"</span>, <span class="string">"Engineering"</span>, 30), (<span class="string">"Cathy"</span>, <span class="string">"HR"</span>, 28)]
        df = spark.createDataFrame(data, [<span class="string">"name"</span>, <span class="string">"department"</span>, <span class="string">"age"</span>])

        <span class="comment"># Group by department and calculate average age</span>
        avg_age_df = df.groupBy(<span class="string">"department"</span>).agg(avg(<span class="string">"age"</span>).alias(<span class="string">"avg_age"</span>))

        <span class="comment"># Show result</span>
        avg_age_df.show()
    </pre>

    <h2>5. Create a UDF (User Defined Function)</h2>
    <pre class="python">
        <span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf
        <span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType

        <span class="comment"># Sample data</span>
        data = [(1, <span class="string">"Alice"</span>), (2, <span class="string">"Bob"</span>), (3, <span class="string">"Cathy"</span>)]
        df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"name"</span>])

        <span class="comment"># Define a UDF to add a prefix to a name</span>
        <span class="keyword">def</span> add_prefix(name):
            <span class="keyword">return</span> <span class="string">"Mr./Ms. "</span> + name

        add_prefix_udf = udf(add_prefix, StringType())

        <span class="comment"># Apply the UDF</span>
        df_with_prefix = df.withColumn(<span class="string">"name_with_prefix"</span>, add_prefix_udf(col(<span class="string">"name"</span>)))

        <span class="comment"># Show result</span>
        df_with_prefix.show()
    </pre>

    <h2>6. Handling Missing Data</h2>
    <pre class="python">
        <span class="comment"># Sample data with missing values</span>
        data = [(1, <span class="string">"Alice"</span>, 25), (2, <span class="string">"Bob"</span>, None), (3, <span class="string">"Cathy"</span>, 28)]
        df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>])

        <span class="comment"># Fill missing values in 'age' with a default value of 0</span>
        filled_df = df.na.fill({<span class="string">"age"</span>: 0})

        <span class="comment"># Show result</span>
        filled_df.show()
    </pre>

    <h2>7. Write Data to a CSV File</h2>
    <pre class="python">
        <span class="comment"># Sample data</span>
        data = [(1, <span class="string">"Alice"</span>, 25), (2, <span class="string">"Bob"</span>, 30), (3, <span class="string">"Cathy"</span>, 22)]
        df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"age"</span>])

        <span class="comment"># Write DataFrame to CSV</span>
        df.write.csv(<span class="string">"/path/to/output"</span>, header=<span class="keyword">True</span>)
    </pre>

</body>

<body>
    <h1>PySpark Read Write SQL</h1>
    <h2>Introduction</h2>
    <p>In this guide, we will deep dive into how to manage data lakes on Databricks using SQL within PySpark. A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale.</p>

    <h2>Loading Data into Data Lake</h2>
    <p>Let's start by loading data into a data lake using PySpark SQL queries.</p>
    <pre><code><span class="keyword">spark.sql</span>(<span class="string">"SELECT * FROM '/mnt/data/sample.csv'"</span>).<span class="keyword">show</span>()
    </code></pre>

    <h2>Data Transformation</h2>
    <p>After loading data into the data lake, you can perform transformations using SQL queries in PySpark. For example, let's perform a group by operation.</p>
    <pre><code><span class="keyword">spark.sql</span>(<span class="string">"SELECT category, SUM(price) FROM sales GROUP BY category"</span>).<span class="keyword">show</span>()
    </code></pre>

    <h2>Writing Data Back to Data Lake</h2>
    <p>Once you have processed the data, you can write it back to your data lake in various formats such as Parquet using PySpark.</p>
    <pre><code><span class="keyword">df_grouped</span>.<span class="function">write</span>.<span class="function">format</span>(<span class="string">"parquet"</span>).<span class="function">save</span>(<span class="string">"/mnt/data/output/"</span>)
    </code></pre>

    <h2>PySpark SQL - Find Duplicate Records</h2>
    <p>This code finds duplicate records in the data based on a specific column using SQL commands in PySpark.</p>
    <pre><code><span class="keyword">spark.sql</span>(<span class="string">"SELECT email, COUNT(email) FROM customers GROUP BY email HAVING COUNT(email) > 1"</span>).<span class="keyword">show</span>()
    </code></pre>

    <h2>PySpark SQL - Top Categories by Price</h2>
    <p>This code retrieves the top categories by the total price using SQL queries in PySpark.</p>
    <pre><code><span class="keyword">spark.sql</span>(<span class="string">"SELECT category, SUM(price) FROM sales GROUP BY category ORDER BY SUM(price) DESC LIMIT 10"</span>).<span class="keyword">show</span>()
    </code></pre>


   <p><hr>

    <h1>PySpark Read Write DataFrame</h1>
    <h2>Introduction</h2>
    <p>In this guide, we will deep dive into how to manage data lakes on Databricks using PySpark. A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale.</p>

    <h2>Loading Data into Data Lake</h2>
    <p>Let's start by loading data into a data lake using PySpark on Databricks.</p>
    <pre><code><span class="keyword">df</span> = <span class="keyword">spark</span>.<span class="function">read</span>.<span class="function">format</span>(<span class="string">"csv"</span>)<span class="function">.option</span>(<span class="string">"header"</span>, <span class="string">"true"</span>).<span class="function">load</span>(<span class="string">"/mnt/data/sample.csv"</span>)
<span class="keyword">df.show()</span>
    </code></pre>

    <h2>Data Transformation</h2>
    <p>After loading data into the data lake, you can perform transformations using PySpark. For example, let's perform a group by operation.</p>
    <pre><code><span class="keyword">df_grouped</span> = <span class="keyword">df.groupBy</span>(<span class="string">"category"</span>).<span class="function">sum</span>(<span class="string">"price"</span>)
<span class="keyword">df_grouped.show()</span>
    </code></pre>

    <h2>Writing Data Back to Data Lake</h2>
    <p>Once you have processed the data, you can write it back to your data lake in various formats such as Parquet or Delta.</p>
    <pre><code><span class="keyword">df_grouped.write.format</span>(<span class="string">"parquet"</span>).<span class="keyword">save</span>(<span class="string">"/mnt/data/output/"</span>)
    </code></pre>

    <h2>PySpark - Find Duplicate Records</h2>
    <p>This code finds duplicate records in the data based on a specific column.</p>
    <pre><code><span class="keyword">df_duplicates</span> = <span class="keyword">df.groupBy</span>(<span class="string">"email"</span>).<span class="function">count</span>().<span class="keyword">filter</span>(<span class="string">"count > 1"</span>)
<span class="keyword">df_duplicates.show()</span>
    </code></pre>

    <h2>PySpark - Top Categories by Price</h2>
    <p>This code retrieves the top categories by the total price, similar to how you might use SQL's GROUP BY.</p>
    <pre><code><span class="keyword">df_top_categories</span> = <span class="keyword">df.groupBy</span>(<span class="string">"category"</span>).<span class="function">sum</span>(<span class="string">"price"</span>).<span class="keyword">orderBy</span>(<span class="string">"sum(price)"</span>, <span class="keyword">ascending</span>=<span class="keyword">False</span>)
<span class="keyword">df_top_categories.show</span>(<span class="keyword">10</span>)
    </code></pre>

  {% include footer.html %}

  </body>
</html>
    

