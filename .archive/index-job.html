---
---
{% include menu.html title="Kevin Luzbetak, MSc - Data & Machine Learning Engineer" %}
<img src="/images/brain-made-circuit-boards-neural-pathways-holographic-data-particles-clean-minimalist-design-sci-fi.png" width=1100 height=160>

<table border="0" cellpadding="10" cellspacing="0">
    <tr>
        <td valign="center">
            <img src="/images/kevin-luzbetak.jpg" alt="Kevin Luzbetak" class="profile-photo" width="240">
        </td>
        <td valign="top" style="padding-left:70px;">
            <h1 style="color: #ffffff !important;">Kevin Thomas Luzbetak, MSc</h1>
            <div class="contact-info">
                <table border="1" cellpadding="6" cellspacing="0" style="border-color: #444444;">
                  <tr><td style="border-color: #444444;">Education</td><td style="border-color: #444444;">Master's Degree in Artificial Intelligence</td></tr>
                  <tr><td style="border-color: #444444;">Location</td><td style="border-color: #444444;">Agoura Hills, Southern California</td></tr>
                  <tr><td style="border-color: #444444;">Resume</td><td style="border-color: #444444;"><a href="https://luzbetak.github.io/resume.pdf">https://luzbetak.github.io/resume.pdf</a></td></tr>
                  <tr><td style="border-color: #444444;">My Email</td><td style="border-color: #444444;"><a href="mailto:Luzbetak5739@gmail.com">Luzbetak5739@gmail.com</a></td></tr>
                  <tr><td style="border-color: #444444;">My Public Github</td><td style="border-color: #444444;"><a href="https://github.com/luzbetak" target="_blank">https://github.com/luzbetak</a></td></tr>
                  <tr><td style="border-color: #444444;">My AI Chatbot Demo</td><td style="border-color: #444444;"><a href="https://protonchat.com" target="_blank">https://ProtonChat.com</a></td></tr>
                </table>
            </div>
        </td>
    </tr>
</table>        

<table width="640" style="width: 640px; margin-left: auto; margin-right: auto; border-collapse: collapse; text-align: center;">
  <tr>
    <td>
      <h3 style="color: red; font-family: sans-serif;">
        Due to a high volume of spam calls,<br>
        I kindly ask that you schedule all phone calls through<br>
        <a href="https://calendly.com/kevin-luzbetak" target="_blank" style="color: blue; text-decoration: underline; font-weight: bold;">https://calendly.com/kevin-luzbetak</a>
      </h3>
    </td>
  </tr>
</table>

<hr align=left width=1100>
<h2>Summary</h2>
<p>
AI Engineer and Data Engineer with 30 years of experience in developing high quality, low
cost AI-powered, data science, data analytics platforms and cloud databases. Specializing in
Retrieval-Augmented Generation (RAG) applications, developing full-stack AI
chatbots using large language models (LLM), vector databases, and hybrid
retrieval architectures. Experienced in architecting distributed data platforms
using Apache Spark, Databricks, and Snowflake across AWS and Azure, with
expertise in ETL pipeline development, Medallion Architecture, and dimensional
data modeling. Hands-on with end-to-end infrastructure including NVIDIA GPU
configuration, model training, and production deployment.
</p>

<br><p>

<h2>Technical Skills</h2>

<table border="1" cellpadding="8" cellspacing="0" style="border-color: #444444;">
    <tr>
        <td style="border-color: #444444;"><strong>Languages</strong></td>
        <td style="border-color: #444444;">Python, Go, Node.js, Bash, SQL</td>
    </tr>
    <tr>
        <td style="border-color: #444444;"><strong>AI/ML</strong></td>
        <td style="border-color: #444444;">LLMs, RAG, Vector Databases (ChromaDB, FAISS, Pinecone), Prompt Engineering, LSTM Neural Networks, NLP, PyTorch, TensorFlow, Hugging Face Transformers, LangChain, Sentence Transformers, Scikit-learn, MLflow, Ollama</td>
    </tr>
    <tr>
        <td style="border-color: #444444;"><strong>Data Engineering</strong></td>
        <td style="border-color: #444444;">Apache Spark, Databricks, Delta Lake, Apache Iceberg, Kafka, Snowflake, Redshift, Medallion Architecture, Unity Catalog, ETL/ELT Pipelines, Pandas, NumPy</td>
    </tr>
    <tr>
        <td style="border-color: #444444;"><strong>Databases &amp; Modeling</strong></td>
        <td style="border-color: #444444;">PostgreSQL, MySQL, Cassandra, Kimball Dimensional Modeling, Star/Snowflake Schemas, ER Modeling, Data Lineage, MDM</td>
    </tr>
    <tr>
        <td style="border-color: #444444;"><strong>Infrastructure &amp; DevOps</strong></td>
        <td style="border-color: #444444;">Ubuntu Server, NVIDIA CUDA/GPU, Docker, GitHub Actions, Apache Airflow, Jenkins, Terraform, Grafana, Splunk, AWS, Azure</td>
    </tr>
</table>

<br><p>

<h2>Education</h2>

<table border="1" cellpadding="8" cellspacing="0" style="border-color: #444444;">
    <tr>
        <td style="border-color: #444444;"><strong>Master's Degree in Computer Science</strong></td>
        <td style="border-color: #444444;">California Lutheran University, Thousand Oaks, CA</td>
        <td style="border-color: #444444;">2014</td>
        <td style="border-color: #444444;">Machine Learning, Distributed Systems, Computer Vision, Embedded Systems</td>
    </tr>
    <tr>
        <td style="border-color: #444444;"><strong>Bachelor's Degree in Information Technology</strong></td>
        <td style="border-color: #444444;">University of Phoenix, Woodland Hills, CA</td>
        <td style="border-color: #444444;">2004</td>
        <td style="border-color: #444444;">Software Development, Database Systems, Artificial Intelligence</td>
    </tr>
</table>

<hr />

  <h1>Model Parameters = Words × Dimensions</h1>

  <h2>Key Concepts</h2>
  <ul>
    <li><strong>Word</strong>: A symbolic label (token) such as <strong>cat</strong>, <strong>dog</strong>, or <strong>car</strong>.</li>
    <li><strong>Dimension</strong>: The number of numeric values used to represent a word.</li>
    <li><strong>Parameter</strong>: A single stored numeric value. Every number in an embedding vector is one parameter.</li>
  </ul>

  <h2>Why This Example Has Exactly 6 Parameters</h2>
  <ol>
    <li>There are <strong>3 words</strong>: cat, dog, and car.</li>
    <li>Each word has <strong>2 dimensions</strong>.</li>
    <li>Total parameters = <strong>3 × 2 = 6</strong>.</li>
  </ol>

  <h3>Where the 6 Parameters Actually Live</h3>

  <pre><code class="language-text">
    Feature Dimensions:  [Object, Animal]
                   cat → [0, 1] → 2 Dimensions
                   dog → [0, 1] → 2 Dimensions
                   car → [1, 0] → 2 Dimensions
          -------------------------------------
          Total: → 3 × 2 = 6 Parameters


          +---------------------------------------+
          |  Vector DB: with Feature Dimensions   |
          +--------+--------+--------+------------+
          |  Word  | Object | Animal | Dimensions |
          +--------+--------+--------+------------+
          |  cat   |   0    |   1    |     2      |
          |  dog   |   0    |   1    |     2      |
          |  car   |   1    |   0    |     2      |
          +--------+--------+--------+------------+
          | Total  |      3 × 2 = 6 Parameters    |
          +--------+--------+--------+------------+
  </pre></code>


  <pre><code class="language-text">
   <h3> Bridging the example to real models:</h3>              
    +------------------+------------+------------+------------------+
    |      Model       |   Words    | Dimensions |    Parameters    |
    +------------------+------------+------------+------------------+
    | This example     |          3 |          2 |                6 |
    | GPT-2 embeddings |     50,257 |        768 |       38,597,376 |
    +------------------+------------+------------+------------------+
  </pre></code>

  <h2>Python Code Example</h2>

  <pre><code class="language-python">import numpy as np

embedding_table = {

    #--- Feature Dim: [Object, Animal] ---#
    "cat": np.array([  0     ,  1 ]),
    "dog": np.array([  0     ,  1 ]),
    "car": np.array([  1     ,  0 ]),

}

def embed(word):
    return embedding_table[word]

print(embed("cat"))
print(embed("dog"))</code></pre>

  <h2>Final Summary</h2>
  <ul>
    <li><strong>Words</strong> are labels.</li>
    <li><strong>Dimensions</strong> describe vector length per word.</li>
    <li><strong>Parameters</strong> are the total stored numeric values.</li>
    <li>This example has <strong>6 parameters</strong>.</li>
  </ul>


  <h1>How the Embedding Numbers Are Computed</h1>

  <h2>Goal</h2>
  <ul>
    <li>Show a <strong>simple, concrete way</strong> to compute the numbers in embedding vectors.</li>
    <li>Explain where values like <strong>[0, 1]</strong> and <strong>[0, 1]</strong> come from.</li>
  </ul>

  <h2>Important Note</h2>
  <ul>
    <li>In real models, these numbers are <strong>learned during training</strong>.</li>
    <li>Here we use a <strong>toy mathematical rule</strong> to make the idea understandable.</li>
  </ul>

  <h2>Simple Rule to Compute Embedding Numbers</h2>
  <ul>
    <li>Assign each word two numeric features:</li>
    <ul>
      <li><strong>Feature 1</strong>: how “animal-like” the word is</li>
      <li><strong>Feature 2</strong>: how “object-like” the word is</li>
    </ul>
    <li>Each feature is a number between <strong>0.0</strong> and <strong>1.0</strong>.</li>
    <li>The two feature values together form a <strong>2-dimensional embedding</strong>.</li>
  </ul>

  <h3>Manual Feature Assignment</h3>
  <ul>
    <li><strong>cat</strong>: mostly animal → [0, 1]</li>
    <li><strong>dog</strong>: mostly animal → [0, 1]</li>
    <li><strong>car</strong>: mostly object → [1, 0]</li>
  </ul>

  <h2>Python Example: Computing the Numbers</h2>

 <hr />

  <pre><code class="language-python">def compute_embedding(animal_score, object_score):
    # Combine two numeric features into a vector
    return [animal_score, object_score]

cat_embedding = compute_embedding(0, 1)
dog_embedding = compute_embedding(0, 1)

print("cat:", cat_embedding)
print("dog:", dog_embedding)</code></pre>

  <h2>What This Represents</h2>
  <ul>
    <li>Each number is a <strong>parameter value</strong>.</li>
    <li>The vector length (2 numbers) is the <strong>dimension</strong>.</li>
    <li>The rule that assigns numbers is the <strong>model logic</strong>.</li>
  </ul>

  <h2>Connection to Real Models</h2>
  <ul>
    <li>Real embedding models start with random numbers.</li>
    <li>Numbers are adjusted using training data and optimization.</li>
    <li>After training, vectors encode semantic meaning.</li>
  </ul>

  <h2>Final Summary</h2>
  <ul>
    <li><strong>[0, 1]</strong> and <strong>[0, 1]</strong> are examples of computed feature values.</li>
    <li>The computation rule is simple here for clarity.</li>
    <li>Real models learn these values automatically.</li>
  </ul>

 
{% include footer.html %}

