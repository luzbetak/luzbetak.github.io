---
---
{% include menu.html title="Kevin Luzbetak - Data & Machine Learning Engineer" %}
<hr align=left width=1000>


<body>
    <header>
        <table border="0" cellpadding="10" cellspacing="0">
            <tr>
                <td valign="top">
                    <img src="kevin-luzbetak.jpg" alt="Kevin Luzbetak" class="profile-photo" width="240">
                </td>
                <td valign="top">
                    <h1>Kevin Thomas Luzbetak</h1>
                    <h3>25+ Years Programming Data Systems & AI-Powered Solutions</h3>
                    <div class="contact-info">
                        <p>Location: Los Angeles County, California<br>
                        Email: Luzbetak5739@gmail.com • Phone: (818) 288-7357<br>
                        Resume: <a href="https://kevinluzbetak.com/resume.pdf">https://KevinLuzbetak.com/resume.pdf</a></p>
                    </div>
                </td>
            </tr>
        </table>
    </header>

<h1>Professional Experience Overview</h1>

<h2>1. Senior Data Engineer – Databricks Inc.</h2>

<p>
At Databricks, I worked on an enterprise HR and people-systems platform where my role was effectively
<strong>Cloud Database Architect</strong> plus <strong>platform engineer</strong>. I designed a
<strong>Delta Lake–based medallion architecture</strong> that unified recruitment, employee lifecycle,
and organizational hierarchy data coming from systems like <strong>Workday</strong> and
<strong>Greenhouse</strong>.
</p>

<p>
I owned the <strong>database schema design</strong>, including normalized core entities and
<strong>SCD Type 2 history tracking</strong>, because leadership needed full historical auditability.
I also implemented <strong>PII protection</strong>, <strong>field-level encryption</strong>, and
<strong>data retention controls</strong>, which is directly relevant to regulated environments such as
financial services.
</p>

<p>
Beyond architecture, I built the <strong>data ingestion framework in Python</strong>, tuned
<strong>Spark jobs for performance</strong>, and implemented automated
<strong>data quality validation</strong> and <strong>anomaly detection</strong> so issues were caught
before impacting downstream analytics. This role reinforced my strength in designing
<strong>governed, production-grade cloud databases</strong> that scale reliably while remaining
compliant.
</p>

<hr />

<h2>2. Senior Data Engineer – Disney Streaming</h2>

<p>
At Disney Streaming, I focused on architecting a <strong>multi-domain cloud data platform</strong>
supporting customer, marketing, and product analytics. I designed a shared
<strong>Apache Iceberg–based open table architecture</strong> that enabled seamless interoperability
between <strong>Snowflake</strong> and <strong>Databricks</strong> without data duplication.
</p>

<p>
I defined <strong>domain-oriented data models</strong> and built reusable ETL frameworks in
<strong>Python and PySpark</strong> so individual teams could scale independently while maintaining
consistent governance and schema standards. I also designed metadata-driven ingestion patterns to
support schema evolution and time travel.
</p>

<p>
From a performance standpoint, I implemented <strong>partition pruning</strong>,
<strong>Z-ordering</strong>, and <strong>adaptive query execution</strong>, achieving significant
reductions in compute cost while improving analytical query latency. This role strengthened my
ability to design cloud database ecosystems that operate across platforms and organizational
boundaries.
</p>

<hr />

<h2>3. Senior Data Engineer – Nike Inc. </h2>

<p>
At Nike, I helped design and operate an <strong>enterprise-scale data platform</strong> supporting
global business operations. I implemented a <strong>Delta Lake medallion architecture</strong>
processing over <strong>one billion events per day</strong>, with strict SLAs for data availability
and performance.
</p>

<p>
A major initiative involved leading a large-scale
<strong>Snowflake-to-Databricks migration</strong>. I designed the migration strategy, schema evolution
approach, and parallel processing logic to ensure <strong>zero downtime</strong> and full data
reconciliation during cutover.
</p>

<p>
I also implemented <strong>infrastructure-as-code using Terraform</strong> to standardize database
provisioning, security policies, and cluster configurations across regions. This role highlighted
how strong database architecture directly drives performance, reliability, and cost efficiency.
</p>

<hr />

<h2>4. Senior Data Engineer – Brighthouse Financial </h2>

<p>
At Brighthouse Financial, I worked in a <strong>highly regulated financial environment</strong>,
supporting systems responsible for managing over <strong>$100B in assets</strong>. I led the migration
of critical datasets from on-premises Hadoop into <strong>Azure Databricks</strong>, transferring
hundreds of terabytes of data while maintaining uptime during market hours.
</p>

<p>
I designed <strong>time-series and historical data models</strong> optimized for actuarial analysis,
regulatory reporting, and long-term retention. These models emphasized auditability, traceability,
and deterministic data transformations.
</p>

<p>
I also tuned Spark workloads to reduce compute costs while improving performance and implemented
monitoring and alerting frameworks to proactively identify data quality and pipeline issues before
they impacted downstream financial reporting.
</p>

<hr />

<h2>5. Senior Data Engineer – Apple Inc. </h2>

<p>
At Apple, I worked on <strong>Apple Pay</strong>, supporting one of the most security-sensitive data
platforms in production. I helped design an <strong>Apache Iceberg–based lakehouse architecture</strong>
used for payment reconciliation and analytics, providing full ACID guarantees and time-travel
capabilities.
</p>

<p>
I engineered both batch and streaming pipelines using <strong>Spark</strong> and
<strong>Kafka</strong>, ensuring <strong>sub-second latency</strong> while maintaining strict
<strong>PCI compliance</strong>. Sensitive payment data was protected using encryption, tokenization,
and tightly controlled access patterns.
</p>

<p>
This role reinforced disciplined engineering practices around correctness, fault tolerance, and
security, where even minor data inconsistencies were unacceptable.
</p>

<hr />

<h2>6. Database Engineer III – Amazon Web Services (AWS)</h2>

<p>
At AWS, I worked directly with Fortune 500 customers to migrate
<strong>petabyte-scale data warehouses</strong> from platforms such as <strong>Teradata</strong> and
<strong>Netezza</strong> into <strong>Amazon Redshift</strong>.
</p>

<p>
I designed high-throughput migration architectures using <strong>AWS DMS</strong> and custom Python
validation frameworks to ensure full data integrity across billions of records. Post-migration, I
optimized Redshift clusters through <strong>WLM tuning</strong>, vacuum strategies, and query
optimization.
</p>

<p>
This role provided deep insight into cloud data warehouse internals and reinforced the importance
of making correct architectural decisions early when operating at extreme scale.
</p>

<hr />

<h2>7. Senior Software Engineer – Nexstar Media Group</h2>

<p>
At Nexstar Media Group, I built <strong>real-time data systems</strong> supporting media streaming,
analytics, and predictive modeling for national television and weather platforms. I designed
<strong>PostgreSQL-based data models</strong> combining transactional and analytical workloads.
</p>

<p>
I developed APIs and data pipelines that handled massive traffic spikes during breaking news and
severe weather events, requiring highly reliable database and caching strategies. I also built
real-time analytics pipelines using <strong>PySpark</strong> and <strong>Elasticsearch</strong>.
</p>

<p>
This role strengthened my ability to design databases that support both operational systems and
analytical workloads simultaneously under extreme load.
</p>


<hr />
    
    <main>

        <section id="education">
            <h2>Education</h2>
            
            <div class="education-item">
                <h3>Master’s Degree in Computer Science</h3>
                <ul>
                    <li><strong>California Lutheran University</strong> • Graduated: 2014</li>
                    <li>Thousand Oaks, California</li>
                    <li>Focus: Machine Learning, Distributed Systems, Computer Vision, Embedded Systems</li>
                </ul>
            </div>
            
            <div class="education-item">
                <h3>Bachelor’s Degree in Information Technology</h3>
                <ul>
                    <li><strong>University of Phoenix</strong> • Graduated: 2004</li>
                    <li>Woodland Hills, California</li>
                    <li>Focus: Software Development, Data Modeling, Artificial Intelligence</li>
                </ul>
            </div>
            
            <div class="education-item">
                <h3>Certificate in C/Linux</h3>
                <ul>
                    <li><strong>UCLA Extension</strong> • Graduated: 2001</li>
                    <li>Los Angeles, California</li>
                    <li>Focus: C/C++ Object-Oriented Programming, Linux System Administration</li>
                </ul>
            </div>
        </section>

    </main>

    <body>

<hr />

<h1>STAR Method: (Situation – Task – Action – Result Workflow)</h1>

<h2>Overview</h2>
<p>
The <strong>STAR method</strong> is a structured workflow used to clearly explain professional experience by breaking an answer into four logical parts:
<strong>Situation</strong>, <strong>Task</strong>, <strong>Action</strong>, and <strong>Result</strong>.
</p>

<h2>STAR Workflow</h2>

<ol>
  <li>
    <h3>Situation (Context) - Analysis</h3>
    <ul>
      <li>Describe the <strong>environment</strong> or background</li>
      <li>Explain the <strong>business or technical problem</strong></li>
      <li>Provide relevant <strong>constraints</strong> such as scale, deadlines, or compliance</li>
    </ul>
    <p><strong>Key question:</strong> What was happening?</p>
  </li>

  <li>
    <h3>Task (Responsibility) - Design</h3>
    <ul>
      <li>Define <strong>your role</strong> in the situation</li>
      <li>Clarify what you were <strong>accountable for</strong></li>
      <li>State the <strong>goal or success criteria</strong></li>
    </ul>
    <p><strong>Key question:</strong> What was I responsible for?</p>
  </li>

  <li>
    <h3>Action (Execution) - Implementation</h3>
    <ul>
      <li>Explain the <strong>specific actions</strong> you took</li>
      <li>Describe the <strong>tools, technologies, or methods</strong> used</li>
      <li>Highlight <strong>technical decisions</strong> and trade-offs</li>
      <li>Show <strong>ownership, problem-solving, and leadership</strong></li>
    </ul>
    <p><strong>Key question:</strong> What did I do to solve the problem?</p>
  </li>

  <li>
    <h3>Result (Outcome) - Testing</h3>
    <ul>
      <li>Present <strong>measurable results</strong> whenever possible</li>
      <li>Show <strong>business or technical impact</strong></li>
      <li>Explain <strong>long-term value</strong> created</li>
    </ul>
    <p><strong>Key question:</strong> What was the outcome?</p>
  </li>
</ol>

<h2>Why the STAR Method Works</h2>

<ul>
  <li>Provides a <strong>clear, logical structure</strong></li>
  <li>Demonstrates <strong>ownership and accountability</strong></li>
  <li>Highlights <strong>impact and results</strong>, not just effort</li>
  <li>Works well for <strong>technical, behavioral, and leadership interviews</strong></li>
</ul>

{% include footer.html %}

